\documentclass[12pt, a4paper, oneside, openright, titlepage]{book}
\usepackage[utf8]{inputenc}
\raggedbottom
\usepackage{import}


\input{../book_packages}

%%% Specific Macros %%%
\newcommand{\bra}[1]{\left\langle#1\right\vert}
\newcommand{\ket}[1]{\left\vert#1\right\rangle}
\newcommand{\braket}[2]{\left\langle#1\right\vert\left.#2\right\rangle}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\brkt}[1]{\left\langle #1\right\rangle}

%%%%%% BEGIN %%%%%%%%%%


\begin{document}

%%%%%% TITLE PAGE %%%%%

\begin{titlepage}
    \centering
    \scshape
    \vspace*{\baselineskip}
    \rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt}
    \rule{\textwidth}{0.4pt}
    
    \vspace{0.75\baselineskip}
    
    {\LARGE Quantum Mechanics: A Complete Guide}
    
    \vspace{0.75\baselineskip}
    
    \rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt}
    \rule{\textwidth}{1.6pt}
    
    \vspace{2\baselineskip}
    Phys 443-543 \\
    \vspace*{3\baselineskip}
    \monthdayyeardate\today \\
    \vspace*{5.0\baselineskip}
    
    {\scshape\Large E Thompson, \\ Physics and Math Honors\\}
    
    \vspace{1.0\baselineskip}
    \textit{Solo Pursuit of Learning}
    \vfill
    \enlargethispage{1in}
    \begin{figure}[b!]
    \makebox[\textwidth]{\includegraphics[width=\paperwidth, height =10cm]{../Crab.jpg}}
    \end{figure}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Part 1
\part{Theory}

%%%%%%%%%%%%%%%%%%%%%% Chapter 1.1
\chapter{The Wave Equation}


\section{The Schr\"{o}dinger Equation}

Consider a particle of mass $m$, constrained along the $x$ axis, subject to a specified force $F(x,t)$. In classical mechanics our goal is to determine the position function, $x(t)$. Once that is known, any other desired quantity such as velocity, momentum, and kinetic energy could be derivded. In deriving $x(t)$, we apply Newton's second law, $F = ma$ (for conservative systems). This together with the appropriate initial conditions fully specifies $x(t)$.

On the other hand, Quantum mechanics approaches this quite differently. In this case we're looking for a particle's \Emph{wave function}, $\Psi(x,t)$, and we get it by solving the \Emph{Schr\"{o}dinger equation}: \begin{equation*}
    \boxed{i\hbar\frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2m}\frac{\partial^2\Psi}{\partial x^2}+V\Psi}
\end{equation*}
This plays the role logically analogous to Newton's second law; given suitable initial conditions, the Schr\"{o}dinger equation determines $\Psi(x,t)$ for all future $t$.


\section{Statistical Interpretation}

\begin{qst}
    What is this ``wave function," and what does it do for you once you've got it? How can we represent a particle with such an object?
\end{qst}


The answer is provided by Born's \Emph{statistical interpretation}, which says that $|\Psi(x,t)|^2$ gives the \Emph{probability of finding the particle at point $x$, at time $t$}, or more precisely \begin{equation*}
    \int_a^b|\Psi(x,t)|^2dx = \left\{\begin{array}{c} \text{probability of finding the particle} \\ \text{between $a$ and $b$, at time $t$}\end{array}\right\}
\end{equation*}
Probability in this case is the \Emph{area} under the graph $|\Psi|^2$. Note that the statistical intepretation introduces a kind of \Emph{indeterminacy} into quantum mechanics, for even if you know everything about the particle in terms of the theory (i.e. its wave function), you still cannot predict with certainty the outcome of a simple experiment to measure its position. This can be quite a disturbing outcome. For instance,

\begin{qst}
    If you do a measurement and find the position of a particle at some point, $A$, where was the particle just before you made the measurement?
\end{qst}

There are three main schools of thought which attempt to address this question: 

\begin{enumerate}
    \item The \Emph{realist} position: The particle was at $A$. Note that if this was true, then quantum mechanics is an incomplete theory. To the realist, indeterminacy is not a fact of nature, but a reflection of our ignorance.
    \item The \Emph{orthodox} position: The particle wasn't really anywhere. It was the act of measurement that forced it to ``choose." Jordan - ``Observations not only disturb what is to be measured, they produce it ... We compel [the particle] to assume a definite position." This is the \Emph{Copenhagen interpretation}, subscrobed by Bohr and his associates.
    \item The \Emph{agnostic} position: Refuse to answer. After all, what sense can there be in making assertions about the status of a particle before a measurement, when the only way of knowing whether you were right is precisely to make a measurement, in which case what you get is no longer ``before the measurement."
\end{enumerate}

In 1964, John Bell astonished the physics community by showing that it makes an observable difference whether the particle had a precise (though unknown) position prior to the measurement, or not. This eliminated agnosticism as a viable option, and made it an experimental question whether 1 or 2 is the correct choice; in the end it was the orthodox position that yielded the most sensical results. 

It is well agreed on that a repeated measurement on the same particle must return the same value. How does the orthodox interpretation account for this? We say that the wave function \Emph{collapses} upon measurement, to a spike at the point of measurement, although it soon spreads out again, in accordance with the Schr\"{o}dinger equation.


\section{Probability}

We begin our discussion of probability with a discrete probability distribution, $P(j)$, consisting of an at most countable number of probabilities. Normalization of probabilities implies that $$\sum_{j=0}^{\infty}P(j) = 1$$
Now, if $f$ is some function of the index variable $j$, we define the average or \Emph{expected value} of $f$ on $j$ to be \begin{equation*}
    \boxed{\langle f(j)\rangle = \sum_{j=0}^{\infty}f(j)P(j)}
\end{equation*}
A special case of this is the average value of the distribution: \begin{equation*}
    \boxed{\langle j\rangle = \sum_{j=0}^{\infty}jP(j)}
\end{equation*}
Now, how do we describe the spread of a distribution? We use the squared-mean-error, or \Emph{variance}, of the distribution: \begin{equation*}
    \boxed{\sigma^2 := \langle (j-\langle j\rangle)^2\rangle =: \langle (\Delta j)^2\rangle}
\end{equation*}
Here $\sigma$ itself is known as the \Emph{standard deviation}. It is useful to note the equality $$\sigma^2 = \langle(\Delta j)^2\rangle = \langle j^2\rangle - \langle j\rangle^2$$
so taking the square root we have $$\sigma = \sqrt{\langle j^2\rangle - \langle j \rangle^2}$$


We now move to working with probability distributions over continuous variables: variables ranging over intervals in $\R$. But, when making this switch we can no longer sensibly talk about the probability of an event occurring at a particular value of our continuous variable, and instead must discuss the probability that a result lies in some interval. Thus, we have the heuristic $$\left\{\begin{array}{c} \text{probability that an individual (chosen }\\ \text{at random) lies between $x$ and $(x+dx)$}\end{array}\right\} = \rho(x)dx$$
where the proportionality is known as our \Emph{probability density function}. 

\begin{defn}
    The probability that a measurement $X$ lies between points $a$ and $b$ in some finite interval for a continuous probability distribution with probability density $\rho(x)$ is \begin{equation*}
        P(a \leq X \leq b) = \int_a^b\rho(x)dx
    \end{equation*}
\end{defn}

By normalization conditions, we have that \begin{equation*}
    \int_{-\infty}^{\infty}\rho(x)dx = 1
\end{equation*}

\begin{defn}
    The expected value of a function $f$ with respect to a probability distribution with pdf $\rho(x)$ is $$\langle f(x)\rangle = \int_{-\infty}^{\infty}f(x)\rho(x)dx$$
\end{defn}

As in the discrete case, our variance for a continuous random variable is given by $$\sigma^2 := \langle (\Delta x)^2\rangle = \langle (x-\langle x\rangle)^2\rangle = \langle x^2\rangle - \langle x\rangle^2$$


\section{Normalization and the Wave Function}

Returning to the statistical interpretation of the wave function, and noting our discussions of probability in the previous section, we concur that the integral of $|\Psi|^2$ over all $x$ must be $1$ (the particle has to be somewhere) at any time $t$: $$\boxed{\int_{-\infty}^{\infty}|\Psi(x,t)|^2dx = 1}$$ Without this the statistical interpretation would be useless.

But, we must make sure that this normalization condition is consistent with the Schr\"{o}dinger equation which determines the wave function. But, recalling the Schr\"{o}dinger equation $$i\hbar\frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2m}\frac{\partial^2\Psi}{\partial x^2}+V\Psi$$ if $\Psi(x,t)$ is a solution, $A\Psi(x,t)$ is as well for any $A \in \C$. Thus, we can pick this factor so that $\Psi(x,t)$ satisfies normalization. In some cases our solution gives an infinite integral, or even the $0$ integral if dealing with the trivial solution $\Psi \equiv 0$. Such \Emph{non-normalizable} solutions cannot represent particles, and must be rejected. Physically realizable states correspond to the \Emph{square-integrable} solutions to Schr\"{o}dinger's equation.

But, how do we ensure that once we normalize the wave function at $t = 0$, it will remain normalized indefinitely? This follows from the fact that the Schr\"{o}dinger equation has the property that it automatically preserves the normalization of the wave function - without this feature the Schr\"{o}dinger equation would be incompatible with the statistical intepretation, and the whole theory would crumble.

\begin{proof}[Proof of Time-Invariance of Normalization]
    To begin with, $$\frac{d}{dt}\int_{-\infty}^{\infty}|\Psi(x,t)|^2dx = \int_{-\infty}^{\infty}\frac{\partial}{\partial t}|\Psi(x,t)|^2dx$$ By the product rule $$\frac{\partial }{\partial t}|\Psi|^2 = \frac{\partial}{\partial t}(\Psi^*\Psi) = \Psi^*\frac{\partial\Psi}{\partial t}+\frac{\partial \Psi^*}{\partial t}\Psi$$ Now the Schr\"{o}dinger equation reads $$\frac{\partial \Psi}{\partial t} = \frac{i\hbar}{2m}\frac{\partial^2\Psi}{\partial x^2} - \frac{i}{\hbar}V\Psi$$ and hence taking the complex conjugate $$\frac{\partial \Psi^*}{\partial t} = -\frac{i\hbar}{2m}\frac{\partial^2\Psi^*}{\partial x^2} + \frac{i}{\hbar}V\Psi^*$$ so $$\frac{\partial}{\partial t}|\Psi|^2 = \frac{i\hbar}{2m}\left(\Psi^*\frac{\partial^2\Psi}{\partial x^2} - \frac{\partial^2\Psi^*}{\partial x^2}\Psi\right) = \frac{\partial}{\partial x}\left[\frac{i\hbar}{2m}\left(\Psi^*\frac{\partial \Psi}{\partial x} - \frac{\partial \Psi^*}{\partial x}\Psi\right)\right]$$
    Then we can evaluate our original integral using FTOC2 \begin{equation*}
        \frac{d}{dt}\int_{-\infty}^{\infty}|\Psi(x,t)|^2dx = \frac{i\hbar}{2m}\left(\Psi^*\frac{\partial \Psi}{\partial x} - \frac{\partial \Psi^*}{\partial x}\Psi\right)\Big\rvert_{-\infty}^{\infty}
    \end{equation*}
    But $\Psi(x,t)$ must go to zero as $x$ goes to infinity (otherwise the wave function would not be normalizable). It follows that $$\frac{d}{dt}\int_{-\infty}^{\infty}|\Psi(x,t)|^2dx = 0$$
    and hence the integral is constant.
\end{proof}


\section{Momentum}

For a particle in state $\Psi$, the expectation value of $x$ is \begin{equation*}
    \boxed{\langle x\rangle = \int_{-\infty}^{\infty}x|\Psi(x,t)|^2dx}
\end{equation*}
What does this mean? Note it does not mean that if you measure the position of one particle over and over again, $\int x|\Psi|^2dx$ is the average of the results you'll get. On the contrary, the first measurement, whose outcome is indeterminate, will collapse the wave function to a spike at the value actually obtained, and the subsequent measurements if they're performed quickly will simply repeat the same result. Rather, $\langle x\rangle$ is the average of measurements performed on particles all in the state $\Psi$, which means either you must find some way of returning the particle to its original state after each measurement, or else you have to prepare a whole \Emph{ensemble} of particles, each in the same state $\Psi$, and measure the positions of all of them. In short, the expectation value is the average of measurements on an ensemble of identically-prepared systems, not the average of repeated measurements on one and the same system.

As time moves, $\langle x\rangle$ may change since $\Psi$ is time dependent, so we might wish to look at how it changes: \begin{equation*}
    \frac{d\langle x\rangle}{dt} = \int_{-\infty}^{\infty}x\frac{\partial}{\partial t}|\Psi(x,t)|^2dx = \frac{i\hbar}{2m}\int_{-\infty}^{\infty}x\frac{\partial}{\partial x}\left(\Psi^*\frac{\partial\Psi}{\partial x} - \frac{\partial \Psi^*}{\partial x}\Psi\right)dx
\end{equation*}
This expression can be simplified with integration by parts to $$\frac{d\langle x\rangle}{dt} = -\frac{i\hbar}{2m}\int_{-\infty}^{\infty}\left(\Psi^*\frac{\partial \Psi}{\partial x} - \frac{\partial \Psi^*}{\partial x}\Psi\right)dx$$
Performing another integration by parts on the second term, we conclude $$\frac{d\langle x\rangle}{dt} = -\frac{i\hbar}{m}\int_{-\infty}^{\infty}\Psi^*\frac{\partial \Psi}{\partial x}dx$$

What does this mean? Note we are talking about a ``velocity" of the expectation value of $x$, which is \emph{not} the same as the velocity of the particle. Also, note it isn't clear yet if velocity makes sense in quantum mechanics - all we could ask for is the probability of getting a particular velocity. As we shall show later, the \Emph{expectation value of the velocity is equal to the time derivative of the expectation value of position}: $$\langle v\rangle = \frac{d\langle x\rangle}{dt} =  -\frac{i\hbar}{m}\int_{-\infty}^{\infty}\Psi^*\frac{\partial \Psi}{\partial x}dx$$

However, it is customary to work with \Emph{momentum}, $p = mv$, rather than velocity: \begin{equation*}
    \boxed{\langle p\rangle = m\frac{d\langle x\rangle}{dt} =  -i\hbar\int_{-\infty}^{\infty}\Psi^*\frac{\partial \Psi}{\partial x}dx}
\end{equation*}
We make the following observation on the forms of $\langle x\rangle$ and $\langle p\rangle$: \begin{align*}
    \langle x\rangle &= \int \Psi^*[x]\Psi dx \\
    \langle p\rangle &= \int\Psi^*[-i\hbar(\partial/\partial x)]\Psi dx
\end{align*}
We say that the \Emph{operator} $x$ ``represents" position, and the operator $-i\hbar(\partial/\partial x)$ ``represents" momentum.

The fact is all classical dynamical variables can be expressed in terms of position and momentum, and hence their corresponding variables can be derived: $$T = \frac{p^2}{2m}$$ and $$\mathbf{L} = \mathbf{r} \times \mathbf{p}$$
To calculate the expectation value of any such quantity $Q(x,p)$, we simply replace every $p$ by $-i\hbar(\partial/\partial x)$ and insert the resulting operator between $\Psi^*$ and $\Psi$, and integrate: \begin{equation*}
    \boxed{\langle Q(x,p)\rangle = \int \Psi^*\left[Q(x,-i\hbar \partial/\partial x)\right]\Psi dx}
\end{equation*}
For example the expectation value of the kinetic energy is $$\langle T\rangle = -\frac{\hbar^2}{2m}\int\Psi^*\frac{\partial^2\Psi}{\partial x^2}dx$$


\section{The Uncertainty Principle}

With any wave phenomenon (such as a wave on a string), the more packets (pulses) we observe the more well-defined the wavelength becomes, but the less well-defined a ``position" of the wave is, while the less packets we observe the more well-defined the position, but the less clear the wavelength. This applies to the quantum mechanical wave function as well. But, the wavelength of $\Psi$ is related to the momentum of the particle by the \Emph{de Broglie formula}: $$\boxed{p = \frac{h}{\lambda} = \frac{2\pi\hbar}{\lambda}}$$ Thus, a spread in wavelength corresponds to a spread in momentum, and our general observation now says that the more precisely determined a particle's position is, the less precisely determined its momentum. Quantitatively, $$\boxed{\sigma_x\sigma_p \geq \frac{\hbar}{2}}$$ where $\sigma_x$ is the standard deviation in $x$ and $\sigma_p$ is the standard deviation in $p$. This is Heisenberg's \Emph{uncertainty principle}.

Note what the uncertainty principle means: like position measurements, momentum measurements yield precise answers - the ``spread" here refers, as before, to the fact that measurements on identically prepared systems do not yield identical results. You can construct a state such that position measurements will be very close together (by making $\Psi$ a localized ``spike"), but you will pay a price: momentum measurements on this state will be widely scattered. Or you can prepare a state with a definite momentum by making $\Psi$ a long sinusoidal wave, but in that case position measurements will be widely scattered.


%%%%%%%%%%%%%%%%%%%%%% Chapter 1.2
\chapter{Time Independent Schr\"{o}dinger Equation}


\section{Stationary States}

\begin{qst}
    How do we get $\Psi(x,t)$?
\end{qst}

As was characterized in the first chapter, we need to solve the Schr\"{o}dinger equation: $$i\hbar\frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2m}\frac{\partial^2\Psi}{\partial x^2} + V\Psi$$
for a specified potential $V(x,t)$. For this chapter we assume $V$ is independent of $t$. In this case we employ the method of \Emph{separation of variables}, looking for a solution of the form $$\Psi(x,t) = \psi(x)\varphi(t)$$
Although initially this will give us a really small subset of the solution space, the solutions we do obtain will be of great import, and we will hopefully be able to patch them together to obtain the most general solution.

Following this scheme we have $$\frac{\partial \Psi}{\partial t} = \psi\frac{d\varphi}{dt},\;\;\frac{\partial^2\Psi}{\partial^2x} = \frac{d^2\psi}{dx^2}\varphi$$
and the Schr\"{o}dinger equation reads $$i\hbar\psi\frac{d\varphi}{dt} = -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2}\varphi+V\psi\varphi$$
or dividing through by $\psi\varphi$, $$i\hbar\frac{1}{\varphi}\frac{d\varphi}{dt} = -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2}\frac{1}{\psi}+V$$
The left side is a function of $t$ along, and the right side is a function of $x$ alone. The only way this can possibly hold is if both sides are constant. Let us denote this separation constant by $E$: \begin{equation*}
    i\hbar\frac{1}{\varphi}\frac{d\varphi}{dt} = E,\;\;\implies\;\;\frac{d\varphi}{dt} = -\frac{iE}{\hbar}\varphi
\end{equation*}
and $$-\frac{\hbar^2}{2m}\frac{1}{\psi}\frac{d^2\psi}{dx^2}+V=E$$ or $$\boxed{-\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} + V\psi = E\psi}$$
The first of these is easy to solve, with $\varphi = C\exp\left\{-\frac{iEt}{\hbar}\right\}$ being the general solution, but we can just absorb the constant into $\psi$ since we are considering the product $\varphi\psi$. Then $$\varphi(t) = e^{-iEt/\hbar}$$ The second equation is called the \Emph{time-independent Schr\"{o}dinger equation}; we now need $V(x)$ to go further.

\begin{rmk}
    Separable solutions are interesting for three reasons:\begin{itemize}
        \item[(i)] They correspond to \Emph{stationary states}. Although the wave-function itsefl, $$\Psi(x,t) = \psi(x)e^{-iEt/\hbar}$$ depends on $t$, the probability density, $$|\Psi(x,t)|^2 = \Psi^*\Psi = |\psi(x)|^2$$ is indepdent of time. The same result occurs when calculating the expectation value of any dynamical variable: $$\langle Q(x,p)\rangle = \int\psi^*\left[Q\left(x,-i\hbar\frac{d}{dx}\right)\right]\psi dx$$
            In particular, $\langle x\rangle$ is constant, and hence $\langle p \rangle = m\frac{d\langle x\rangle}{dt} = 0$ (nothing ever happens in a stationary state)
        \item[(ii)] They are states of \emph{definite total energy}. The Hamiltonian operator, obtained from $H(x,p) = \frac{p^2}{2m}+V(x)$ obtained from the substitution $p\rightarrow -i\hbar(\partial/\partial x)$, is therefore $$\hat{H} = -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2} + V(x)$$
            Thus, the time-independent Schr\"{o}dinger equation can be written $$\hat{H}\psi = E\psi$$
            and the expecation value of the total energy is $$\langle H\rangle = \int \psi^*\hat{H}\psi dx = E\int|\psi|^2dx = E\int|\Psi|^2dx = E$$
            Moreover, $\hat{H}^2\psi = \hat{H}(E\psi) = E\hat{H}\psi = E^2\psi$, and hence $$\langle H^2\rangle = \int\psi^*\hat{H}^2\psi dx = E^2\int|\psi|^2dx = E^2$$ so the variance of $H$ is $$\sigma_H^2 = \langle H^2\rangle - \langle H\rangle^2 = 0$$
            Consequently, a separable solution has the property that every measurement of the total energy is certain to return the value $E$.
        \item[(iii)] The general solution is spanned by the separable solutions (i.e. is a linear combination of them). Indeed, since the time-independent solution is linear in $\psi$, every linear combination of solutions is again a solution. It so happens that every solution to the time-dependent Schr\"{o}dinger equation can be written in the form $$\Psi(x,t) = \sum_{n=1}^{\infty}c_n\psi_n(x)e^{-iE_nt/\hbar}$$ for suitable constants, $c_1,c_2,...$, and where $\psi_n$ is the time-independent solution with corresponding energy $E_n$.
    \end{itemize}
\end{rmk}

As we shall show later, the coefficients in the expansion of $\Psi(x,t)$ in terms of the $\psi_n(x)$ correspond to probabilities. In particular, $|c_n|^2$ is the probability that a measurement of the energy would return the value $E_n$. A competent measurement will always yield one of the ``allowed" values, and $|c_n|^2$ is the probability of getting a particular value $E_n$. Of course, as we shall later show, $\sum_{n=1}^{\infty}|c_n|^2 = 1$, and the expectation value of the energy must be $$\langle H\rangle =\sum_{n=1}^{\infty}|c_n|^2E_n$$
Notice that because the constants are independent of time, so too is the probability of getting a particular energy, and a fortiori, the expectation value of $H$. These are manifestations of \Emph{energy conservation} in quantum mechanics.




\section{The Infinite Square Well}


Suppose we consider a system with potential $$V(x) = \left\{\begin{array}{lc} 0, & 0 \leq x \leq a \\ \infty, & \text{otherwise} \end{array}\right.$$
A particle in this potential is completely free, except at the two ends ($x = 0$ and $x = a$), where an infinite force prevents it from escaping. 

Outside the well, $\phi(x) = 0$ (the probability of finding the particle there is zero). Inside the well, where $V = 0$, the time-independent Schr\"{o}dinger equation reads $$-\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} = E\psi$$
or $\frac{d^2\psi}{dx^2} = -k^2\psi$, where $k \equiv \frac{\sqrt{2mE}}{\hbar}$, assuming $E \geq 0$. This is the equation of the classical \Emph{simple harmonic oscillator}, the general solution of which is $$\psi = A\sin kx + B \cos kx$$ where $A$ and $B$ are arbitrary constants. Typically $A$ and $B$ are determined by boundary conditions. 

Ordinarily we require $\psi$ and $d\psi/dx$ to be continuous at the boundary, but where the potential goes to infinity only the first of these need apply. (note $\Psi(x,t)$ need not be a continuous function of $x$) Continuity equates to $\psi(0) = \psi(a) = 0$. This tells us that $B = \psi(0) = 0$, and hence $\psi(x) = A\sin kx$. Further, $0 = \psi(a) = A\sin ka$, so either $A = 0$, or $ka = \pi n$, for $n \in \N$. But $k = 0$ is not interesting as that would imply $\psi(x) = 0$, and by oddness of the sine function positives and negatives are inconsequential, so the distinct solutions are \begin{equation*}
    k_n = \frac{n\pi}{a}, \;\; n =1,2,3,...
\end{equation*}
Thus, the possible values of $E$ are \begin{equation*}
    \boxed{E_n = \frac{\hbar^2k_n^2}{2m} = \frac{n^2\pi^2\hbar^2}{2ma^2}}
\end{equation*}
and we arrive at a result with quantized energies. To find $A$ we normalize $\psi$: \begin{equation*}
    \int_0^a|A|^2\sin^2(kx)dx = |A|^2\frac{a}{2} = 1,\;\text{ so }\;|A|^2 = \frac{2}{a}
\end{equation*}
For simplicity we take the positive real root, $A = \sqrt{2/a}$. Inside the well, the solutions are \begin{equation*}
    \boxed{\psi_n(x) = \sqrt{\frac{2}{a}}\sin\left(\frac{n\pi}{a}x\right)}
\end{equation*}
The state corresponding to $n = 1$, with the lowest energy, is called the \Emph{ground state}, while all other states are called \Emph{excited states}.

\begin{prop}
    The collection of eigenfunctions $\psi_n(x)$ satisfy the following properties: \begin{itemize}
        \item They are alternately even and odd, with respect to the center of the well.
        \item Each successive state has one more \Emph{node} (zero-crossing/root) (note we don't count the endpoints, so $\psi_1$ has none)
        \item They are mutually orthogonal with respect to the standard inner product $$\int\psi_m(x)^*\psi_n(x)dx = \delta_{mn}$$
        \item They are \Emph{complete}, in the sense that any other function can be expressed as a linear combination of them: $$f(x) = \sum_{n=1}^{\infty}c_n\psi_n(x) = \sqrt{\frac{2}{a}}\sum_{n=1}^{\infty}c_n\sin\left(\frac{n\pi}{a}x\right)$$
            (this is simply a Fourier Series with odd periodic expansion for $f(x)$). We can compute the Fourier coefficients using the standard method for complete orthonormal sets: $$c_m = \braket{\psi_m}{f} = \int\psi_m(x)^*f(x)dx$$
    \end{itemize}
\end{prop}


The first property is true whenever the potential is symmetric, the second is universal for eigenfunctions, orthogonality is quite general, and completeness holds for all potentials often encountered physically.

The stationary states of the infinite square well are then $$\Psi_n(x,t) = \sqrt{\frac{2}{a}}\sin\left(\frac{n\pi}{a}x\right)e^{-i(n^2\pi^2\hbar/2ma^2)t}$$ Then the most general solution is of the form $$\Psi(x,t) = \sum_{n=1}^{\infty}c_n\sqrt{\frac{2}{a}}\sin\left(\frac{n\pi}{a}x\right)e^{-i(n^2\pi^2\hbar/2ma^2)t}$$
By completeness of the $\psi$'s, we may choose appropriate $c_n$ such that $\Psi(x,0) = \sum_{n=1}^{\infty}c_n\psi_n(x)$ given any initial conditions. These are precisely the coefficients that hold in general.


\section{The Harmonic Oscillator}


Recall from classical mechanics for formula known as \Emph{Hooke's law} for classical harmonic oscillators of mass $m$ and force constant $k$, $F = -kx = m\frac{d^2x}{dt^2}$, and the solution $x(t) = A\sin(\omega t) + B\cos(\omega t)$, where $\omega \equiv \sqrt{\frac{k}{m}}$ is the angular frequency of oscillation. The associated potential, $V(x) = \frac{1}{2}kx^2$, has graph a simple parabola.

We can generalize this to potentials about their minima. If $x_0$ is a minima of a potential $V$, $V'(x_0) = 0$ and we can adjust $V$ be a constant such that $V(x_0)$, so its Taylor series takes the form $$V(x) = \frac{1}{2}V''(x_0)(x-x_0)^2 + O((x-x_0)^3)$$
sufficiently close to the minima $V(x) \approx \frac{1}{2}V''(x_0)(x-x_0)^2$, which describes simple harmonic oscillation about the minima with spring constant $k = V''(x_0)$. 

Our problem now is to solve the Schr\"{o}dinger equation for the potential $$V(x) = \frac{1}{2}m\omega^2x^2$$
where $\omega = \sqrt{\frac{k}{m}}$. It suffices to find separable solutions, and so solve the time-independent Schr\"{o}dinger equation: $$-\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} + \frac{1}{2}m\omega^2x^2\psi = E\psi$$
We solve this problem in two ways.

\subsection{Ladder Operator Technique}

To begin, we rewrite the time-independent Schr\"{o}dinger equation as $$\frac{1}{2m}\left[\hat{p}^2 + (m\omega x)^2\right]\psi = E\psi$$
using $\hat{p} = -i\hbar\frac{\partial}{\partial x}$. The idea is to factor the Hamiltonian. But, as we are dealing with distinct operators $\hat{p}$ and $\hat{x}$, we don't necessarily have commutativity as we do for complex numbers, so we need to proceed cautiously. 

\begin{defn}
    We define the \Emph{ladder operators} to be $$\hat{a}_+ := \frac{1}{\sqrt{2\hbar m\omega}}(-i\hat{p} + m\omega x),\;\;\hat{a}_- := \frac{1}{\sqrt{2\hbar m\omega}}(+i\hat{p}+m\omega x)$$
\end{defn}
What are the products of these operators? $$\hat{a}_-\hat{a}_+ = \frac{1}{2\hbar m\omega}\left[\hat{p}^2+i\hat{p}m\omega x - im\omega x\hat{p} + (m\omega x)^2\right] = \frac{1}{2\hbar m\omega}\left[\hat{p}^2+(m\omega x)^2 - im\omega(x\hat{p} - \hat{p}x)\right]$$
and $$\hat{a}_+\hat{a}_- = \frac{1}{2\hbar m\omega}\left[\hat{p}^2-i\hat{p}m\omega x + im\omega x\hat{p} + (m\omega x)^2\right] = \frac{1}{2\hbar m\omega}\left[\hat{p}^2+(m\omega x)^2 + im\omega(x\hat{p} - \hat{p}x)\right]$$
In both cases we have the term $(x\hat{p}-\hat{p}x)$, the \Emph{commutator} of $x$ and $\hat{p}$, denoted $[\hat{x},\hat{p}]$, for $\hat{x} = x$. In this notation $$\hat{a}_-\hat{a}_+ = \frac{1}{2\hbar m\omega}[\hat{p}^2+(m\omega x^2)] - \frac{i}{2\hbar}[\hat{x},\hat{p}]$$
and $$\hat{a}_+\hat{a}_- = \frac{1}{2\hbar m\omega}[\hat{p}^2+(m\omega x^2)] + \frac{i}{2\hbar}[\hat{x},\hat{p}]$$
Let $f(x)$ be an arbitrary function in the Hilbert space. It follows that \begin{align*}
    [\hat{x},\hat{p}]f(x) &= \left[x(-i\hbar)\frac{d}{dx}(f) - (-i\hbar)\frac{d}{dx}(xf)\right] \\
    &= -i\hbar\left[xf'(x) - xf'(x) - f(x)\right] = i\hbar f(x)
\end{align*}
so $$[\hat{x},\hat{p}] = i\hbar$$
this is often called the \Emph{canonical commutation relation}. With this equation, going back we have $$\hat{a}_-\hat{a}_+ = \frac{1}{\hbar \omega}\hat{H} + \frac{1}{2},\;\;\hat{a}_+\hat{a}_- = \frac{1}{\hbar \omega}\hat{H} - \frac{1}{2}$$
so $$\hat{H} = \hbar\omega\left(\hat{a}_-\hat{a}_+ - \frac{1}{2}\right) = \hbar \omega\left(\hat{a}_+\hat{a}_- + \frac{1}{2}\right)$$
Observe that $[\hat{a}_-,\hat{a}_+] = 1$. Then the Schr\"{o}dinger equation for the harmonic oscillator can be written $$\hbar \omega \left(\hat{a}_{\pm}\hat{a}_{\mp}\pm\frac{1}{2}\right)\psi = E\psi$$

\begin{prop}
    If $\psi$ satisfies the Schr\"{o}dinger equation with energy $E$, then $\hat{a}_{\pm}\psi$ satisfies the Schr\"{o}dinger equation with energy $E\pm\hbar\omega$.
\end{prop}
\begin{proof}
    Observe \begin{align*}
        \hat{H}(\hat{a}_{\pm}\psi) = \hbar\omega\left(\hat{a}_{\pm}\hat{a}_{\mp} \pm \frac{1}{2}\right)(\hat{a}_{\pm}\psi) \\
        &= \hbar \omega\hat{a}_{\pm}\left(\hat{a}_{\mp}\hat{a}_{\pm} \pm1 \mp\frac{1}{2}\right)\psi \\
        &= \left(E \pm \hbar\omega\right)(\hat{a}_{\pm}\psi)
    \end{align*}
    so $$\hat{H}(\hat{a}_+\psi) = (E + \hbar\omega)(\hat{a}_+\psi),\;\;\hat{H}(\hat{a}_-\psi) = (E-\hbar\omega)(\hat{a}_-\psi)$$
\end{proof}

We call $\hat{a}_+$ the \Emph{raising operator} and $\hat{a}_-$ the \Emph{lowering operator} for this reason.

\begin{qst}
    What if I apply the lowering operator repeatedly?
\end{qst}

At some point the machine must fail, but how can that happen? We know $\hat{a}_-\psi$ is a new solution, but there is no guarantee it will be normalizable-it may be zero, or have infinite integral. In practice it is the former. There exists $\psi_0$ such that $\psi_0 \neq 0$, but $$\hat{a}_-\psi_0 = 0$$
We can determine $\psi_0(x)$ by $$\frac{1}{\sqrt{2\hbar m\omega}}\left(\hbar\frac{d}{dx} + m\omega x\right)\psi_0 = 0$$
or $\frac{d\psi_0}{dx} = -\frac{m\omega}{\hbar}x\psi_0$. By separation of variables, $$\psi_0(x) = Ae^{-\frac{m\omega}{2\hbar}x^2}$$
and normalizing $$1 = |A|^2\int_{-\infty}^{\infty}e^{-m\omega x^2/\hbar}dx = |A|^2\sqrt{\frac{\pi\hbar}{m\omega}}$$
so $A^2 = \sqrt{m\omega/\pi\hbar}$, and hence \begin{equation*}
    \boxed{\psi_0(x) = \left(\frac{m\omega }{\pi\hbar}\right)^{1/4}e^{-\frac{m\omega}{2\hbar}x^2}}
\end{equation*}
To determine the energy, we plug into the Schr\"{o}dinger equation, $\hbar\omega(\hat{a}_+\hat{a}_- +1/2)\psi_0 = E_0\psi_0$, which implies $$E_0 = \frac{1}{2}\hbar\omega$$
This is the \Emph{ground state} energy of the quantum harmonic oscillator. Then, for each $n \in \N$, $$\psi_n(x) = A_n(\hat{a}_+)^n\psi_0(x),\;\text{ with }\;E_n = \left(n+\frac{1}{2}\right)\hbar\omega$$

We can also use this to obtain normalization. Note $\hat{a}_{\pm}\psi_n \propto \psi_{n\pm 1}$, so $\hat{a}_+\psi_n = c_n\psi_{n+1}$ and $\hat{a}_-\psi_n = d_n\psi_{n-1}$.

\begin{prop}
    $\hat{a}_{\mp}$ is the \Emph{hermitian conjugate}, or \Emph{adjoint} of $\hat{a}_{\pm}$.
\end{prop}
\begin{proof}
    Observe \begin{equation*}
        \int_{-\infty}^{\infty}f^*\cdot(\hat{a}_{\pm}g)dx = \frac{1}{\sqrt{2\hbar m\omega}}\int_{-\infty}^{\infty}f^*\cdot \left(\mp \hbar\frac{d}{dx} + m\omega x\right)gdx
    \end{equation*}
    and integration by parts takes $\int f^*\cdot (dg/dx)dx$ to $-\int(df/dx)^*gdx$. So \begin{align*}
        \int_{-\infty}^{\infty}f^*\cdot (\hat{a}_{\pm}g)dx &= \frac{1}{\sqrt{2\hbar m\omega}}\int_{-\infty}^{\infty}\left[\left(\pm\hbar\frac{d}{dx} + m\omega x\right)f\right]^*gdx \\
        &= \int_{-\infty}^{\infty}(\hat{a}_{\mp}f)^*gdx
    \end{align*}
\end{proof}

It follows that $$\braket{\hat{a}_{\pm}\psi_n}{\hat{a}_{\pm}\psi_n} = \braket{\hat{a}_{\mp}\hat{a}_{\pm}\psi_n}{\psi_n}$$
Using $\hat{a}_+\hat{a}_-\psi_n = n\psi_n$ and $\hat{a}_-\hat{a}_+\psi_n = (n+1)\psi_n$, so $$\braket{\hat{a}_+\psi_n}{\hat{a}_+\psi_n} = (n+1) = |c_n|^2$$ and $$\braket{\hat{a}_-\psi_n}{\hat{a}_-\psi_n} = n = |d_n|^2$$
Thus, \begin{equation*}
    \boxed{\hat{a}_+\psi_n = \sqrt{n+1}\psi_{n+1},\;\;\hat{a}_-\psi_n = \sqrt{n}\psi_{n-1}}
\end{equation*}
In general, $$\boxed{\psi_n = \frac{1}{\sqrt{n!}}(\hat{a}_+)^n\psi_0}$$
As in the square well-case, we have orthonormality - this follows from the fact that these $\psi_n$ are eigenfunctions of $\hat{a}_+\hat{a}_-$, which is Hermitian being the product of an operator and its adjoint, with eigenvalue $n$, so for $\psi_n,\psi_m$, $m \neq n$, the distinct eigenvalues (with the discrete spectra) imply orthonormality of the result. Then we can expand $\Psi(x,0)$ as a linear combination of stationary states using Fourier coefficients $c_n$. As before, $|c_n|^2$ is the probability that a measurement of the energy would yield the value $E_n$.





\section{The Free Particle}


We now consider the free particle ($V(x) = 0$ everywhere). The time-independent Schr\"{o}dinger equation now reads \begin{equation*}
    -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} = E\psi
\end{equation*}
or \begin{equation*}
    \frac{d^2\psi}{dx^2} = -k^2\psi,\;\text{ where }k := \frac{\sqrt{2mE}}{\hbar}
\end{equation*}
Writing the solution in exponential form we have $$\psi(x) = Ae^{ikx} + Be^{-ikx}$$
Since there are no boundary conditions, the free particle can carry any positive energy. Tacking on the standard time dependence. $\exp(-iEt/\hbar)$, \begin{equation*}
    \Psi(x,t) = Ae^{ik\left(x-\frac{\hbar k}{2m}t\right)} + Be^{-ik\left(x+\frac{\hbar k}{2m}t\right)}
\end{equation*}
Note any function of $x$ and $t$ that depends on these variables in the special combination $x\pm vt$ for some constant $v$ represents a wave of unchanging shape, traveling in the $\mp x$ direction at speed $v$. You can think of it as a fixed point on the waveform (for example a maxima or minima) corresponding to the argument being constant, $x\pm vt = $ const, or equivalently $x = \mp vt + $ const.
Thus, the first term represents a wave traveling to the right while the second represents a wave traveling to the left.

We could also write this as \begin{equation*}
    \Psi_k(x,t) = Ae^{i\left(kx - \frac{\hbar k^2}{2m}t\right)}
\end{equation*}
where now $k$ can be negative: \begin{equation*}
    k := \left\{\begin{array}{cc} \frac{\sqrt{2mE}}{\hbar} & \text{traveling to the right} \\ -\frac{\sqrt{2mE}}{\hbar} & \text{traveling to the left} \end{array}\right.
\end{equation*}

The `stationary states' of the free particle are propagating waves with wavelength $\lambda = 2\pi/|k|$, and according to the de Broglie formula carrying momentum $$p = \hbar k$$
The speed of these waves (the coefficient of $t$) is \begin{equation*}
    \boxed{v_{quantum} = \frac{\hbar|k|}{2m} = \sqrt{\frac{E}{2m}}}
\end{equation*}
On the other hand, the \emph{classical} speed of a free particle with energy $E$ is given by $E = \frac{1}{2}mv^2$ (pure kinetic, since $V = 0$), so $$v_{classical} = \sqrt{\frac{2E}{m}} = 2v_{quantum}$$
We'll return to this discrepancy in a moment. First we must address the following issue:
\begin{equation*}
    \text{\emph{This wave function is not normalizable}}
\end{equation*}
\begin{equation*}
    \int_{-\infty}^{\infty}\Psi^*_k\Psi_kdx = |A|^2\int_{-\infty}^{\infty}dx = |A|^2(\infty)
\end{equation*}
Consequently, the separable solutions do not represent physically realizable states in this case. A free particle cannot exist in a stationary state, or to put it another way, there is no such thing as a free particle with a definite energy.

But, we can still construct the general colution to the time-dependent Schr\"{o}dinger equation as a linear combination of separable solutions, only this time it is over a continuous variable $k$:\begin{equation*}
    \boxed{\Psi(x,t) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\phi(k)e^{i\left(kx-\frac{\hbar k^2}{2m}t\right)}dk}
\end{equation*}
For appropriate $\phi(k)$ this can be normalized. But it necessarily carries a range of $k$s, and hence a range of energies and speeds. We call it a \Emph{wave packet}.

In general we are given $\Psi(x,0)$, and want to find $\Psi(x,t)$. For a free particle the only question is how to determined $\phi(k)$ so as to match the initial wave function: \begin{equation*}
    \Psi(x,0) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\phi(k)e^{ikx}dk
\end{equation*}
This is classically solved by the Inverse Fourier transform: \begin{equation*}
    \boxed{\phi(k) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\Psi(x,0)e^{-ikx}dx}
\end{equation*}

The issue of $\Psi_k(x,t)$ not traveling at the classical speed vanishes when we realize it is not a physically realizable state. Nonetheless, we can still derive information about the particle velocity from the wave function. Note a wave packet is a superposition of sinusoidal functions whose amplitude is modulated by $\phi$ (ripples contained in an envolope). What corresponds to the particle velocity is not the speed of the individual ripples (the \Emph{phase velocity}), but rather the speed of the envelope (the \Emph{group velocity}). Depending on the wave the group velocity can be greater than, less than, or equal to the phase velocity. For a wave packet of the generic form \begin{equation*}
    \Psi(x,t) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\phi(k)e^{i(kx-\omega t)}dk
\end{equation*}
where $\omega = \hbar k^2/2m$ is the \Emph{dispersion relation} in our case. Assume $\phi(k)$ is narrowly peaked about some particular value $k_0$ (for broad spreads in $k$, the wave packets change shape rapidly so there isn't necessarily a well defined notion of a ``group"). We Taylor expand about $k_0$, giving $\omega(k)\approx \omega_0 + \omega_0'(k-k_0)$, where $\omega_0' = \omega'(k_0)$. Changing variables to $s\equiv k-k_0$, we have \begin{align*}
    \Psi(x,t) &\approx \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\phi(k_0+s)e^{i[(k_0+s)x-(\omega_0+\omega_0')t]}ds \\
    &= \frac{1}{\sqrt{2\pi}}e^{i(k_0x-\omega_0t)}\int_{-\infty}^{\infty}\phi(k_0+s)e^{is(x-\omega_0't)}ds
\end{align*}
The term in front is a sinusoidal wave (the ripples), traveling at speed $\omega_0/k_0$. It is modulated by the integral (the envolope), which is a function of $x-\omega_0't$, and therefore propagates at the speed $\omega_0'$. Thus the phase velocity is \begin{equation*}
    \boxed{v_{phase} = \frac{\omega}{k}}
\end{equation*}
while the group velocity is \begin{equation*}
    \boxed{v_{group} = \frac{d\omega}{dk}}
\end{equation*}
both evaluated at $k_0$.

In our case of $\omega = \hbar k^2/2m$, this gives $\omega/k = \hbar k/2m$ and $d\omega/dk = \hbar k/m$, confirming that the group velocity of the wave packet matches the classical particle velocity.



\section{The Delta-Function Potential}

\subsection{Bound States and Scattering States}

Recall in classical mechanics that if the energy of a particle is less than the potential at some points, it is in a \Emph{bound state}, and oscillates between the turning points at which $E = V(x)$. When this is not the case and the particle is free to fly either to plus or minus infinity, or both, it is said to be in a \Emph{scattering state}. 

In quantum mechanics the particle may ``leak" through a finite potential barrier, even if it is classically bound, exhibiting the phenomena of \Emph{tunneling}, so all that matters is the potential at infinity: \begin{equation*}
    \left\{\begin{array}{c} E < V(-\infty)\text{ and }V(+\infty)\implies\text{ bound state} \\ E>V(-\infty)\text{ or }V(+\infty)\implies\text{ scattering state}\end{array}\right.
\end{equation*}


\subsection{The Delta-Function Well}

\begin{defn}
    The \Emph{dirac delta function} is characterized by $\delta(x) = 0, x\neq 0$ and $\delta(x) = \infty$ if $x = 0$, with $$\int_{-\infty}^{\infty}\delta(x)dx = 1$$
\end{defn}

Let's consider a potential of the form \begin{equation*}
    V(x) = -\alpha\delta(x)
\end{equation*}
where $\alpha$ is some positive constant. The Schr\"{o}dinger equation reads \begin{equation*}
    -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2}-\alpha\delta(x)\psi = E\psi
\end{equation*}
It yields both bound states $(E < 0)$, and scattering states $(E > 0)$.

First consider bound states. In the region $x < 0$, $V(x) = 0$ so \begin{equation*}
    \frac{d^2\psi}{dx^2} = -\frac{2mE}{\hbar^2}\psi = \kappa^2\psi
\end{equation*}
for $\kappa = \frac{\sqrt{-2mE}}{\hbar}$, with general solution \begin{equation*}
    \psi(x) = Ae^{-\kappa x}+Be^{\kappa x}
\end{equation*}
As it must go to zero when $x\rightarrow -\infty$, we must have $A = 0$, so $$\psi(x) = Be^{\kappa x}, (x < 0)$$
Similarly, for $x > 0$ $V(x)$ is again $0$ so $$\psi(x) = Fe^{-\kappa x}, (x > 0)$$
We now apply the standard boundary conditions: \begin{equation*}
    \boxed{\left\{\begin{array}{ccc} 1. & \psi & \text{ is always continuous} \\ 2. & d\psi/dx & \text{ is continuous except at points where $V = \infty$} \end{array}\right.}
\end{equation*}
The first condition tells us $F = B$. The second condition we can't apply. To look at the discontinuity of $d\psi/dx$ at $0$ we integrate the Schr\"{o}dinger equation from $-\epsilon$ to $+\epsilon$, and take the limit \begin{equation*}
    -\frac{\hbar^2}{2m}\int_{-\epsilon}^{+\epsilon}\frac{d^2\psi}{dx^2}dx + \int_{-\epsilon}^{+\epsilon}V(x)\psi(x)dx = E\int_{-\epsilon}^{+\epsilon}\psi(x)dx
\end{equation*}
In the limit as $\epsilon \rightarrow 0$, \begin{equation*}
    \Delta\left(\frac{d\psi}{dx}\right) :=\lim\limits_{\epsilon\rightarrow 0}\left(\frac{\partial\psi}{\partial x}\Bigg\rvert_{+\epsilon} - \frac{\partial \psi}{\partial x}\Bigg\rvert_{-\epsilon}\right) = \frac{2m}{\hbar^2}\lim\limits_{\epsilon\rightarrow 0}\int_{-\epsilon}^{+\epsilon}V(x)\psi(x)dx
\end{equation*}
By definition of the $\delta$ function we have that \begin{equation*}
    \boxed{\Delta \left(\frac{d\psi}{dx}\right) = -\frac{2m\alpha}{\hbar^2}\psi(0)}
\end{equation*}
We can also compute $d\psi/dx\vert_+ = -B\kappa$ and $d\psi/dx\vert_- = +B\kappa$, so $\Delta(d\psi/dx) = -2B\kappa$. Thus, equating these terms we have \begin{equation*}
    \kappa = \frac{m\alpha}{\hbar^2}
\end{equation*}
and the allowed energy is $$E = -\frac{m\alpha^2}{2\hbar^2}$$
Finally, normalizing $\psi$ we have $\frac{|B|^2}{\kappa} = 1$, so \begin{equation*}
    B = \sqrt{\kappa} = \frac{\sqrt{m\alpha}}{\hbar}
\end{equation*}
Evidently we have exactly one bound state:\begin{equation*}
    \boxed{\psi(x) = \frac{\sqrt{m\alpha}}{\hbar}e^{-m\alpha|x|/\hbar^2};\;\;E=-\frac{m\alpha^2}{2\hbar^2}}
\end{equation*}


Now we address scattering states where $E > 0$. For $x < 0$ and $x > 0$ the equation reads $\frac{d^2\psi}{dx^2} = -k^2\psi$ for $\psi = \frac{\sqrt{2mE}}{\hbar}$, with general solutions $\psi(x) = Ae^{ikx} +Be^{-ikx}$ and $\psi(x) = Fe^{ikx} + Ge^{-ikx}$ for $x < 0$ and $x > 0$ respectively. The continuity at $x = 0$ requires $$F+G = A+B$$
For the derivatives we have $d\psi/dx\vert_+ = ik(F-G)$ and $d\psi/dx\vert_- = ik(A-B)$, so $\Delta(d\psi/dx) = ik(F-G-A+B)$. Meanwhile $\psi(0) = A+B$, so the second boundary condition says \begin{equation*}
    ik(F-G-A+B) = -\frac{2m\alpha}{\hbar^2}(A+B)
\end{equation*}
or more compactly \begin{equation*}
    F-G = A(1+2i\beta)-B(1-2i\beta),\text{ where }\beta :=\frac{m\alpha}{\hbar^2k}
\end{equation*}
We now have two equations and four unknowns (five counting $k$). Note this is not a normalizable state. Instead of proceeding with this, we look at the physical significance of the constants. Combining with the wiggle factor we note $A$ is the amplitude of a wave coming in from the left, $B$ is the amplitude of a wave returning to the left, $F$ is the amplitude of a wave traveling off to the right, and $G$ is the amplitude of a wave coming in from the right. In typical scattering we investigate a source from one direction. Let's say it is coming in from the left. In that case the amplitude of the wave coming in from the right will be zero, $G = 0$. $A$ is the amplitude of the \Emph{incident wave}, $B$ is the amplitude of the \Emph{reflected wave}, and $F$ is the amplitude of the \Emph{transmitted wave}. Solving our equations for $B$ and $F$ we find \begin{equation*}
    B = \frac{i\beta}{1-i\beta}A,\;\;F = \frac{1}{1-i\beta}A
\end{equation*}

The probability of finding a particle at a specific location is given by $|\Psi|^2$, so the relative probability that an incident particle will be reflected back is \begin{equation*}
    R = \frac{|B|^2}{|A|^2} = \frac{\beta^2}{1+\beta^2}
\end{equation*}
called the \Emph{reflection coefficient}. Meanwhile, the probability that a particle will continue right on through it the \Emph{transmission coefficient}:\begin{equation*}
    T = \frac{|F|^2}{|A|^2} = \frac{1}{1+\beta^2}
\end{equation*}
Note the sum equals $1$: $R+T = 1$. Written explicitly we have \begin{equation*}
    \boxed{R=\frac{1}{1+(2\hbar^2E/m\alpha^2)},\;\;T=\frac{1}{1+(m\alpha^2/2\hbar^2E)}}
\end{equation*}

As $E$ increases, so does $T$ (the probability of transmission). But note that this is all analysis of the stationary states, which are not normalizable and hence do not actually represent physical particles. If we flip the well into a barrier the bound state is killed, but the reflection and transmission coefficients remain unchanged.


\section{The Finite Square Well}

Now we consider the example of a finite square well: \begin{equation*}
    V(x) = \left\{\begin{array}{cc} -V_0 & -a\leq x \leq a \\ 0 & |x| > 0 \end{array}\right.
\end{equation*}
where $V_0 > 0$ is constant. This admits both bound ($E < 0)$ and scattering $(E > 0)$ states.

Let us consider $E < 0$ first. For $x < -a$ we are in a similar solution to the infinite case with $\psi(x) = Be^{\kappa x}$ where $\kappa = \frac{\sqrt{-2mE}}{\hbar}$. In the region $-a < x < a$, $V(x) = -V_0$, and the wave equation reads \begin{equation*}
    \frac{d^2\psi}{dx^2} = -l^2\psi
\end{equation*}
where $l = \frac{\sqrt{2m(E+V_0)}}{\hbar}$. Although $E < 0$, we require $E > -V_0$, so $E+V_0 > 0$. The general solution is $$\psi(x) = C\sin(lx) + D\cos(lx),\;(-a < x < a)$$
Finally in $x > 0$ we again have $\psi(x) = Fe^{-\kappa x}$ like the infinite case for positive $x$. We now impose our boundary conditions at $\pm a$. Note our potential is an even function so the solution is either even or odd. For an even solution \begin{equation*}
    \psi(x) = \left\{\begin{array}{cc} Fe^{-\kappa x}, & x  > a \\ D\cos(lx) & 0 < x < a \\ \psi(-x) & x < 0 \end{array}\right.
\end{equation*}
By continuity of $\psi$ at $a$ $Fe^{-\kappa a} = D\cos(la)$ and by continuity of $d\psi/dx$ $-\kappa Fe^{-\kappa a} = -lD\sin(la)$. Dividng these we find $$\kappa = l\tan(la)$$
According to these equations $(\kappa^2+l^2) = 2mV_0/\hbar^2$, so $\kappa a = \sqrt{2mV_0a^2/\hbar^2 - l^2a^2}$, and $$\tan(la) = \sqrt{(\sqrt{2mV_0}/\hbar l)^2-1}$$
Writing $z = la$ and $z_0 = \frac{a}{\hbar}\sqrt{2mV_0}$ we can write $\tan(z) = \sqrt{(z_0/z)^2-1}$. Two limiting cases of solutions to this equation are of interest:
\begin{itemize}
    \item Wide deep well: If $z_0$ is very large, the intersections occur just slightly below $z_n = n\pi/2$ with $n$ odd. It follows that $$E_n+V_0\approx \frac{n^2\pi^2\hbar^2}{2m(2a)^2}$$
        So the finite square well goes over to the infinite square well, as $V_0\rightarrow \infty$ (however for any finite $V_0$ there are only a finite number of bound states)
    \item Shallow narrow well: As $z_0$ decreases, the number of bound states decreases, until for $z_0 < \pi/2$ only one remains.
\end{itemize}

For the scattering states $E > 0$, to the left we have $$\psi(x) = Ae^{ikx} + Be^{-ikx}$$
where $k = \frac{\sqrt{2mE}}{\hbar}$. Inside the well where $V(x) = -V_0$, $$\psi(x) = C\sin(lx) + D\cos(lx),\;\;(-a < x < a)$$
where, as before, \begin{equation*}
    l = \frac{\sqrt{2m(E+V_0)}}{\hbar}
\end{equation*}
To the right, assuming no incoming wave in this region, we have $\psi(x) = Fe^{ikx}$. Here $A$ is the incident amplitude, $B$ is the reflected amplitude, and $F$ is the transmitted amplitude. Applying the boundary conditions, we have $$Ae^{-ika} + Be^{ika} = -C\sin(la) + D\cos(la)$$ 
for continuity at $-a$, $$ik\left[Ae^{-ika} -Be^{ika}\right] = l[C\cos(la)+D\sin(la)]$$
for continuity of $d\psi/dx$ at $-a$, $$C\sin(la)+D\cos(la) = Fe^{ika}$$
for continuity of $\psi$ at $a$, and $$l[C\cos(la)-D\sin(la)] = ikFe^{ika}$$
for continuity of $d\psi/dx$ at $a$.
We can solve for $B$ and $F$ in terms of $A$ as follows: \begin{align*}
    B &= i\frac{\sin(2la)}{2kl}(l^2-k^2)F \\
    F &= \frac{e^{-2ika}A}{\cos(2la)-i\frac{(k^2+l^2)}{2kl}\sin(2la)}
\end{align*}
The transmission coefficient, $T= |F|^2/|A|^2$, expressed in the original variables is given by $$T^{-1} = 1+\frac{V_0^2}{4E(E+V_0)}\sin^2\left(\frac{2a}{\hbar}\sqrt{2m(E+V_0)}\right)$$
Note $T = 1$ whenever the sine is zero, which is to say whenever $$\frac{2a}{\hbar}\sqrt{2m(E_n+V_0)} = n\pi$$
The energies for perfect transmission are then given by \begin{equation*}
    E_n+V_0 =\frac{n^2\pi^2\hbar^2}{2m(2a)^2}
\end{equation*}
which happen to be precisely the allowed energies for the infinite square well.


%%%%%%%%%%%%%%%%%%%%%% Chapter 1.3
\chapter{Formalism}

\section{Hilbert Space}

The Quantum theory is based on two constructs: wave functions and operators. 

\begin{defn}
    The \Emph{state} of a system is represented by its wave function, while \Emph{observables} are represented by operators.
\end{defn}

We note that wave functions can be considered as abstract vectors, while operators act as their linear transformations.

\begin{defn}
    In an $N$ dimensional space $k^N$ we will denote a vector, seen as a column vector, by $\ket{\alpha}$. We also denote the conjugate transpose of a vector by the notation $\bra{\alpha} = \alpha^*$. It follows that the complex inner product of two vectors is simply $$\braket{\alpha}{\beta} = \sum_{i=1}^Na_i^*b_i$$
\end{defn}

But, vectors in quantum mechanics are often functions, and hence live in infinite-dimensional spaces. Now, we don't want all single variate functions of $x$, as this will be much too large and not physically meaningfull. Indeed, to represent a possible physical state we need at minimum that a wave function $\Psi$ must be \Emph{normalized}: $$\int|\Psi|^2dx = 1$$ The set of all \Emph{square-integrable} functions, on a specified interval, constitute a much smaller vector space. Mathematicians call it $L^2(a,b)$, for fixed $a<b \in \R$; physicists call it \Emph{Hilbert space}. 
\begin{rmk}
    In quantum mechanics, \Emph{wave functions live in Hilbert space}.
\end{rmk}

\begin{defn}
    We define the \Emph{inner product of two functions}, $f(x)$ and $g(x)$, in Hilbert space, $L^2(a,b)$, by $$\braket{f}{g} := \int_a^bf(x)^*g(x)dx$$
\end{defn}

\begin{thm}[Schwarz Inequality]
    If $V$ is a complex inner product space with inner product $\langle,\rangle$, then for any $a,b \in V$ we have the inequality $$|\langle a,b\rangle|^2 \leq \langle a,a\rangle\langle b,b\rangle$$
\end{thm}
\begin{proof}
    Let $a,b \in V$, and without loss of generality suppose $a, b \neq 0_V$. Then let $t \in \R\backslash\{0\}$. It follows that $$0 \leq \langle ta-t^{-1}b,ta-t^{-1}b\rangle = t^2\langle a,a\rangle - \langle a,b\rangle - \langle a,b\rangle^* + t^{-2}\langle b,b\rangle = t^2\langle a,a\rangle - 2Re(\langle a,b\rangle)+t^{-2}\rangle b,b\rangle$$ Then set $t^2 = \frac{\sqrt{\langle b,b\rangle}}{\sqrt{\langle a,a\rangle}}$. We then observe that $$2Re(\langle a,b\rangle) \leq 2\sqrt{\langle a,a\rangle}\sqrt{\langle b,b\rangle}$$ Note $\langle a,b\rangle = e^{i\theta}|\langle a,b\rangle|$. Then, replacing $a$ with $ae^{i\theta}$ in the inequality, the right side remains unchanged since $|e^{i\theta}| = 1$, and we have $$|\langle a,b\rangle| = Re(e^{-i\theta}\langle a,b\rangle) \leq \sqrt{\langle a,a\rangle}\sqrt{\langle b,b\rangle}$$ and so squaring both sides we obtain the desired inequality: $$|\langle a,b\rangle|^2 \leq \langle a,a\rangle \langle b,b\rangle$$
\end{proof}

If $f$ and $g$ are both square integrable, their inner product is guarenteed to exist. This follows from the integral Schwarz inequality (note we have not yet proven that $\braket{f}{g}$ is an inner product on the Hilbert space, so we can't use our proof above). After proving $$\left|\int_a^bf(x)^*g(x)dx\right| \leq \sqrt{\int_a^b|f(x)|^2dx \int_a^b|g(x)|^2dx}$$ it will follow that $\braket{\cdot}{\cdot}$ is indeed a complex inner product on Hilbert space - however, we note that we may have a function which is zero at all but a set of measure zero, in which case it is square integrable with inner product zero; thus, technically Hilbert space is a space of equivalence classes of functions which agree everywhere, accept possibly on a set of measure zero.

\begin{defn}
    A function is said to be \Emph{normalized} if its inner product with itself is $1$; two functions are \Emph{orthogonal} if their inner product is $0$; and a set of functions $\{f_n\}$ is \Emph{orthonormal} if they are normalized and mutually orthogonal, i.e. $$\braket{f_m}{f_n} = \delta_{mn}$$
\end{defn}

\begin{defn}
    A set of functions is \Emph{complete} if any other function (in Hilbert space) can be expressed as a linear combination of them: $$f(x) = \sum_{n=1}^{\infty}c_nf_n(x)$$ That is to say, it is complete if the span of $\{f_n\}$ is dense in Hilbert space.
\end{defn}

If the functions $\{f_n(x)\}$ are orthonormal, we have the Fourier coefficients $$c_n = \frac{\braket{f_n}{f}}{\braket{f_n}{f_n}} = \braket{f_n}{f}$$


\section{Observables}

\begin{defn}
    The \Emph{expectation value} of an observable $Q(x,p)$ can be expressed neatly in the inner product notation $$\langle Q \rangle = \int\Psi^*\hat{Q}\Psi dx = \braket{\Psi}{\hat{Q}\Psi}$$ where $\hat{Q}$ is the operator constructed from $Q$ by replacing $p$ with $-i\hbar\frac{d}{dx}$. Note $\hat{Q}$ is linear on Hilbert space, but it may send a function from Hilbert space outside it, in which case the domain of the operator may need to be restricted.
\end{defn}

The outcome of a measurement has to be real, and so, a foriori, the average of many measurements must be real as well: $$\langle Q\rangle = \langle Q\rangle^*$$
This implies that $$\braket{\Psi}{\hat{Q}\Psi} = \braket{\hat{Q}\Psi}{\Psi}$$ which implies that $\hat{Q}$ is self-adjoint, since this must hold for any wavefunction $\Psi$. An operator which is self-adjoint in a complex inner product space is called \Emph{Hermitian}.

\begin{defn}
    An operator $T:V\rightarrow V$ on a complex inner product space is said to be self-adjoint if $T = T^*$, where $T^*$ is the unique linear operator satisfying $$\langle Tv,w\rangle = \langle v,T^*w\rangle$$ for all $v,w \in V$.
\end{defn}
Although our notion of self-adjoint for observables is equivalent in complex inner product spaces.

\begin{rmk}
    Thus, we have the conclusion that \Emph{Observables are represented by hermitian operators}.
\end{rmk}

For example, we can show that the momentum operator is Hermitian: $$\braket{f}{\hat{p}g} = \int_{-\infty}^{\infty}f(x)^*(-i\hbar)\frac{dg}{dx}dx = -i\hbar f(x)^*g(x)\Big\rvert_{-\infty}^{\infty} + \int_{-\infty}^{\infty}\left(-i\hbar\frac{df}{dx}(x)\right)^*g(x)dx = \braket{\hat{p}f}{g}$$ Note that the integration by parts term vanishes since $f(x)$ and $g(x)$ are square integrable and so must go to zero at $\pm\infty$ (for the functions of concern in physics). 

\begin{defn}
    In general, the \Emph{hermitian conjugate}, or \Emph{adjoint} of a function operator $\hat{Q}$ is denoted $\hat{Q}^{\text{\textdagger}}$, and defined implicitly by $$\braket{f}{\hat{Q}g} = \braket{\hat{Q}^{\text{\textdagger}}f}{g}$$ for all $f$ and $g$.
\end{defn}

This is the same definition as before for general inner product spaces.


\subsection{Determinate States}

Recall that ordinarily, when you measure an observable $Q$ on an ensemble of identically prepared systems, all in the same state $\Psi$, you do not get the same result each time, which is the \Emph{indeterminacy} of quantum mechanics. But, if we can prepare a state such that every measurement $Q$ is certain to return the same value, $q$, we call such a state a \Emph{determinate state}, for the observable $Q$. Stationary states are an example of determinate states of the Hamiltonian operator.

The standard deviation of an observable $Q$, in a determinate state, should be zero, so $$\sigma^2 = \left\langle (Q-\langle Q\rangle)^2\right\rangle = \braket{\Psi}{(\hat{Q}-q)^2\Psi} = \braket{(\hat{Q}-q)\Psi}{(\hat{Q}-q)\Psi} = 0$$ (recall observables are Hermitian operators, and $\langle Q \rangle = q$ is real). But, the only vector whose inner product with itself vanishes (for our physical functions) is $0$, so $$\hat{Q}\Psi = q\psi$$ 

\begin{defn}
    We say that $q \in \R$ is an \Emph{eigenvalue} with associated non-zero \Emph{eigenfunction} $\Psi$ for an operator $\hat{Q}$ on Hilbert space, if $$\hat{Q}\Psi = q\Psi$$ 
\end{defn}

Thus, we have that \Emph{determinate states of $Q$ are eigenfunctions of $\hat{Q}$}. Measurement of $Q$ on such a state is certain to yield the eigenvalue $q$.

\begin{defn}
    The collection of all eigenvalues of an operator is called its \Emph{spectrum}. The spectrum is said to be \Emph{degenerate} if two or more linearly independent eigenfunctions share the same eigenvalue.
\end{defn}

\section{Eigenfunctions of a Hermitian Operator}

Eigenfunctions of hermitian operators fall into two categories: if the spectrum is \Emph{discrete}, then the eigenfunctions lie in Hilbert space and they constitute physically realizable states; if the spectrum is \Emph{continuous}, then the eigenfunctions are not normalizable, and they do not represent possible wave functions (though linear combinations of them involving a spread of eigenvalues may be normalizable). An operator may have a discrete spectra, a continuous spectra, or have a discrete part and a continuous part.

\subsection{Discrete Spectra}

The normalizable eigenfunctions of a hermitian operator have two important properties:

\begin{thm}
    Eigenvalues of Hermitian operators with discrete spectra are real.
\end{thm}
\begin{proof}
    Suppose $\hat{Q}f = qf$, so $q$ is an eigenvalue. We then have that $$\braket{f}{qf} = \braket{f}{\hat{Q}f} = \braket{\hat{Q}f}{f} = \braket{qf}{f}$$ so $q^*\braket{f}{f} = q\braket{f}{f}$. Since $\braket{f}{f}$ is non-zero, as $f$ is an eigenfunction, and hence non-zero, we have $q^* = q \in \R$.
\end{proof}

So, we determine that if you measure an observable on a particle in a determinate state, you will get a real number.

\begin{thm}
    Eigenfunctions belonging to distinct eigenvalues of a Hermitian operator with discrete spectra are orthogonal.
\end{thm}
\begin{proof}
    Suppose $\hat{Q}f = qf$ and $\hat{Q}g = pg$, for $q \neq p$. Then we have that $$\braket{qf}{g} = \braket{\hat{Q}f}{g} = \braket{f}{\hat{Q}g} = \braket{f}{pg}$$ so $(q-p)\braket{f}{g} = 0$, where $q^* = q$ from the previous theorem. Then as $q \neq p$, it follows that $\braket{f}{g} = 0$, so $f$ and $g$ are orthogonal.
\end{proof}

Note that the second Theorem does not give us information on degenerate states $(p = q)$. However, if a collection of eigenfunctions share the same eigenvalue, any linear combination of them is again an eigenfunction with the same eigenvalue, so we can use the \Emph{Gram-Schmidt orthogonalization procedure} to construct orthogonal eigenfunctions within each degenerate subspace.

In a finite-dimensional vector space the eigenvectors of a hermitian matrix form a basis for the space. But, this does not generalize to infinite-dimensional spaces. However, the property itself is essential to the internal consistency of quantum mechanics, so we take it as an axiom (or more precisely, as a restriction on the class of hermitian operators that can represent observables): 

\begin{axi}
    The eigenfunctions of an observable operator are complete: Any function (in Hilbert space) can be expressed as a linear combination of them.
\end{axi}

\subsection{Continuous Spectra}

If the spectrum of a hermitian operator is continuous, the eigenfunctions are not normalizable and the proofs of the first two theorems in the last section fail, because the inner products may not exist.

\begin{eg}
    Consider the momentum operator $\hat{p} = -i\hbar\frac{d}{dx}$ on the interval $(-\infty,\infty)$. Let $f_p(x)$ be an eigenfunction with eigenvalue $p$: $$-i\hbar\frac{d}{dx}f_p(x) = pf_p(x)$$
    The general solution is $$f_p(x) = Ae^{ipx/\hbar}$$ This is not square-integrable over the given interval for any complex value of $p$, so the momentum operator has no eigenfunctions in Hilbert space.

    Yet, if we restrict ourselves to real eigenvalues, we do recover a kind of ``ersatz orthonormality." Observe that $$\int_{-\infty}^{\infty}f_{p'}^*(x)f_p(x)dx = |A|^2\int_{-\infty}^{\infty}e^{i(p-p')x/\hbar}dx = |A|^22\pi\hbar\delta(p-p')$$
    If we pick $A = 1/\sqrt{2\pi \hbar}$, so that $$f_p(x) = \frac{1}{\sqrt{2\pi \hbar}}e^{ipx/\hbar}$$ then $$\braket{f_{p'}}{f_p} = \delta(p-p')$$
    which is reminiscent of true orthonormality - the indices are now continuous variables, and the Kronecker delta has become a Dirac delta. Most importantly the eigenfunctions with real eigenvalues are complete. with the sum replaced by an integral: Any (square integrable) function $f(x)$ can be written in the form \begin{equation*}
        f(x) = \int_{-\infty}^{\infty}c(p)f_p(x)dp = \frac{1}{\sqrt{2\pi\hbar}}\int_{-\infty}^{\infty}c(p)e^{ipx/\hbar}dx
    \end{equation*}
    The coefficients, $c(p)$, are obtained using Fourier's technique: \begin{equation*}
        \braket{f_{p'}}{f} = \int_{-\infty}^{\infty}c(p)\braket{f_{p'}}{f_p}dp = \int_{-\infty}^{\infty}c(p)\delta(p-p')dp = c(p')
    \end{equation*}
    Alternatively, you can view the form of $f(x)$ as a Fourier transform, and find $$c(p) = \frac{\sqrt{\hbar}}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(t\hbar)e^{-ipt}dt = \frac{1}{\sqrt{2\pi\hbar}}\int_{-\infty}^{\infty}f(x)e^{-ipx/\hbar}dx$$
    which is of course $\braket{f_{p'}}{f}$.
\end{eg}

The eigenfunctions of momentum are sinusoidal with wavelength $$\lambda = \frac{2\pi \hbar}{p}$$ This is the old de Broglie formula. Note we now know there is no such thing as a particle with determinate momentum, according to our formalism. But we could make a normalizable wave packet with a narrow range of momenta, and it is to such an object that the de Broglie relation applies.

Although none of the eigenfunctions of $\hat{p}$ live in Hilbert space, a certain family of them (those with real eigenvalues) reside in the nearby regions, with a kind of quasi-normalizability. They do not represent possible physical states, but they are still very useful.

\begin{eg}
    Let $g_y(x)$ be an eigenfunction with eigenvalue $y$ for the position operator $\hat{x} = x$, so $$\hat{x}g_y(x) = xg_y(x) = yg_y(x)$$ Here $y$ is a fixed number, but $x$ is a continuous variable. What function of $x$ has the property that multiplying by $x$ is the same as multiplying by the constant $y$? The only such one is zero, except at the one point $x = y$; in fact it is nothing but the Dirac delta function: $$g_y(x) = A\delta(x-y)$$ This time the eigenvalue has to be real; the eigenfunctions are not square integrable, but they admit Dirac orthonormality: $$\int_{-\infty}^{\infty}g^*_{y'}(x)g_y(x)dx = |A|^2\int_{-\infty}^{\infty}\delta(x-y')\delta(x-y)dx = |A|^2\delta(y-y')$$ If we pick $A = 1$, so $$g_y(x) = \delta(x-y)$$ then $$\braket{g_{y'}}{g_y} = \delta(y-y')$$
    These eigenfunctions are also complete: $$f(x) = \int_{-\infty}^{\infty}c(y)g_y(x)dy = \int_{-\infty}^{\infty}c(y)\delta(x-y)dy$$
    with $c(y) = f(y)$.
\end{eg}

If the spectrum of a hermitian operator is continuous, the eigenfunctions are not normalizable, they are not in Hilbert space and they do not represent possible physical states; nevertheless, the eigenfunctions with real eigenvalues are Dirac orthonormalizable and complete.

\section{Generalized Statistical Interpretation}

The \Emph{generalized statistical interpretation} allows us to determine the possible results of any measurements, and their probabilities. Together with the Schr\"{o}dinger equation, which tells you how the wave function evolves in time, it is the foundation of quantum mechanics:

\begin{namthm}[Generalized Statistical Interpretation]
    If you measure an observable $Q(x,p)$ on a particle in the state $\Psi(x,t)$, you are certain to get one of the eigenvalues of the hermittian operator $\hat{Q}(x,-i\hbar d/dx)$. If the spectrum of $\hat{Q}$ is discrete, the probability of getting the particular eigenvalue $q_n$ associated with the orthonormalized eigenfunction $f_n(x)$ is $$|c_n|^2,\;\;\text{ where }\;\; c_n = \braket{f_n}{\Psi}$$
    If the spectrum is continuous, with real eigenvalues $q(z)$ and associated Dirac-orthonormalized eigenfunctions $f_z(x)$, the probability of getting a result in the range $dz$ is $$|c(z)|^2dz \;\;\text{ where }\;\;c(z) = \braket{f_z}{\Psi}$$
    Upon measurement, the wave function ``collapses" to the corresponding eigenstate.
\end{namthm}

The eigenfunctions of an observable operator are complete, so the wave function can be written as a linear combination of them: $$\Psi(x,t) = \sum_nc_n(t)f_n(x)$$
(For simplicity we assume the spectrum is discrete) Because the eigenfunctions are orthonormal, the coefficients are given by $$c_n(t) = \braket{f_n}{\Psi} = \int f_n(x)^*\Psi(x,t)dx$$
Qualitatively, $c_n$ tells you ``how much $f_n$ is contained in $\Psi$," and given that a measurement has to return one of the eigenvalues of $\hat{Q}$, it seems reasonable that the probability of getting the particular eigenvalue $q_n$ would be determined by the ``amount of $f_n$" in $\Psi$. But because probabilities are determined by the absolute square of the wave function, the precise measure is actually $|c_n|^2$. 

The total probability (summed over all possible outcomes) is of course $1$ $$\sum_n|c_n|^2 = 1$$ and sure enough, this follows from the normalization of the wave function: \begin{align*}
    1 &= \braket{\Psi}{\Psi} = \braket{\left(\sum_{n'}c_{n'}f_{n'}\right)}{\left(\sum_nc_nf_n\right)} \\
    &= \sum_{n'}\sum_nc_{n'}^*c_n\braket{f_{n'}}{f_n} \\
    &= \sum_{n'}\sum_nc_{n'}^*c_n\delta_{n'n} \\
    &= \sum_{n}c_n^*c_n = \sum_{n}|c_n|^2
\end{align*}
Similarly, the expectation value of $Q$ should be the sum over all possible outcomes of the eigenvalue times the probability of getting that eigenvalue $$\langle Q\rangle = \sum_nq_n|c_n|^2$$
Indeed, by definition we have $$\langle Q\rangle = \braket{\Psi}{\hat{Q}\Psi} = \braket{\left(\sum_{n'}c_{n'}f_{n'}\right)}{\left(\hat{Q}\sum_nc_nf_n\right)}$$ 
but $\hat{Q}f_n = q_nf_n$, so $$\langle Q\rangle = \sum_{n'}\sum_nc_{n'}^*c_nq_n\braket{f_{n'}}{f_n} = \sum_nq_n|c_n|^2$$

\begin{qst}
    Can we reproduce, in this language, the original statistical interpretation for position measurements?
\end{qst}

Yes! A measurement of $x$ on a particle in state $\Psi$ must return one of the eigenvalues of the position operator. As we have found previously, every real number $y$ is an eigenvalue of $x$, and the corresponding Dirac-orthonormalized eigenfunction is $g_y(x) = \delta(x-y)$. Evidently, $$c(y) = \braket{g_y}{\Psi} = \int_{-\infty}^{\infty}\delta(x-y)\Psi(x,t)dx = \Psi(y,t)$$
so the probability of getting a result in the range $dy$ is $|\Psi(y,t)|^2dy$, which is precisely the original statistical interpretation.

What about momentum? We found previously that the Dirac-orthonormalized eigenfunctions of the momentum operator are $f_p(x) = (1/\sqrt{2\pi \hbar}\exp(ipx/\hbar)$, so $$c(p) = \braket{f_p}{\Psi} = \frac{1}{\sqrt{2\pi\hbar}} \int_{-\infty}^{\infty}e^{-ipx/\hbar}\Psi(x,t)dx$$
This is such an essential quantity that we give it a special name and symbol:

\begin{defn}
    We define the \Emph{space wave function}, $\Phi(p,t)$, to be \begin{equation*}
        \Phi(p,t) = \braket{f_p}{\Psi} = \frac{1}{\sqrt{2\pi \hbar}} \int_{-\infty}^{\infty}e^{-ipx/\hbar}\Psi(x,t)dx
    \end{equation*}
\end{defn}
It is essentially the Fourier transform of the position space wave function $\Psi(x,t)$, which by Plancherel's theorem is its inverse Fourier transform: \begin{align*}
    \Phi(p,t) &= \frac{1}{\sqrt{2\pi\hbar}} \int_{-\infty}^{\infty}e^{-ipx/\hbar}\Psi(x,t)dx \\
    \Psi(x,t) &= \frac{1}{\sqrt{2\pi\hbar}}\int_{-\infty}^{\infty}e^{ipx/\hbar}\Phi(p,t)dp
\end{align*}
According to the generalized statistical interpretation, the probability that a measurement of momentum would yield a result in the range $dp$ is $|\Phi(p,t)|^2dp$.


\section{The Uncertainty Principle}

In this section we aim to prove a reasonably general version of the uncertainty principle, and explore some of its ramifications:

\begin{proof}[Uncertainty Principle]
    For any observable $A$, we have $$\sigma_A^2 = \langle (\hat{A}-\langle A\rangle)^2\rangle = \braket{\left(\hat{A}-\langle A\rangle\right)\Psi}{\left(\hat{A}-\langle A\rangle\right)\Psi} = \braket{f}{f}$$ since observable operators are Hermitian (i.e. self-adjoint), where $f \equiv \left(\hat{A} - \langle A\rangle\right)\Psi$. Likewise, for any other observable, $B$, $$\sigma_B^2 = \braket{g}{g},\;\;\text{ where }\;g \equiv \left(\hat{B}-\langle B\rangle\right)\Psi$$
    Therefore, invoking the Cauchy-Schwarz inequality on sesquilinear forms (complex inner products), $$\sigma_A^2\sigma_B^2 = \braket{f}{f}\braket{g}{g} \geq |\braket{f}{g}|^2$$ Now, for any complex number $z$, $$|z|^2 = \text{Re}(z)^2 + \text{Im}(z)^2 \geq \text{Im}(z)^2 = \left[\frac{1}{2i}(z-z^*)\right]^2$$
    Therefore, letting $z = \braket{f}{g}$, $$\sigma_A^2\sigma_B^2 \geq \left(\frac{1}{2i}\left[\braket{f}{g} - \braket{g}{f}\right]\right)^2$$
    But, exploiting the fact that $(\hat{A}-\langle A\rangle)$ is hermitian, \begin{align*}
        \braket{f}{g} &= \braket{\left(\hat{A}-\langle A\rangle\right)\Psi}{\left(\hat{B}-\langle B\rangle\right)\Psi} = \braket{\Psi}{\left(\hat{A}-\langle A\rangle\right)\left(\hat{B}-\langle B\rangle\right)\Psi} \\
        &= \braket{\Psi}{\left(\hat{A}\hat{B} - \hat{A}\langle B\rangle - \hat{B}\langle A\rangle + \langle A\rangle\langle B\rangle\right)\Psi} \\
        &= \braket{\Psi}{\hat{A}\hat{B}\Psi} - \langle B\rangle\braket{\Psi}{\hat{A}\psi} - \langle A\rangle \braket{\Psi}{\hat{B}\Psi} + \langle A\rangle \langle B\rangle \braket{\Psi}{\Psi} \\
        &= \langle \hat{A}\hat{B}\rangle - \langle B\rangle\langle A\rangle - \langle A\rangle \langle B\rangle + \langle A\rangle \langle B \rangle \\
        &= \langle \hat{A}\hat{B}\rangle - \langle A\rangle \langle B\rangle 
    \end{align*}
    Similarly, we have $$\braket{g}{f} = \langle \hat{B}\hat{A}\rangle - \langle A \rangle \langle B\rangle $$
    so $$\braket{f}{g} - \braket{g}{f} = \langle \hat{A}\hat{B}\rangle - \langle \hat{B}\hat{A}\rangle = \langle [\hat{A},\hat{B}]\rangle$$ where $$[\hat{A},\hat{B}] := \hat{A}\hat{B} - \hat{B}\hat{A}$$ is the commutator of the two operators. In conclusion, \begin{equation}
        \boxed{\sigma_A^2\sigma_B^2 \geq \left(\frac{1}{2i}\left\langle\left[\hat{A},\hat{B}\right]\right\rangle\right)^2}
    \end{equation}
\end{proof}
This is the generalized \Emph{uncertainty principle}.

For an example, suppose $\hat{A} = x$, and $\hat{B} = -i\hbar d/dx$. Then $$[\hat{x},\hat{p}] = -i\hbar x \frac{d}{dx} + i\hbar \frac{d}{dx}x$$ applied to some $f(x)$ is \begin{align*}
    [\hat{x},\hat{p}](f) &= -i\hbar xf'(x) + i\hbar(f(x) + xf'(x)) = i\hbar f(x)
\end{align*}
so $[\hat{x},\hat{p}] = i\hbar$. So $$\sigma_x^2\sigma_p^2 \geq \left(\frac{1}{2i}i\hbar\right)^2 = \frac{\hbar^2}{2^2}$$
or since the standard deviations are by nature positive, $$\sigma_x\sigma_p \geq \frac{\hbar}{2}$$
This was our original Heisenberg uncertainty principle. There is in fact an ``uncertainty principle" for every pair of observables whose operators do not commute. We call such pairs of observable \Emph{incompatible observables}. Incompatible observables do not have shared eigenfunctions, at least in the sense that they cannot have a complete set of common eigenfunctions.

By contrast, \Emph{compatible} or commuting observables do admit complete sets of simultaneous eigenfunctions (that is: states that are determinate for both observables).

Note that the uncertainty principle is \Emph{not} an extra postulate in quantum theory, but rather a consequence of the statistical interpretation. You might wonder how this holds up experimentally. You can certainly measure the position of a particle, but the act of measurement collapses the wave function to a narrow spike, which necessarily carries a broad range of wavelengths (hence momenta) in its Fourier decomposition. If you now measure the momentum, the state will collapse to a long sinusoidal wave, with now a well-defined wavelength, but the particle no longer has the position you got in the first measurement. Only if the wave function were simultaneously an eigenstate of both observables would it be possible to make the second measurement without disturbing the state of the particle. In recent years the Bohr-Heisenberg explanation has been called into question.

\subsection{The Minimum-Uncertainty Wave Packet}

We have twice encountered wave functions that hit the position-momentum uncertaitny limit, $\sigma_x\sigma_p = \hbar/2$: the ground state of the harmonic oscillator, and the Gaussian wave packet for the free particle. This raises the question, what is the most general minimum-uncertainty wave packet? Suppose we find the conditions under which both the inequalities in our previous proof are equalities.

The Cauchy-Schwarz inequalities becomes an equality if and only if one function is a scalar multiple of the other, $g(x) = cf(x)$ for some $c \in \C$. Meanwhile the other inequality we had only becomes an equality if $\braket{f}{g} = c\braket{f}{f}$ is purely imaginary, so $c$ must be purely imaginary. Call $c = ia$, $a \in \R$. The necessary and sufficient condition for minimum uncertainty then is $$g(x) = iaf(x),\;\;a \in \R$$


For the position-momentum uncertainty principle this criterion becomes: $$\left(-i\hbar \frac{d}{dx} - \langle p \rangle\right)\Psi = ia(x - \langle x \rangle)\Psi$$
which is a differential equation for $\Psi$ as a function of $x$. Its general solution is $$\Psi(x) = Ae^{-a(x-\langle x\rangle)^2/2\hbar}e^{i\langle p\rangle x/\hbar}$$
Evidently, the minimum-uncertainty wave packet is a gaussian.

\subsection{The Energy-Time Uncertainty Principle}

We often pair the position-momentum uncertainty principle with the \Emph{energy-time uncertainty principle} $$\Delta t\Delta E \geq \frac{\hbar}{2}$$ 
In the context of special relativity, the energy-time form can be thought of as a consequence of the position-momentum version, because $x$ and $t$ (or rather, $ct$) go together in the position-time four-vector, while $p$ and $E$ (or rather, $E/c$) go together in the energy-momentum four vector. But, the Schr\"{o}dinger equation is explicitly nonrelativistic, and so this implication does not hold. We shall now derive this uncertainty principle, and in doing so show that it is very much distinct from the position-momentum version.

Indeed, position, momentum, and energy are all dynamical variables of a system (measurable characteristics at any given time), but time itself is not a dynamical variable (not at any rate in a nonrelativistic theory). Time is the \emph{independent} variable of which the dynamical quantities are \emph{functions}. In particular, $\Delta t$ is not the standard deviation of a collection of time measurements, but rather the time it takes the system to change substantially (roughly).

As a measure of the rate our system is changing, we comput the time derivative of the expectation value of some observable, $Q(x,p,t)$: $$\frac{d}{dt}\langle Q\rangle = \frac{d}{dt}\braket{\Psi}{\hat{Q}\Psi} = \braket{\frac{\partial \Psi}{\partial t}}{\hat{Q}\Psi} + \braket{\Psi}{\frac{\partial \hat{Q}}{\partial t}\Psi} + \braket{\Psi}{\hat{Q}\frac{\partial \Psi}{\partial t}}$$  
Nowl, the Schr\"{o}dinger equation says $$i\hbar\frac{\partial\Psi}{\partial t} = \hat{H}\Psi$$
where $H = p^2/2m + V$ is the Hamiltonian. So $$\frac{d}{dt}\langle Q\rangle = -\frac{1}{i\hbar}\braket{\hat{H}\Psi}{\hat{Q}\Psi} + \frac{1}{i\hbar}\braket{\Psi}{\hat{Q}\hat{H}\Psi} + \left\langle \frac{\partial\hat{Q}}{\partial t}\right\rangle$$
But $\hat{H}$ is hermitian (being the operator of an observabel), so $\braket{\hat{H}\Psi}{\hat{Q}\Psi} = \braket{\Psi}{\hat{H}\hat{Q}\Psi}$, and hence \begin{equation*}
    \boxed{\frac{d}{dt}\langle Q\rangle = \frac{i}{\hbar}\left\langle\left[\hat{H},\hat{Q}\right]\right\rangle + \left\langle \frac{\partial\hat{Q}}{\partial t}\right\rangle}
\end{equation*}
We call this the \Emph{generalized Ehrenfest theorem}. In the typical case where the operator does not depend explicitly on time, it tells us that the rate of change of the expectation value is determined by the commutator of the operator with the Hamiltonian. In particular, if $\hat{Q}$ commutes with $\hat{H}$, then $\langle Q\rangle$ is constant, and in this sense $Q$ is a conserved quantity.

Now, suppose we pick $A = H$ and $B = Q$ in the generalized uncertainty principle, and assume that $Q$ does not depend explicitly on time $t$: $$\sigma_H^2\sigma_Q^2 \geq \left(\frac{1}{2i}\left\langle \left[\hat{H},\hat{Q}\right]\right\rangle\right)^2 = \left(\frac{1}{2i}\frac{\hbar}{i}\frac{d\langle Q\rangle}{dt}\right)^2 = \frac{\hbar^2}{2^2}\left(\frac{d\langle Q\rangle}{dt}\right)^2$$ or simply $$\sigma_H\sigma_Q \geq \frac{\hbar}{2}\left|\frac{d\langle Q\rangle}{dt}\right|$$

Then we define $\Delta E \equiv \sigma_H$ and $$\Delta t \equiv \frac{\sigma_Q}{|d\langle Q\rangle/dt|}$$
Then $$\Delta E\Delta t \geq \frac{\hbar}{2}$$
and that is the energy-time uncertainty principle. But notice what is meant by $\Delta t$ here since $\sigma_Q = \left|\frac{d\langle Q\rangle}{dt}\right|\Delta t$, $\Delta t$ represents the \emph{amount of time it takes the expectation value of $Q$ to change by one standard deviation}. In particular, $\Delta t$ depends entirely on what observable $Q$ you are looking at. But if $\Delta E$ is small, then the rate of change of all observables must be very gradual, or in other words, if any observable changes rapidly, the uncertainty in the energy must be large.

\section{Vectors and Operators}

First, note that the state of a system in quantum mechanics is represented by a vector $\ket{\mathcal{S}(t)}$, that lives out in Hilbert space, but we can express it with respect to any number of different bases. The wave function $\Phi(x,t)$ is the $x$ ``component" in the expansion of $\ket{\mathcal{S}(t)}$ in the basis of position eigenfunctions: $$\Psi(x,t) = \braket{x}{\mathcal{S}(t)}$$ with $\ket{x}$ standing for the eigenfunction of $\hat{x}$ with eigenvalue $x$. The momentum space wave function $\Phi(p,t)$ is the $p$ component in the expansion of $\ket{\mathcal{S}(t)}$ in the basis of momentum eigenfunctions: $$\Phi(p,t) = \braket{p}{\mathcal{S}(t)}$$
with $\ket{p}$ standing for the eigenfunctio of $\hat{p}$ with eigenvalue $p$. Or we could expand $\ket{\mathcal{S}(t)}$ in the basis of energy eigenfunctions (supposing for simplicity that the spectrum is discrete): $$c_n(t) = \braket{n}{\mathcal{S}(t)}$$
with $\ket{n}$ standing for the $n$th eigenfunction of $\hat{H}$. But it's all the same state; the functions $\Psi$ and $\Phi$, and the collection of coefficient functions $\{c_n\}$, contain exactly the same information - they are simply three different ways of identifying the same vector: \begin{align*}
    &\ket{\mathcal{S}(t)} \\
    & \int\Psi(y,t)\ket{y}dy = \int\Psi(y,t)\delta(x-y)dy \\
    & \int\Phi(p,t)\ket{p}dp = \int\Phi(p,t)\frac{1}{\sqrt{2\pi\hbar}}e^{ipx/\hbar}dp \\
    & \sum_nc_n(t)e^{-iE_nt/\hbar}\psi_n(x)
\end{align*}

Next, operators (representing observables) are special linear transformations on Hilbert space, so they transform vectors between each-other: $$\ket{\beta} = \hat{Q}\ket{\alpha}$$
Just as vectors are represented with respect to an orthonormal basis $\{\ket{e_n}\}$ (assumed here to be discrete for simplicity) by their components $$\ket{\alpha} = \sum_na_n\ket{e_n},\;\;\ket{\beta} = \sum_nb_n\ket{e_n},\;\;a_n = \braket{e_n}{\alpha},\;\;b_n = \braket{e_n}{\beta}$$
operators are represented with respect to a particular basis by their \Emph{matrix elements} (is not necessarily finite, and so will not necessarily form a classical matrix) $$\bra{e_m}\hat{Q}\ket{e_n} =: Q_{mn}$$
Note that sums are replaced by integrals in the case of a continuous (uncountable) basis. In this notation our equation $\ket{\beta} = \hat{Q}\ket{\alpha}$ becomes $$\sum_nb_n\ket{e_n} = \sum_na_n\hat{Q}\ket{e_n}$$
or taking the inner product with $\ket{e_m}$, $$\sum_nb_n\braket{e_m}{e_n} = \sum_na_n\bra{e_m}\hat{Q}\ket{e_n}$$ and hence $$b_m = \sum_nQ_{mn}a_n$$ Thus, the matrix elements of $\hat{Q}$ tell you how the components transform. Note that we can only use the notation $\bra{e_m}\hat{Q}\ket{e_n}$ without explicitly stating which side of the inner product $\hat{Q}$ is on since $\hat{Q}$ is a Hermitian transformation.

Just as vectors look different when expressed in different bases, so too do operators. Two nice examples are \begin{align*}
    \hat{x}\;(\text{the position operator}) &\rightarrow \left\{\begin{array}{lc} x & (\text{in position space}) \\ i\hbar\partial/\partial p & (\text{in momentum space}) \end{array}\right. \\
    \hat{p}\;(\text{the momentum operator}) &\rightarrow \left\{\begin{array}{lc} -i\hbar\partial/\partial x & (\text{in position space}) \\ p & (\text{in momentum space}) \end{array}\right.
\end{align*}

Note that the state of a system, $\mathcal{S}(t)$, is represented by its wave function, $\Psi(x,t)$, \Emph{in the position basis}.

Next, we observe that the \Emph{bra}'s, $\bra{\alpha}$, are not vectors but instead are linear functionals corresponding to the image of the vector, or \Emph{ket}, $\ket{\beta}$ under the Riesz representation monomorphism sending a vector $\ket{\alpha}$ to the linear functional $\bra{\alpha} := \braket{\alpha}{\cdot}$ in its dual space. In a function space this takes the form $$\bra{f} = \braket{f}{\cdot} = \int f^*(x)[\cdot]dx$$
In $\C^n$ with the standard Euclidean inner product, $\ket{\alpha}$ corresponds to a column vector, while we can associate $\bra{\beta} = \ket{\beta}^{\text{\textdagger}}$ to the conjugate transpose of a vector using the definitions of matrix multiplication.

We can likewise define the notation $\ket{\alpha}\bra{\beta} := \braket{\beta}{\cdot}\ket{\alpha}$, which defines a linear operator on our space (in particular a one-dimensional linear operator). This is most useful when $\beta$ and $\alpha$ are the same, which gives $$\hat{P} := \ket{\alpha}\bra{\alpha}$$ which acts as the projection operator onto the one-dimensional subspace spanned by $\ket{\alpha}$, as $$\hat{P}\ket{\beta} = (\braket{\alpha}{\beta})\ket{\alpha}$$

If $\{\ket{e_n}\}$ is a discrete orthonormal basis, so $\braket{e_m}{e_n} = \delta_{mn}$, then $$\sum_n\ket{e_n}\bra{e_n} = 1$$ the identity operator. For if we let this operator act on any vector $\ket{\alpha}$, we recover $\ket{\alpha}$ in terms of the basis $\{\ket{e_n}\}$: $$\sum_{n}(\braket{e_n}{\alpha})\ket{e_n} = \ket{\alpha}$$ Similarly, if $\{\ket{e_z}\}$ is a Dirac orthonormalized continuous basis, $$\braket{e_z}{e_{z'}} = \delta(z-z')$$ then $$\int\ket{e_z}\bra{e_z}dz = 1$$
Note that saying that we can write the identity operator in this way for some set of vectors is equivalent to the notion of completeness, in the sense of unique representations of all vectors in the space.

Note that in this notation, the item inside the brackets of the ket or bra, for example $\alpha$ in $\ket{\alpha}$, is simply a name, not a mathematical object.

\subsection{Changing Bases}

The point of Dirac notation is to free us from the confines of any particular basis, so how can we represent changes of basis using Dirac notation? First recall we can represent the identity operator with respect to a basis of position eigenstates $\ket{x}$ (continuous), momentum eigenstates $\ket{p}$ (continuous), and energy eigenstates $\ket{n}$ (discrete) by \begin{align*}
    1 &= \int dx\ket{x}\bra{x} \\
    1 &= \int dp \ket{p}\bra{p} \\
    1 &= \sum\ket{n}\bra{n}
\end{align*}
Applying these to the state vector $\ket{\mathcal{S}(t)}$ we obtain \begin{align*}
    \ket{\mathcal{S}(t)} &= \int dx\ket{x}\braket{x}{\mathcal{S}(t)} = \int \Psi(x,t)\ket{x}dx \\
    \ket{\mathcal{S}(t)} &= \int dp \ket{p}\braket{p}{\mathcal{S}(t)} = \int \Phi(p,t)\ket{p}dp \\
    \ket{\mathcal{S}(t)} &= \sum\ket{n}\braket{n}{\mathcal{S}(t)} = \sum c_n(t)\ket{n}
\end{align*}

Operators act on kets, and the outcome of this operation can be expressed in any basis by taking the inner product with an appropriate basis vector. 



%%%%%%%%%%%%%%%%%%%%%% Chapter 1.4
\chapter{Quantum Mechanics in Three Dimensions}

\section{The Schr\"{o}dinger Equation in 3D}

The 3D Schr\"{o}dinger equation is given by \begin{equation*}
    \boxed{i\hbar\frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2m}\nabla^2\Psi + V\Psi}
\end{equation*}
The probability of finding the particle in the infinitesimal volume $d^3r = dxdydz$ is $|\Psi(r,t)|^2d^3r$, and the normalization condition reads $$\int|\Psi|^2d^3r = 1$$
If $V$ is independent of time there will be a complete set of stationary states \begin{equation*}
    \Psi_n(r,t) = \psi(r)e^{-iE_nt/\hbar}
\end{equation*}
where the spatial wave function $\psi_n$ satisfies the time-independent Schr\"{o}dinger equation \begin{equation*}
    \boxed{-\frac{\hbar^2}{2m}\nabla^2\psi + V\psi = E\psi}
\end{equation*}
The general solution to the timde-dependent Schr\"{o}dinger equation is \begin{equation*}
    \Psi(r,t) = \sum c_n\psi_n(r)e^{-iE_nt/\hbar}
\end{equation*}
with the constants $c_n$ determined by the initial wave function, $\Psi(r,0)$, in the usual way.

\begin{rmk}
    For $i,j \in [x,y,z]$ we have that \begin{equation*}
        [r_i,p_j] = -[p_i,r_j] = i\hbar\delta_{ij},\;\;[r_i,r_j] = [p_i,p_j] = 0
    \end{equation*}
\end{rmk}

\subsection{Spherical Coordinates}

We consider the case of \Emph{central potentials}, where $V(r) = V(\norm{r})$. In this case we adapt to spherical coordinates, $(r,\theta,\phi)$, with the Laplacian taking the form \begin{equation*}
    \nabla^2 = \frac{1}{r^2}\partial_r\left(r^2\partial_r\right) + \frac{1}{r^2\sin\theta}\partial_{\theta}\left(\sin\theta\partial_{\theta}\right) + \frac{1}{r^2\sin^2\theta}\left(\partial_{\phi}^2\right)
\end{equation*}
In spherical coordinates the time-independent Schr\"{o}dinger equation reads \begin{equation*}
    -\frac{\hbar^2}{2m}\left[\frac{1}{r^2}\partial_r\left(r^2\partial_r\right) + \frac{1}{r^2\sin\theta}\partial_{\theta}\left(\sin\theta\partial_{\theta}\right) + \frac{1}{r^2\sin^2\theta}\left(\partial_{\phi}^2\right)\right] + V\psi = E\psi
\end{equation*}
We begin by looking for separated solutions of the form \begin{equation*}
    \psi(r,\theta,\phi) = R(r)Y(\theta,\phi)
\end{equation*}
Putting this into our equation and isolating terms we can write \begin{equation*}
    \left\{\frac{1}{R}\partial_r(r^2\partial_rR)-\frac{2mr^2}{\hbar^2}[V(r)-E]\right\} + \frac{1}{Y}\left\{\frac{1}{\sin\theta}\partial_{\theta}(\sin\theta\partial_{\theta}Y) + \frac{1}{\sin^2\theta}\partial_{\phi}^2Y\right\} = 0
\end{equation*}
As we have separated our equation into $r$ and angle dependences, we can write them in terms of a separation constant $\ell(\ell+1)$: \begin{align*}
    \frac{1}{R}\partial_r(r^2\partial_rR)-\frac{2mr^2}{\hbar^2}[V(r)-E] &= \ell(\ell+1) \\
    \frac{1}{Y}\left\{\frac{1}{\sin\theta}\partial_{\theta}(\sin\theta\partial_{\theta}Y) + \frac{1}{\sin^2\theta}\partial_{\phi}^2Y\right\} &= -\ell(\ell+1)
\end{align*}


First for the angular equation we can write \begin{equation*}
    \sin\theta\partial_{\theta}(\sin\theta\partial_{\theta}Y)+\partial_{\phi}^2Y = -\ell(\ell+1)\sin^2\theta Y
\end{equation*}
Again we look for separated solutions of the form $Y(\theta,\phi) = \Theta(\theta)\Phi(\phi)$. Plugging this in and isolating the dependences we can write\begin{equation*}
    \left\{\frac{1}{\Theta}\left[\sin\theta\partial_{\theta}(\sin\theta\partial_{\theta}\Theta)\right]+\ell(\ell+1)\sin^2\theta\right\}+\frac{1}{\Phi}\partial_{\phi}^2\Phi = 0
\end{equation*}
and again we can write our equations in terms of a separation constant we denote by $-m^2$: \begin{align*}
    \frac{1}{\Theta}\left[\sin\theta\partial_{\theta}(\sin\theta\partial_{\theta}\Theta)\right]+\ell(\ell+1)\sin^2\theta &= m^2 \\
    \frac{1}{\Phi}\partial_{\phi}^2\Phi &= -m^2
\end{align*}
For the $\phi$ equation we easily find \begin{equation*}
    \Phi(\phi) = e^{im\phi}
\end{equation*}
where we absorb the $e^{-im\phi}$ solution in the fact that we allow $m$ to run negative in the future. As a physical requirement based on the nature of $\phi$ we have that $\Phi(\phi+2\pi) = \Phi(\phi)$. This implies that $m \in \Z$. 

Now for the $\theta$ equation we have \begin{equation*}
    \sin\theta\partial_{\theta}(\sin\theta\partial_{\theta}\Theta) + \left[\ell(\ell+1)\sin^2\theta-m^2\right]\Theta = 0
\end{equation*}
the solution to this equation is \begin{equation*}
    \Theta(\theta) = AP_{\ell}^m(\cos\theta)
\end{equation*}
where $P_{\ell}^m$ is the \Emph{associated Legendre function} defined by \begin{equation*}
    P_{\ell}^m(x) := (-1)^m(1-x^2)^{m/2}\frac{d^m}{dx^m}P_{\ell}(x)
\end{equation*}
where $P_{\ell}(x)$ is the \Emph{$\ell$th Legendre polynomial}, defined by the \Emph{Rodrigues formula}: \begin{equation*}
    P_{\ell}(x) := \frac{1}{2^{\ell}\ell!} \frac{d^{\ell}}{dx^{\ell}}(x^2-1)^{\ell}
\end{equation*}
Notice that $\ell$ must be a non-negative integer, and if $m > \ell$ then $P_{\ell}^m = 0$. So for any given $\ell$ there are $(2\ell+1)$ possible values of $m$:\begin{equation*}
    \ell=0,1,2,...;\;\;m=-\ell,-\ell+1,...,-1,0,1,...,\ell-1,\ell
\end{equation*}
For convenience we require that $R$ and $Y$ are normalized separately: \begin{equation*}
    \int_0^{\infty}|R|^2r^2dr = 1,\;\;\;\int_0^{\pi}\int_0^{2\pi}|Y|^2\sin\theta d\theta d\phi = 1
\end{equation*}
The normalized angular wave functions are called \Emph{spherical harmonics}:\begin{equation*}
    \boxed{Y_{\ell}^m(\theta,\phi) = \sqrt{\frac{(2\ell+1)}{4\pi}\frac{(\ell-m)!}{(\ell+m)!}}e^{im\phi}P_{\ell}^m(\cos\theta)}
\end{equation*}
They satisfy the orthogonality relation \begin{equation*}
    \int_0^{\pi}\int_0^{2\pi}[Y_{e\ll}^m(\theta,\phi)]^*\left[Y_{\ell'}^{m'}(\theta,\phi)\right]\sin\theta d\theta d\phi = \delta_{\ell\ell'}\delta_{mm'}
\end{equation*}


Now, note that only the radial part is effected by the shape of the potential $V(r)$:\begin{equation*}
    \partial_r(r^2\partial_rR) - \frac{2mr^2}{\hbar^2}[V(r)-E]R = \ell(\ell+1)R
\end{equation*}
We define $u(r) = rR(r)$, so $R = u/r$ and our equation simplifies to \begin{equation*}
    -\frac{\hbar^2}{2m}\partial_r^2u + \left[V +\frac{\hbar^2}{2m}\frac{\ell(\ell+1)}{r^2}\right]u = Eu
\end{equation*}
Note that this is identical to the one dimensional Schr\"{o}dinger equation but with the \Emph{effective potential} \begin{equation*}
    V_{eff} = V + \frac{\hbar^2}{2m}\frac{\ell(\ell+1)}{r^2}
\end{equation*}
which contains the so-called \Emph{centrifugal term}, $(\hbar^2/2m)[\ell(\ell+1)/r^2]$.


\section{The Hydrogen Atom}

We approximate the proton in the Hydrogen atom as motionless. The Coulomb potential for the electron in the atom is \begin{equation*}
    V(r) = -\frac{e^2}{4\pi \epsilon_0}\frac{1}{r}
\end{equation*}
and the radial equation reads \begin{equation*}
    -\frac{\hbar^2}{2m_e}\partial_r^2u + \left[-\frac{e^2}{4\pi\epsilon_0}\frac{1}{r}+\frac{\hbar^2}{2m_e}\frac{\ell(\ell+1)}{r^2}\right]u = Eu
\end{equation*}


Let $\kappa := \frac{\sqrt{-2m_eE}}{\hbar}$, where $E < 0$ for bound states. We write our equation then as \begin{equation*}
    \frac{1}{\kappa^2}\partial_r^2u = \left[1-\frac{m_ee^2}{2\pi\epsilon_0\hbar^2\kappa}\frac{1}{\kappa r}+\frac{\ell(\ell+1)}{(\kappa r)^2}\right]u
\end{equation*}
We then introduce $\rho := \kappa r$ and $\rho_0 := \frac{m_ee^2}{2\pi\epsilon_0\hbar^2\kappa}$, so that \begin{equation*}
    \partial_{\rho}^2u = \left[1-\frac{\rho_0}{\rho}+\frac{\ell(\ell+1)}{\rho^2}\right]u
\end{equation*}
As $\rho\rightarrow \infty$, $\partial_{\rho}^2u \rightarrow u$, so in this limit $u(\rho) \rightarrow Ae^{-\rho}+Be^{\rho} \approx Ae^{-\rho}$ as $B = 0$ in order for the function to be normalizable. On the other end, when $\rho\rightarrow 0$, $\partial_{\rho}^2u\rightarrow \frac{\ell(\ell+1)}{\rho^2}u$, and our general solution is $u(\rho) = C\rho^{\ell+1}+D\rho^{-\ell}$, but $D$ must be $0$ for $u$ to be normalizable, so $u(\rho) \approx C\rho^{\ell+1}$. Then we expect that $u(\rho) = \rho^{\ell+1}e^{-\rho}v(\rho)$ for some simpler function $v(\rho)$. In order for this function to satisfy the radial equation, $v$ must satisfy \begin{equation*}
    \rho\partial_{\rho}^2v + 2(\ell+1-\rho)\partial_{\rho}v + [\rho_0-2(\ell+1)]v = 0
\end{equation*}
We assume we can express $v$ as $v(\rho) = \sum_{j=0}^{\infty}c_j\rho^j$. Using our differential equation we obtain the recursion relation \begin{equation*}
    c_{j+1} = \frac{2(j+\ell+1)-\rho_0}{(j+1)(j+2\ell+2)}c_j
\end{equation*}
For our solution to not blow up asymptotically, we need the series to terminate at some $N \in \Z$. In that case $2(N+\ell)-\rho_0 = 0$. Defining $n := N+\ell$ we have $\rho_0 = 2n$. Then we can write $E_n$ as \begin{equation*}
    \boxed{E_n = -\left[\frac{m_e}{2\hbar^2}\left(\frac{e^2}{4\pi\epsilon_0}\right)^2\right]\frac{1}{n^2} = \frac{E_1}{n^2}}
\end{equation*}
which is the \Emph{Bohr formula}, and we also find the \Emph{Bohr radius} \begin{equation*}
    \boxed{a := \frac{4\pi\epsilon_0\hbar^2}{m_ee^2}=0.529\times 10^{-10}\;m}
\end{equation*}
Then $\rho = r/an$. The spatial wave functions are labeled as \begin{equation*}
    \psi_{n\ell m}(r,\theta,\phi) = R_{n\ell}(r)Y_{\ell}^m(\theta,\phi)
\end{equation*}


The total degeneracy of an energy level $E_n$ is $d(n) = \sum_{\ell=0}^{n-1}(2\ell+1)=n^2$. The polynomial $v(\rho)$ is a given by $v(\rho) = L_{n-\ell-1}^{2\ell+1}(2\rho)$ where \begin{equation*}
    L_q^p(x) := (-1)^p\frac{d^p}{dx^p}L_{p+q}(x)
\end{equation*} 
is an \Emph{associated Laguerre polynomial}, and \begin{equation*}
    L_q(x) := \frac{e^x}{q!}\frac{d^q}{dx^q}(e^{-x}x^q)
\end{equation*}
is the $q$th \Emph{Laguerre polynomial}. Then \begin{equation*}
    \boxed{\psi_{n\ell m} = \sqrt{\left(\frac{2}{na}\right)^2\frac{(n-\ell-1)!}{2n(n+\ell)!}}e^{-r/na}\left(\frac{2r}{na}\right)^{\ell}\left[L_{n-\ell-1}^{2\ell+1}(2r/na)\right]Y_{\ell}^m(\theta,\phi)}
\end{equation*}
The wave functions are mutually orthogonal \begin{equation*}
    \int \psi_{n\ell m}^*\psi_{n'\ell'm'}r^2\sin\theta drd\theta d\phi = \delta_{nn'}\delta_{\ell\ell'}\delta_{mm'}
\end{equation*}
The quantum numbers $n,\ell,$ and $m$ can be identified from the nodes of the wave function. The number of radial nodes is given by $N-1$ (for hydrogen this is $n-\ell-1$). The quantum number $m$ counts the number of nodes of the real or imaginary part of the wave function in the $\phi$ direction. These nodes are planes containing the $z$ axis on which the real or imaginary part of $\psi$ vanishes. Finally, $\ell-m$ gives the number of nodes in the $\theta$ direction. These are cones about the $z$ axis on which $\psi$ vanishes.


\section{Angular Momentum}

Recall the principal quantum number $n$ determines the energy of the state. As we shall now see, $\ell$ and $m$ are related to the orbital angular momentum. Quantum mechanically we have the angular momentum operators \begin{equation*}
    \hat{L}_x = y\hat{p}_z - z\hat{p}_y,\;\;\hat{L}_y=z\hat{p}_x - x\hat{p}_z,\;\;L_z=x\hat{p}_y - y\hat{p}_x
\end{equation*}
First, we observe that $L_x$ and $L_y$ do not commute: In particular, \begin{equation*}
    \boxed{[L_x,L_y] = i\hbar L_z,\;\;[L_y,L_z] = i\hbar L_x,\;\;[L_z,L_x] = i\hbar L_y}
\end{equation*}
This implies that $L_x,L_y$, and $L_z$ are incompatible observables. From the generalized uncertainy principle \begin{equation*}
    \sigma_{L_x}^2\sigma_{L_y}^2 \geq \left(\frac{1}{2i}\langle i\hbar L_z\rangle\right)^2 = \frac{\hbar^2}{4}\langle L_z\rangle^2
\end{equation*}
Now consider the square of the total angular momentum \begin{equation*}
    L^2 := L_x^2+L_y^2+L_z^2
\end{equation*}
which does commute with $L_x,L_y$, and $L_z$. Compactly \begin{equation*}
    [L^2,L] = 0
\end{equation*}
Then we can hope to find simultaneous eigenstates of $L^2$ and (say) $L_z$: \begin{equation*}
    L^2f = \lambda f ,\;\;\;L_z f = \mu f
\end{equation*}
We define the \Emph{ladder operators}: \begin{equation*}
    L_{\pm} := L_x \pm iL_y
\end{equation*}
Its commutator with $L_z$ is \begin{equation*}
    [L_z,L_{\pm}] = \pm\hbar L_{\pm}
\end{equation*}
and $[L^2,L_{\pm}] = 0$. Then observe that \begin{equation*}
    L^2(L_{\pm}f) = L_{\pm}(L^2f) = \lambda L_{\pm}f
\end{equation*}
so $L_{\pm}f$ is an eigenfunction of $L^2$ with the same eigenvalue $\lambda$, and \begin{equation*}
    L_z(L_{\pm}f) = [L_z,L_{\pm}]f + L_{\pm}L_zf = \pm\hbar L_{\pm}f + L_{\pm}(\mu f) = (\mu \pm \hbar)(L_{\pm}f)
\end{equation*}
so $L_{\pm}f$ is an eigenfunction of $L_z$ with the new eigenvalue $\mu\pm\hbar$. We call $L_+$ the \Emph{raising operator} and $L_-$ the \Emph{lowering operator}.

In order to ensure that the $z$-component doesn't exceed the total angular momentum there must be a top rung, $f_t$, such that $L_+f_t = 0$. Let $\hbar \ell$ be the eigenvalue of $L_z$ at the top rung, so \begin{equation*}
    L_zf_t = \hbar \ell f_t,\;\;L^2f_t = \lambda f_t
\end{equation*}
Now observe that \begin{equation*}
    L_{\pm}L_{\mp} = L^2 - L_z^2 \pm\hbar L_z,\;\text{ or }\;L^2 = L_{\pm}L_{\mp} + L_z^2 \mp \hbar L_z
\end{equation*}
It follows that \begin{equation*}
    L^2f_t = \hbar^2\ell(\ell+1)f_t
\end{equation*}
and hence $$\lambda = \hbar^2\ell(\ell+1)$$
Similarly we need a bottom rung, $f_b$, such that $L_-f_b = 0$. Let $\hbar\ell'$ be the eigenvalue of $L_z$ at the bottom rung so $L_zf_b=\hbar\ell'f_b$ and $L^2f_b = \lambda f_b$.Then \begin{equation*}
    L^2f_b = \hbar^2\ell'(\ell'-1)f_b
\end{equation*}
In order for this to hold, we need $\ell' = -\ell$. So the eigenvalues of $L_z$ are $m\hbar$, where $m$ goes from $-\ell$ to $+\ell$ in $N$ integer steps. In particular it follows that $\ell = -\ell+N$, and hence $\ell = N/2$, so $\ell$ must be an integer or half-integer. The eigenfunctions are characterized by \begin{equation*}
    \boxed{L^2f_{\ell}^m = \hbar^2\ell(\ell+1)f_{\ell}^m;\;\;L_zf_{\ell}^m=\hbar mf_{\ell}^m}
\end{equation*}
where $\ell = 0,1/2,1,3/2,...; m = -\ell,-\ell+1,...,\ell-1,\ell$. 
\begin{note}
    A particle cannot have a determinate angular momentum vector any more than it can simultaneously have a determinate position and momentum (and for the same reason). If $L_z$ has a well-defined value, then $L_x$ and $L_y$ do not.
\end{note}
We shall soon see that the eigenfunctions of $f_{\ell}^m = Y_{\ell}^m$.

We need to write $L_x,L_y,$ and $L_z$ in spherical coordinates. First $L = i\hbar(r\times \nabla)$, so \begin{equation*}
    L = -i\hbar\left(\hat{\phi}\partial_{\theta} - \hat{\theta}\frac{1}{\sin\theta}\partial_{\phi}\right)
\end{equation*}
Thus \begin{equation*}
    L = -i\hbar\left[(-\sin\phi\hat{i}+\cos\phi\hat{j})\partial_{\theta}-(\cos\theta\cos\phi\hat{i}+\cos\theta\sin\phi\hat{j}-\sin\theta\hat{k})\frac{1}{\sin\theta}\partial_{\phi}\right]
\end{equation*}
so \begin{align*}
    L_x &= -i\hbar(-\sin\phi\partial_{\theta} - \cos\phi\cot\theta\partial_{\phi}) \\
    L_y &= -i\hbar(+\cos\phi\partial_{\theta} - \sin\phi\cot\theta\partial_{\phi}) \\
    L_z &= -i\hbar\partial_{\phi}
\end{align*}
Using Euler's formula we can write the raising and lowering operators in spherical coordinates as \begin{equation*}
    L_{\pm} = \pm\hbar e^{\pm i\phi}(\partial_{\theta}\pm i\cot\theta\partial_{\phi})
\end{equation*}
Then \begin{equation*}
    L^2 = -\hbar^2\left[\frac{1}{\sin\theta}\partial_{\theta}(\sin\theta\partial_{\theta}) + \frac{1}{\sin^2\theta}\partial_{\phi}^2\right]
\end{equation*}
Then $f_{\ell}^m$ being an eigenfunctionn of $L^2$ with eigenvalue $\hbar^2\ell(\ell+1)$ is saying that it satisfies the angular equation, and the statement that it is an eigenfunction of $L_z$ with eigenvalue $m\hbar$ is the same as saying it satisfies the azimuthal equation. From our previous work this implies it is the spherical harmonics. Using these observations we can write the Schr\"{o}dinger wave equation as \begin{equation*}
    \frac{1}{2mr^2}\left[-\hbar^2\partial_r(r^2\partial_r) + L^2\right]\psi + V\psi = E\psi
\end{equation*}


\section{Spin}

There is an \Emph{intrinsic} angular momentum which elementary particles carry that is not associated with any spatial motion, and for which we call spin $\mathbf{S}$. We require that it satisfies the same commuting relations \begin{equation*}
    [S_x,S_y] = i\hbar S_z,\;\;[S_y,S_z] = i\hbar S_x,\;\;[S_z,S_x] = i\hbar S_y
\end{equation*}
It follows that the eigenvectors of $S^2$ and $S_z$ satisfy \begin{equation*}
    S^2\ket{s\,m} = \hbar^2s(s+1)\ket{s\,m};\;\;S_z\ket{s\,m} = \hbar m\ket{s\,m}
\end{equation*}
and \begin{equation*}
    S_{\pm}\ket{s\,m} = \hbar\sqrt{s(s+1)-m(m\pm 1)}\ket{s\,(m\pm1)}
\end{equation*}
where $S_{\pm} = S_x \pm iS_y$. In this case the eigenfunctions are not functions of $\theta$ or $\phi$ at all. We also consider half-integer values of $s$ in this case, with $s = 0,1/2,1,3/2,...$. Each elementary particle has a specific and immutable value of $s$, which we call its spin.

\subsection{Spin 1/2}

All ordinary matter (protons, neutrons, and electrons), quarks, and leptons are spin 1/2 particles. For $S_z$ we have just two eigenstates, which we call \Emph{spin up}, $\ket{\frac{1}{2}\,\frac{1}{2}}$, and \Emph{spin down}, $\ket{\frac{1}{2}\,\left(-\frac{1}{2}\right)}$. This forms a basis for this two-level quantum system. In its coordinate representation we denote spin states by \Emph{spinor}s: $$\chi = \begin{pmatrix} a \\ b \end{pmatrix} = a\chi_+ + b\chi_-$$
In this basis our operators take the form \begin{equation*}
    [S^2] = \frac{3}{4}\hbar^2I_2, [S_z] = \frac{\hbar}{2}\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
\end{equation*}
as well as \begin{equation*}
    [S_+] = \hbar \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix},\;[S_-] = \hbar\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}
\end{equation*}
and \begin{equation*}
    [S_x] = \frac{\hbar}{2}\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix},\;[S_y] = \frac{\hbar}{2}\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} 
\end{equation*}
We can write these in terms of the associated Pauli matrices: \begin{equation*}
    \sigma_x = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}, \sigma_y = \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}, \sigma_z = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
\end{equation*}
$S_x$ and $S_y$, by symmetry, have the same eigenvalues as $S_z$, and we can perform a change of basis to find the probability of measuring an up or down spin along one of their axes.

\subsection{Electron in a Magnetic Field}


\subsection{Addition of Angular Momenta}

We now consider two particles with spins $s_1$ and $s_2$. Consider the first in state $\ket{s_1\;m_1}$ and the second is in state $\ket{s_2\;m_2}$. We denote the composite state by $\ket{s_1\;s_2\;m_1\;m_2}$: \begin{align*}
    {S^{(1)}}^2\ket{s_1\;s_2\;m_1\;m_2} &= s_1(s_1+1)\hbar^2\ket{s_1\;s_2\;m_1\;m_2} \\
    {S^{(2)}}^2\ket{s_1\;s_2\;m_1\;m_2} &= s_2(s_2+1)\hbar^2\ket{s_1\;s_2\;m_1\;m_2} \\
    S_z^{(1)}\ket{s_1\;s_2\;m_1\;m_2} &= m_1\hbar \ket{s_1\;s_2\;m_1\;m_2} \\
    S_z^{(2)}\ket{s_1\;s_2\;m_1\;m_2} &= m_2\hbar \ket{s_1\;s_2\;m_1\;m_2}
\end{align*}

What is the total angular momentum, $\mathbf{S}= \mathbf{S}^{(1)}+\mathbf{S}^{(2)}$, of the system? First the $z$ component would simply be \begin{equation*}
    S_z\ket{s_1\;s_2\;m_1\;m_2} = (S_z^{(1)}+S_z^{(2)})\ket{s_1\;s_2\;m_1\;m_2} = \hbar(m_1+m_2)\ket{s_1\;s_2\;m_1\;m_2}
\end{equation*}
where we write $m = m_1+m_2$. Before finding $s$ generally we consider a specific example.

\begin{eg}
    Consider the case of two spin $1/2$ particles\---say, the electron and proton in the ground state of hydrogen. We have four possibilities: \begin{align*}
        \ket{\uparrow\uparrow} &= \ket{\frac{1}{2}\;\frac{1}{2}\;\frac{1}{2}\;\frac{1}{2}},\;\;m=1 \\
        \ket{\uparrow\downarrow} &= \ket{\frac{1}{2}\;\frac{1}{2}\;\frac{1}{2}\;\frac{-1}{2}},\;\;m=0 \\
        \ket{\downarrow\uparrow} &= \ket{\frac{1}{2}\;\frac{1}{2}\;\frac{-1}{2}\;\frac{1}{2}},\;\;m=0 \\
        \ket{\downarrow\downarrow} &= \ket{\frac{1}{2}\;\frac{1}{2}\;\frac{-1}{2}\;\frac{-1}{2}},\;\;m=-1 
    \end{align*}
    Why do we have two $m = 0$ states? To untable this we consider the lowering operator $S_- = S_-^{(1)} + S_-^{(2)}$ to the state $\ket{\uparrow\uparrow}$: \begin{equation*}
        S_-\ket{\uparrow\uparrow} = \hbar(\ket{\downarrow\uparrow}+\ket{\uparrow\downarrow})
    \end{equation*}
    Then the three states with $s = 1$ in the notation $\ket{s\;m}$ are \begin{align*}
        \ket{1\;1} &= \ket{\uparrow\uparrow} \\
        \ket{1\;0} &= \frac{1}{\sqrt{2}}(\ket{\uparrow\downarrow}+\ket{\downarrow\uparrow}) \\
        \ket{1\;-1} &= \ket{\downarrow\downarrow}
    \end{align*}
    In this case we say $s = 1$ is a \textbf{triplet} combination. Meanwhile the orthogonal state with $m = 0$ carries $s = 0$: $\ket{0\;0} = \frac{1}{\sqrt{2}}(\ket{\uparrow\downarrow}-\ket{\downarrow\uparrow})$ is a singlet state. 
\end{eg}

When we combine spin $s_1$ with spin $s_2$ we get every spin from $(s_1+s_2)$ down to $(s_1-s_2)$, or $(s_2-s_1)$ if $s_2 > s_2$, in integer steps: \begin{equation*}
    s = (s_1+s_2), (s_1+s_2-1),...,|s_1-s_2|
\end{equation*}
Roughly, the highest total spin occurs when the individual spins are aligned parallel to one another, and the lowest occurs when they are antiparallel. For example, if a hydrogen atom is in a state $\psi_{n\ell m}$, the net angular momentum of the electron is $\ell+1/2$ or $\ell-1/2$; if you now throw in the spin of the proton, the atom's total angular momentum quantum number is $\ell+1,\ell,$ or $\ell-1$.

The combined state $\ket{s\;m}$ with total spin $s$ and $z$-component $m$ will be some linear combination of the composite states $\ket{s_1\;s_2\;m_1\;m_2}$: \begin{equation*}
    \ket{s\;m} = \sum_{m_1+m_2=m}C_{m_1m_2m}^{s_1s_2s}\ket{s_1\;s_2\;m_1\;m_2}
\end{equation*}
The constants $C_{m_1m_2m}^{s_1s_2s}$ are called \Emph{Clebsch-Gordan coefficients}. 










%%%%%%%%%%%%%%%%%%%%%% Chapter 1.5
\chapter{Identical Particles}

\section{Two-Particle Systems}

The state of a two-particle system is a function of the coordinates of each particle, $\mathbf{r}_1$ and $\mathbf{r}_2$, and time: \begin{equation*}
    \Psi(\mathbf{r}_1,\mathbf{r}_2,t)
\end{equation*}
Its time evolution is determined by the Schr\"{o}dinger equation $i\hbar\frac{\partial \Psi}{\partial t} = \hat{H}\Psi$, where $\hat{H}$ is the Hamiltonian for the whole: \begin{equation*}
    \hat{H} = -\frac{\hbar^2}{2m_1}\nabla_1^2 - \frac{\hbar^2}{2m_2}\nabla_2^2 + V(\mathbf{r}_1,\mathbf{r}_2,t)
\end{equation*}
The statistical interpretation carries over with $|\Psi|^2d^3\mathbf{r}_1d^3\mathbf{r}_2$ being the probability of finding particle $1$ in volume $d^3\mathbf{r}_1$ and particle $2$ in volume $d^3\mathbf{r}_2$. As always $\Psi$ is normalized. For time-independent potentials we have a complete set of solutions by separation of variables, $\Psi(\mathbf{r}_1,\mathbf{r}_2,t) = \psi(\mathbf{r}_1,\mathbf{r}_2)e^{-iEt/\hbar}$ where $\psi$ satisfies the time-independent Schr\"{o}dinger equation \begin{equation*}
    -\frac{\hbar^2}{2m_1}\nabla_1^2\psi - \frac{\hbar^2}{2m_2}\nabla_2^2\psi + V\psi = E\psi
\end{equation*}
In two special cases we can reduce to one-particle problems. The case of \Emph{noninteracting particles} where $V(\mathbf{r}_1,\mathbf{r}_2) = V_1(\mathbf{r}_1)+V_2(\mathbf{r}_2)$, and then by separation of variables $\psi(\mathbf{r}_1,\mathbf{r}_2) = \psi_a(\mathbf{r}_1)\psi_b(\mathbf{r}_2)$. In this case the time-independent Schr\"{o}dinger equation splits into two pieces: one for $a$ and one for $b$, with energies $E_a$ and $E_b$ such that $E=E_a+E_b$. Then we can write \begin{equation*}
    \Psi(\mathbf{r}_1,\mathbf{r}_2,t) = (\psi_a(\mathbf{r}_1)e^{-iE_at/\hbar})(\psi_b(\mathbf{r}_2)e^{-iE_bt/\hbar}) = \Psi_a(\mathbf{r}_1,t)\Psi_b(\mathbf{r}_2,t)
\end{equation*}
Any linear combination of wavefunctions of this form will also satisfy the time-dependent Schr\"{o}dinger equation. For example \begin{equation*}
    \Psi(\mathbf{r}_1,\mathbf{r}_2,t) = \frac{3}{5}\Psi_a(\mathbf{r}_1,t)\Psi_b(\mathbf{r}_2,t) + \frac{4}{5}\Psi_c(\mathbf{r}_1,t)\Psi_d(\mathbf{r}_2,t)
\end{equation*}
is an example of two \Emph{entangled particles}, where knowing the state of one will give the state of the other. An entangled state is one that cannot be written as a product of single-particle states.

Our other simplifying case is when we have \Emph{central potentials}, so $V(\mathbf{r}_1,\mathbf{r}_2) = V(|\mathbf{r}_1 -\mathbf{r}_2|)$.

In general, two particles will be subject to both external forces and to mutual interactions, making the analysis more complicated.

\subsection{Bosons and Fermions}

Consider two non-interacting particles in states $\psi_a(\mathbf{r})$ and $\psi_b(\mathbf{r})$, respectively. Then $\psi(\mathbf{r}_1,\mathbf{r}_2) = \psi_a(\mathbf{r}_1)\psi_b(\mathbf{r}_2)$, assuming the particles are \Emph{distinguishable}.To deal with particles that are \Emph{indistinguishable in principle} we construct a wave function that is noncommittal as to which particle is in which state. We can either do this symmetrically or antisymmetrically: \begin{equation*}
    \psi_{\pm}(\mathbf{r}_1,\mathbf{r}_2) = A[\psi_a(\mathbf{r}_1)\psi_b(\mathbf{r}_2) \pm \psi_b(\mathbf{r}_1)\psi_a(\mathbf{r}_2)]
\end{equation*}
The type of identical particles with symmetric wavefunctions are \Emph{bosons}, and the identical particles with antisymmetric wavefunctions are \Emph{fermions}. It happens that all particles with integer spin are bosons and all particles with half integer spin are fermions.

\begin{cor}
    Two identical fermions cannot occupy the same state. For if $\psi_a = \psi_b$, then $\psi_- = 0$ which is not normalizable and hence not a wavefunction. This is the \Emph{Pauli exclusion principle}.
\end{cor}

\subsection{Exchange Forces}

Suppose one particle is in state $\psi_a(x)$ and the other is in state $\psi_b(x)$, and these two states are orthogonal and normalized. If the two particles are distinguishable with number $1$ in state $\psi_a$, then the combined wavefunction is $\psi(x_1,x_2) = \psi_a(x_1)\psi_b(x_2)$. If they are identical bosons the composite is $\psi_+(x_1,x_2) = \frac{1}{\sqrt{2}}[\psi_a(x_1)\psi_b(x_2)+\psi_b(x_1)\psi_a(x_2)]$, and if they are identical fermions it is $\psi_-(x_1,x_2) = \frac{1}{\sqrt{2}}[\psi_a(x_1)\psi_b(x_2)-\psi_b(x_1)\psi_a(x_2)]$.

Now we consider the expectation value of the square of the separation distance between the particles: \begin{equation*}
    \langle (x_1-x_2)^2\rangle = \langle x_1^2\rangle + \langle x_2^2\rangle - 2\langle x_1x_2\rangle
\end{equation*}

In the case of distinguishable particles $\langle x_1^2\rangle = \langle x^2\rangle_a$, $\langle x_2^2\rangle = \langle x^2\rangle_b$, and $\langle x_1x_2\rangle = \langle x\rangle_a\langle x\rangle_b$. In this case then \begin{equation*}
    \langle (x_1-x_2)^2\rangle_d = \langle x^2\rangle_a + \langle x^2\rangle_b - 2\langle x\rangle_a\langle x\rangle_b
\end{equation*}
Now let's consider the case of identical particles. Here $\langle x_1^2\rangle = \frac{1}{2}(\langle x^2\rangle_a+\langle x^2\rangle_b)$, and similarly $\langle x_2^2\rangle = \frac{1}{2}(\langle x^2\rangle_b+\langle x^2\rangle_a)$. Naturally $\langle x_1^2\rangle = \langle x_2^2\rangle$ since they are indistinguishable. Then $\langle x_1x_2\rangle = \langle x\rangle_a\langle x\rangle_b \pm |\langle x\rangle_{ab}|^2$, where $\langle x\rangle_{ab} := \int x\psi_a(x)^*\psi_b(x)dx$, so \begin{equation*}
    \langle (x_1-x_2)^2\rangle_{\pm} = \langle x^2\rangle_a+\langle x^2\rangle_b - 2\langle x\rangle_a\langle x\rangle_b \mp 2|\langle x\rangle_{ab}|^2 = \langle (x_1-x_2)^2\rangle_d \mp 2|\langle x\rangle_{ab}|^2
\end{equation*}
Thus, identical bosons tend to be closer together and identical fermions tend to be farther apart than distinguishable particles in the same two states. Notice that $\langle x\rangle_{ab}$ vanishes unless the two wave functions actually overlap: if $\psi_a(x)$ is zero wherever $\psi_b(x)$ is non-zero, its value will be zero. We often use this fact to treat electrons with non-overlapping wave functions as distinguishable.

The force we see pulling identical bosons together and pushing identical fermions apart here is called the \Emph{exchange force} (note it isn't a real force).

\subsection{Spin}

The \Emph{complete state} of an electron includes not only its position wave function but also a spinor, describing the orientation of its spin: $\psi(\mathbf{r})\chi$. When we put together the two-particle state we find $\psi(\mathbf{r}_1,\mathbf{r}_2)\chi(1,2)$, and this whole function, not just the spatial part, has to be antisymmetric with respect to exchange.

\subsection{Generalized Symmetrization Principle}

Let $\hat{P}$ denote the \Emph{exchange operator}, which interchanges two particles. Evidently $\hat{P}^2 = 1$, the identity, so its eigenvalues are $\pm 1$. Now if the two particles are identical the Hamiltonian must treat them the same: $m_1 = m_2$ and $V(\mathbf{r}_1,\mathbf{r}_2,t) = V(\mathbf{r}_2,\mathbf{r}_1,t)$. It follows that $\hat{P}$ and $\hat{H}$ are compatible observables, i.e. $[\hat{P},\hat{H}] = 0$, so $d\langle \hat{P}\rangle/dt = 0$. If a system starts in a symmetric or antisymmetric eigenstate of $\hat{P}$, it will stay that way forever. The \Emph{symmetrization axiom} says that for identical particles the state is not merely allowed, but required to satisfy $\ket{(1,2)} = \pm\ket{(2,1)}$ with $+$ for bosons and $-$ for fermions. In general, if $n$ identical particles occur, the state must be symmetric or antisymmetric under the interchange of any two of the particles. 




%%%%%%%%%%%%%%%%%%%%%% Chapter 1.6
\chapter{Symmetries and Conservation Laws}

\section{Introduction}

Conservation laws in classical and quantum mechanics result from symmetries. What is a symmetry? It is some transformation that leaves the system unchanged. When we say that a system has a symmetry, we mean that the Hamiltonian is unchanged by some transformation, such as a rotation or a translation.

\subsection{Transformations in Space}

The \textbf{translation operator} takes a function and shifts it a distance $a$. The operator that accomplishes this is defined by \begin{equation*}
    \hat{T}(a)\psi(x) = \psi'(x) = \psi(x-a)
\end{equation*}
which shifts the function right by an amount $a$.

The operator that reflects a function about the origin, the \textbf{parity operator} in one dimension is defined by \begin{equation*}
    \hat{\prod}\psi(x) = \psi'(x) = \psi(-x)
\end{equation*}
In three dimensions parity changes the sign of all three coordinates.

Finally, the operator that rotates a function about the $z$ axis through an angle $\varphi$ is most naturally expressed in polar coordinates as \begin{equation*}
    \hat{R}_z(\varphi)\psi(r,\theta,\phi) = \psi(r,\theta,\phi-\varphi)
\end{equation*}


\section{The Translation Operator}

We can express $\hat{T}(a)$ in terms of the momentum operator, to which it is intimately related. To that end, we replace $\psi(x-a)$ by its Taylor series \begin{align*}
    \hat{T}(a)\psi(x) &= \psi(x-a) = \sum_{n=0}^{\infty}\frac{1}{n!}(-a)^n\frac{d^n}{dx^n}\psi(x)  \\
    &= \sum_{n=0}^{\infty}\frac{1}{n!}\left(\frac{-ia}{\hbar}\hat{p}\right)^n\psi(x)
\end{align*}
The right hand side of this equation is the exponential, so \begin{equation*}
    \hat{T}(a)=\text{exp}\left[-\frac{ia}{\hbar}\hat{p}\right]
\end{equation*}
We say that momentum is the \textbf{``generator" of translations}.

Note that $\hat{T}(a)$ is a unitary operator \begin{equation*}
    \hat{T}(a)^{-1} = \hat{T}(-a) = \hat{T}(a)^{\text{\textdagger}}
\end{equation*}
The first equality is physically obvious and the second equality then follows from taking the adjoint of the exponential.

\subsection{How Operators Transform}

We can also consider what it means to translate an operator. The transformated operator is defined to be the operator that gives the same expectation value in the untranslated state $\psi$ as does the original operator in the translated state: \begin{equation*}
    \left\langle \hat{T}\psi\right\vert\hat{Q}\left\vert \hat{T}\psi\right\rangle = \bra{\psi}\hat{Q}'\ket{\psi}
\end{equation*}
We have two ways to calculate the effect of a translation on an expectation value: we can shift the wave function over some distance (an \textbf{active transformation}), or one could leave the wave function and shift the origin of the coordinate system in the opposite direction (a \textbf{passive transformation}). The transformed operator is the operator in this shifted coordinate system. Using our previous equation \begin{equation*}
    \bra{\psi}\hat{T}^{\text{\textdagger}}\hat{Q}\hat{T}\ket{\psi} = \bra{\psi}\hat{Q}'\ket{\psi}
\end{equation*}
As this holds for all $\psi$ \begin{equation*}
    \boxed{\hat{Q}' =\hat{T}^{\text{\textdagger}}\hat{Q}\hat{T}}
\end{equation*}
It can be shown that $\hat{x}' = \hat{x}+a$ for $\hat{T}(a)$, and $\hat{p}' = \hat{p}$. From this a general operator can be written \begin{equation*}
    \hat{Q}'(\hat{x},\hat{p}) = \hat{T}^{\text{\textdagger}}\hat{Q}(\hat{x},\hat{p})\hat{T} = \hat{Q}(\hat{x}',\hat{p}') = \hat{Q}(\hat{x}+a,\hat{p})
\end{equation*}

\subsection{Translational Symmetry}

A system is \textbf{translationally invariant}, i.e. has translational symmetry, if the Hamiltonian is unchanged by the transformation \begin{equation*}
    \hat{H}' = \hat{T}^{\text{\textdagger}}\hat{H}\hat{T} = \hat{H}
\end{equation*}
Because $\hat{T}$ is unitary, this is equivalent to $\hat{H}\hat{T} = \hat{T}\hat{H}$, so a system has translational symmetry if and only if \begin{equation*}
    \left[\hat{H},\hat{T}\right] = 0
\end{equation*}


For a particle of mass $m$ moving in a one-dimensional potential, the Hamiltonian is \begin{equation*}
    \hat{H} = \frac{\hat{p}^2}{2m} + V(x)
\end{equation*}
The transformed Hamiltonian is $\hat{H}' = \frac{\hat{p}^2}{2m} + V(x+a)$, so translational symmetry is equivalent to \begin{equation*}
    V(x+a) = V(x)
\end{equation*}
in this case. Such a system can either have a \textbf{continuous translational symmetry} if the potential is constant, or a periodic potential such as an electron might encounter in a crystal; such a system is said to have \textbf{discrete translational symmetry}.

For a system with a discrete translational symmetry, the most important consequence is Bloch's theorem. Recall that if two operators commute they have a complete set of simultaneous eigenstates. This means that if the Hamiltonian is translationally invariant, then the eigenstates $\psi(x)$ of the Hamiltonian can be chosen to be simultaneously eigenstates of $\hat{T}$: \begin{equation*}
    \hat{H}\psi(x) = E\psi(x),\;\;\;\hat{T}(a)\psi(x) = \lambda\psi(x)
\end{equation*}
Since $\hat{T}$ is unitary, $|\lambda| = 1$, so we can write $\lambda = e^{i\phi}$. By convention we write $\phi = -qa$ where $\hbar q$ is called the \textbf{crystal momentum}. Therefore the stationary states of a particle of mass $m$ moving in a periodic potential have the property \begin{equation*}
    \psi(x-a) = e^{-iqa}\psi(x)
\end{equation*}
We can also write \begin{equation*}
    \psi(x) = e^{iqx}u(x)
\end{equation*}
where $u(x)$ is a periodic function of $x$ with $u(x+a) = u(x)$ and $e^{iqx}$ is a traveling wave with wavelength $2\pi/q$. This is \textbf{Bloch's theorem} and it says that the stationary states of a particle in a periodic potential are periodic functions multiplying traveling waves.


If a system has continuous translation symmetry then the Hamiltonian commutes with $\hat{T}(a)$ for any $a$. In this case we consider an \textbf{infinitesimal translation} \begin{equation*}
    \hat{T}(\delta) = e^{-i\delta \hat{p}/\hbar}\approx 1-i\frac{\delta}{\hbar}\hat{p}
\end{equation*}
where $\delta$ is an infinitesimal length.

If the Hamiltonian has continuous translational symmetry, then it must be unchanged under any translation, including an infinitesimal one. Equivalently it commutes with the translation operator, and hence \begin{equation*}
    \left[\hat{H},\hat{T}(\delta)\right] = \left[\hat{H},1-i\frac{\delta}{\hbar}\hat{p}\right] = 0\implies \left[\hat{H},\hat{p}\right] = 0
\end{equation*}
So if the Hamiltonian has continuous translational symmetry it must commute with the momentum operator. Further by the \textbf{generalized Ehrenfest's theorem} we have momentum conservation \begin{equation*}
    \frac{d}{dt}\langle p\rangle = \frac{i}{\hbar}\left\langle \left[\hat{H},\hat{p}\right]\right\rangle = 0
\end{equation*}



\section{Conservation Laws}

In quantum mechanics a system does not in general have a definite energy or momentum before a process begines (or afterward). What does it mean, in that case, to say that the observable $\hat{Q}$ is or is not conserved? Here are two possibilities: \begin{itemize}
    \item \textbf{First definition}: The expectation value $\langle \hat{Q}\rangle$ is independent of time
    \item \textbf{Second definition}: The probability of getting any particular value is independent of time
\end{itemize}
For simplicity suppose $\partial \hat{Q}/\partial t = 0$. In that case we have by generalized Ehrenfest that the expectation value of $\hat{Q}$ is independent of time if the operator commutes with the Hamiltonian. It so happens the same criterion guarantees the second definition.

Recall that the probability of getting the result $q_n$ in a measurement of $\hat{Q}$ at time $t$ is \begin{equation*}
    P(q_n) = |\braket{f_n}{\Psi(t)}|^2
\end{equation*}
where $f_n$ is the corresponding eigenvector. We know the time evolution of the wave function is \begin{equation*}
    \ket{\Psi(t)} = \sum_me^{-iE_mt/\hbar}c_m\ket{\psi_m}
\end{equation*}
where the $\ket{\psi_n}$ are the eigenstates of $\hat{H}$, and therefore \begin{equation*}
    P(q_n) = \left|\sum_me^{-iE_mt/\hbar}c_m\braket{f_n}{\psi_m}\right|^2
\end{equation*}
Since $\hat{Q}$ and $\hat{H}$ commute, we can find a complete set of simultaneous eigenstates, so without loss of generality $\ket{f_n} = \ket{\psi_n}$, so by orthonormality \begin{equation*}
    P(q_n) = |c_n|^2
\end{equation*}
which is time independent.

\section{Parity}

\subsection{Parity in One Dimension}

A spatial inversion is implemented by the parity operator $\hat{\prod}$; in one dimension, \begin{equation*}
    \hat{\prod}\psi(x) = \psi'(x) = \psi(-x)
\end{equation*}
Evidently, the parity operator is an involution. It is also Hermitian, $\hat{\prod}^{\text{\textdagger}} = \hat{\prod}$, and so unitary. In this case the operator transform is \begin{equation*}
    \hat{Q}' = \hat{\prod}^{\text{\textdagger}}\hat{Q}\hat{\prod} = \hat{\prod}\hat{Q}\hat{\prod}
\end{equation*}
Our basic results are that \begin{align*}
    \hat{\prod}^{\text{\textdagger}}\hat{x}\hat{\prod} &= -\hat{x} \\
    \hat{\prod}^{\text{\textdagger}}\hat{p}\hat{\prod} &= -\hat{p}
\end{align*}

\begin{defn}
    A system is said to have \textbf{inversion symmetry} if the Hamiltonian is invariant under the parity transformation, or using the unitarity of the parity operator, $\left[\hat{H},\hat{\prod}\right] = 0$. 
\end{defn}
Inversion symmetry has the following implications: \begin{itemize}
    \item If $\psi_n$ is a simultaneous eigenstate of $\hat{\prod}$ and $\hat{H}$, then $\psi_n$ is either even or odd.
    \item For a one-dimensional potential, inversion symmetry is equivalent to the potential being an even function of position.\item According to Ehrenfest's theorem, \begin{equation*}
            \frac{d}{dt}\langle \prod\rangle = \frac{i}{\hbar}\left\langle \left[\hat{H},\hat{\prod}\right]\right\rangle = 0
    \end{equation*}
        so parity is conserved for a particle moving in a symmetric potential.
\end{itemize}

\subsection{Parity in Three Dimensions}

The spatial inversion generated by the parity operator in three dimensions is \begin{equation*}
    \hat{\prod}\psi(r) = \psi(-r)
\end{equation*}
and the transforms of $\hat{r}$ and $\hat{p}$ are precisely their negatives. Thus any other operator transforms as \begin{equation*}
    \hat{\prod}^{\text{\textdagger}}\hat{Q}(r,p)\hat{\prod} = \hat{Q}(-r,-p)
\end{equation*}

\subsection{Parity Selection Rules}

\textbf{Selection rules} tell you when a matrix element is zero based on the symmetry of the situation. Recall a matrix element is an object of the form $\bra{b}\hat{Q}\ket{a}$, with $\ket{b}$ and $\ket{a}$ particular orthonormal basis vectors.

Consider the electric dipole moment operator, $\hat{p}_e = q\hat{r}$. The selection rules for this operator determine which atomic transitions are allowed and which are forbidden. It is odd under parity since $\hat{r}$ is, and for eigenstates $\psi_{n\ell m}$ and $\psi_{n'\ell'm'}$ denoted in bra and ket notation, \begin{align*}
    \bra{n'\ell'm'}\hat{p}_e\ket{n\ell m} &= -\bra{n'\ell'm'}\hat{\prod}^{\text{\textdagger}}\hat{p}_e\hat{\prod}\ket{n\ell m} \\
    &= -\bra{n'\ell'm'}(-1)^{\ell'}\hat{p}_e(-1)^{\ell}\ket{n\ell m} \\
    &= (-1)^{\ell+\ell'+1}\bra{n'\ell'm'}\hat{p}_e\ket{n\ell m}
\end{align*}
Consequently we have that \begin{equation*}
    \bra{n'\ell'm'}\hat{p}_e\ket{n\ell m} = 0
\end{equation*}
if $\ell+\ell'$ is even; this is \textbf{Laporte's rule}. In general it applied to any operator that is odd under parity. If we have an operator which is even under parity, then its matrix element is zero if $\ell+\ell'$ is odd.


\section{Rotational Symmetry}


The operator that rotates a function about the $z$ axis by an angle $\varphi$ is \begin{equation*}
    \hat{R}_z(\varphi)\psi(r,\theta,\phi) = \psi'(r,\theta,\phi) = \psi(r,\theta,\phi-\varphi)
\end{equation*}
is closely related to the $z$ component of angular momentum. In fact, \begin{equation*}
    \hat{R}_z(\varphi) = \exp\left[-\frac{i\varphi}{\hbar}\hat{L}_z\right]
\end{equation*}
and we say that $\hat{L}_z$ is the \textbf{generator of rotations} about the $z$ axis. 

To see how $\hat{r}$ and $\hat{p}$ transform under rotations consider an infinitesimal rotation: \begin{equation*}
    \hat{R}_z(\delta) \approx 1 - \frac{i\delta}{\hbar}\hat{L}_z
\end{equation*}
Then the operator $\hat{x}$ transforms as \begin{equation*}
    \hat{x}' \approx \hat{x}-\delta\hat{y}
\end{equation*}
and similarly $\hat{y}'\approx \hat{y}+\delta\hat{x}$ and $\hat{z}' = \hat{z}$. In matrix form we expect \begin{equation*}
    \begin{pmatrix} \hat{x}' \\ \hat{y}' \\ \hat{z}' \end{pmatrix} = \begin{pmatrix} \cos\varphi & -\sin\varphi & 0 \\ \sin\varphi & \cos\varphi & 0 \\ 0 & 0& 1 \end{pmatrix}\begin{pmatrix} \hat{x} \\ \hat{y} \\ \hat{z}\end{pmatrix}
\end{equation*}
But for $\delta$ infinitesimal $\cos\varphi\rightarrow 1$ and $\sin\varphi \rightarrow \delta$.


\subsection{Rotations in 3D}

In general for a unit vector $n$ describing an axis of rotation, \begin{equation*}
    \hat{R}_n(\varphi) = \exp\left[-\frac{i\varphi}{\hbar}n\cdot\hat{L}\right]
\end{equation*}
Thus indeed angular momentum is the \textbf{generator of rotations}.

Any operator with three components that transforms the same way as the position operator under rotations is called a \textbf{vector operator}. In particular $\hat{V}' = D\hat{V}$ where $D$ is the same matrix as in $\hat{r}' = D\hat{r}$. This transformation rule follows from the commutation relations \begin{equation*}
    \boxed{\left[\hat{L}_i,\hat{V}_j\right] = i\hbar \epsilon_{ijk}\hat{V}_k}
\end{equation*}

\begin{table}[H]
    \centering
    \begin{tabular}{cccc}
        \hline \hline
         & parity & rotations & examples  \\ \hline
         true vector $\hat{V}$ & $\left\{\hat{\prod},\hat{V}_i\right\} = 0$ & $\left[\hat{L}_i,\hat{V}_j\right] = i\hbar \epsilon_{ijk}\hat{V}_k$ & $\hat{r},\hat{p}$ \\ 
         pseudovector $\hat{V}$ & $\left[\hat{\prod},\hat{V}_i\right] = 0$ & $\left[\hat{L}_i,\hat{V}_j\right] = i\hbar \epsilon_{ijk}\hat{V}_k$ & $\hat{L}$ \\
        true scalar $\hat{f}$ & $\left[\hat{\prod},\hat{f}\right] = 0$ & $\left[\hat{L}_i,\hat{f}\right] = 0$ & $\hat{r}\cdot\hat{r}$ \\
        pseudoscalar $\hat{f}$ & $\left\{\hat{\prod},\hat{f}\right\} = 0$ & $\left[\hat{L}_i,\hat{f}\right] = 0$ & \\
    \end{tabular}
    \caption{Classification of operators}
    \label{tab:class}
\end{table}

where $\{\hat{f},\hat{g}\} = \hat{f}\hat{g} + \hat{g}\hat{f}$. Note a \textbf{scalar operator} is a single quantity that is unchanged by rotations: i.e.e $\left[\hat{L}_i,\hat{f}\right] = 0$.


\subsection{Continuous Rotational Symmetry}

For a particle of mass $m$ moving in a potential $V(r)$, the Hamiltonian is rotationally invariant if $V(r) = V(|r|)$, i.e. it has a central potential. In this case the Hamiltonian commutes with a rotation by any angle about an arbitrary axis.\begin{equation*}
    \left[\hat{H},\hat{R}_n(\varphi)\right] = 0
\end{equation*}
Using the infinitesimal expansion of the rotation operator, we find that $\hat{H}$ commutes with all components of $\hat{L}$, so in particular $$\left[\hat{H},\hat{L}\right] = 0$$
From Ehrenfest's theorem we then have that for a central potential angular momentum is conserved - i.e. \textbf{conservation of angular momentum is a consequence of rotational invariance}.

In this case we also have that $\hat{H},\hat{L}_z$, and $\hat{L}^2$ form a \textbf{complete set of compatible observables} for the bound states of a central potential. Thus we can choose a simultaneous eigenbasis for these operators: \begin{align*}
    \hat{H}\psi_{n\ell m} &= E_n\psi_{n\ell m} \\
    \hat{L}_z\psi_{n\ell m} &= m\hbar \psi_{n\ell m} \\
    \hat{L}^2\psi_{n\ell m} &= \ell(\ell+1)\hbar^2\psi_{n\ell m}
\end{align*}
Saying they are complete means the quantum numbers $n,\ell,$ and $m$ uniquely specify a bound state of the Hamiltonian.


\section{Degeneracy}

Symmetry is the source of most degeneracy in quantum mechanics. Recall a symmetry implies the existence of an operator $\hat{Q}$ that commutes with the Hamiltonian. Why does symmetry lead to degeneracy in the energy spectrum? The basic idea is that if we have a stationary state $\ket{\psi_n}$, and then $\ket{\psi'_n} = \hat{Q}\ket{\psi_n}$ is a stationary state with the same energy. Indeed \begin{equation*}
    \hat{H}\ket{\psi_n'} = \hat{H}\left(\hat{Q}\ket{\psi_n}\right) = \hat{Q}\hat{H}\ket{\psi_n} = \hat{Q} E_n\ket{\psi_n} = E_n\hat{Q}\ket{\psi_n} = E_n\ket{\psi_n'}
\end{equation*}

Symmetry does not always lead to degeneracy, and continuous symmetries do not lead to infinite degeneracy. The reason is that the states $\ket{\psi_n}$ and $\ket{\psi_n'}$ might be the same. In fact, if there is only a single symmetry operator $\hat{Q}$ (or multiple which all commute), you do not get degeneracy in the spectrum. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Part 2
\part{Applications}

%%%%%%%%%%%%%%%%%%%%%% Chapter 2.1
\chapter{Time-Independent Perturbation Theory}

\section{Nondegenerate Perturbation Theory}

Let $H^0$ denote a known potential with known solutions to the Schr\"{o}dinger equation, $\ket{\psi_n^0}$, with eigenvalues $E_n^0$. We perturb the solution slightly, taking $H = H^0 + \lambda H^1$. In general we cannot solve this exactly, and instead we must rely on \textbf{perturbation theory}. We write solutions to the Schr\"{o}dinger equation for this potential in power series form $$\ket{\psi_n} = \ket{\psi_n^0}+\lambda\ket{\psi_n^1}+\lambda^2\ket{\psi_n^2}+\cdots$$
$$E_n = E_n^0+\lambda E_n^1+\lambda^2 E_n^2+\cdots$$
Here $E_n^1$ is the \textbf{first order correction} to the $n$th energy, etcetera. Plugging in and comparing powers of $\lambda$ we obtain a collection of equations \begin{align*}
    H^0\ket{\psi_n^0} &= E_n^0\ket{\psi_n^0} \\
    H^0\ket{\psi_n^1}+H^1\ket{\psi_n^0} &= E_n^0\ket{\psi_n^1}+E_n^1\ket{\psi_n^0} \\
    H^0\ket{\psi_n^{k+1}}+H^1\ket{\psi_n^k} &= \sum_{i=0}^kE_n^i\ket{\psi_n^{k-i}}
\end{align*}

Taking the inner product of the second expression with $\ket{\psi_n^0}$ we obtain $$E_n^1 = \bra{\psi_n^0}H^1\ket{\psi_n^0}$$
In other words the first order correction to the energy is given by the expectation value of the perturbation Hamiltonian with respect to the unperterbed wavefunctions.

To find $\psi_n^1$ we note $\ket{\psi_n^1} = \sum_{m\neq n}c_m^{(n)}\psi_m^0$ for some coefficients, since we can remove and factor of $\psi_n^0$ and absorb it into that term in the original expression. As we're dealing with the nondegenerate case $$c_m^{(n)} = \frac{\bra{\psi_m^0}H^1\ket{\psi_n^0}}{E_n^0-E_m^0}$$
and so $$\ket{\psi_n^1} = \sum_{m\neq n}\frac{\bra{\psi_m^0}H^1\ket{\psi_n^0}}{E_n^0-E_m^0}\ket{\psi_m^0}$$

We can compute the second order correction to the energy by $$E_n^2 = \sum_{m\neq n}\frac{|\bra{\psi_m^0}H^1\ket{\psi_n^0}|^2}{E_n^0-E_m^0}$$


\section{Degenerate Perturbation Theory}

If the unperturbed states are degenerate we can no longer simply using the equation for $c_m^{(n)}$ given above, as we would be dividing by zero. First suppose that we have $\psi_a^0$ and $\psi_b^0$ unperturbed states which are orthonormal and have the same unperturbed energy. Typically the perturbation will lift the degeneracy since as we increase $\lambda$ from $0$ to $1$ the common unperturbed energy $E^0$ splits into two. We want to find the linear combinations of $\psi_a^0$ and $\psi_b^0$ which the splitting states after applying the perturbation reduce to once we turn the perturbation off.

Let $H = H^0+\lambda H^1$. We want to solve $H\psi = E\psi$. Taking $\psi^0 = \alpha\psi_a^0+\beta\psi_b^0$, we obtain the equation $$\alpha\bra{\psi_a^0}H^1\ket{\psi_a^0} + \beta\bra{\psi_a^0}H^1\ket{\psi_b^0} = \alpha E^1$$
Thus $\alpha$ and $\beta$ are given by looking for eigenvectors of $W_{ij} = \bra{\psi_i^0}H^1\ket{\psi_j^0}$, for $i,j = a,b$.

In general, if we have $g$-degenerate states of the same energy we have to solve an eigenvalue problem for a $g\times g$ matrix.

\begin{thm}
    Let $A$ be a hermitian operator that commutes with $H^0$ and $H^1$. If $\psi_{a_1}^0,...,\psi_{a_g}^0$ are degenerate eigenfunctions of $H^0$ which are also eigenfunctions of $A$ with distinct eigenvalues, then $\psi_{a_1}^0,...,\psi_{a_g}^0$ are the ``good" states to use in perturbation theory.
\end{thm}

The eigenvalues of our ``good states", i.e. their inner product with the inner product matrix that you solve the eigenvectors of. In other words it reduces to taking the inner product of the ``good states" with the perturbation Hamiltonian, just like in the non-degenerate perturbation theory. The idea is we want to diagonalize $H^1$ restricted to the degenerate eigenstates.



\section{The Fine Structure of Hydrogen}

Recall the Bohr Hamiltonian \begin{equation*}
    H_{Bohr} = -\frac{\hbar^2}{2m}\Delta^2 - \frac{e^2}{4\pi\epsilon_0}\frac{1}{r}
\end{equation*}
(electron kinetic energy plus Coulombic potential energy). This is not quite the full story. We already know how to correct for the motion of the nucleus; just replace $m$ by the reduced mass. More significant is the so-called \textbf{fine structure}, which is due to two mechanisms: a \textbf{relativistic correction}, and \textbf{spin-orbit coupling}. Compared to the Bohr energies, fine structure is a tiny perturbation---smaller by a factor of $\alpha^2$, where $$\alpha \equiv = \frac{e^2}{4\pi\epsilon_0\hbar c} \approx \frac{1}{137.036}$$
is the \textbf{fine structure constant}. Smaller still is the \textbf{Lamb shift}, associated with the quantization of the electric field, and smaller by yet another order of magnitude is the \textbf{hyperfine structure}, which is due to the interaction between the magnetic dipole moments of the electron and the proton.

\begin{rmk}
    Bohr energies on order of $\alpha^2mc^2$, Fine structure on order of $\alpha^4mc^2$, Lamb shift on order of $\alpha^5mc^2$, and the Hyperfine splitting $(m/m_p)\alpha^4mc^2$.
\end{rmk}

\subsection{Relativistic Correction}

The first term in the Hamiltonian represents kinetic energy, but $T = p^2/2m$ is the classical expression. The relativistic formula is $$T = \frac{mc^2}{\sqrt{1-(v/c)^2}}-mc^2$$
The first term is the total relativistic energy and the second is the rest energy.

The relativistic momentum is $$p = \frac{mv}{\sqrt{1-(v/c)^2}}$$
Then $$T = \sqrt{p^2c^2+m^2c^4} - mc^2$$
In the non-relativistic limit this reduces to the classical result. To lowest order relativistic correction the Hamiltonian gains a perturbation of $$H^1_r = -\frac{p^4}{8m^3c^2}$$
The first order correction to the energy is then $$E_r^1 = -\frac{1}{2mc^2}\left\langle(E-V)^2\right\rangle = -\frac{1}{2mc^2}\left[E^2-2E\brkt{V}+\brkt{V^2}\right]$$
For the case of hydrogen this simplifies to $$E_r^1 = -\frac{(E_n)^2}{2mc^2}\left[\frac{4n}{l+1/2}-3\right]$$
Note that we used non-degenerate perturbation theory here since the $\psi_{nlm}$ are the ``good" states for this problem. Further, from this equation we see that some of the degeneracy of the $n$th energy level has lifted. The $(2l+1)$-fold degeneracy in $m$ remains, which is due to rotational symmetry, that remains intact with this perturbation.


\subsection{Spin-Orbit Coupling}

In the electron frame of reference the proton is moving setting up a magnetic field $B$ which exerts a torque on the spinning electron. The Hamiltonian is $$H = -\mu\cdot B$$
We can express the magnetic field as $B = \frac{1}{4\pi\epsilon_0}\frac{e}{mc^2r^3}L$. 

The magnetic dipole moment is given by $\mu_e = -\frac{e}{m}S$. Then $$H = \left(\frac{e^2}{4\pi\epsilon_0}\right)\frac{1}{m^2c^2r^3}S\cdot L$$
However, we must for correct for the fact that we are in an accelerating frame by accounting for the Thomas procession: $$H_{so}^1 = \left(\frac{e^2}{8\pi\epsilon_0}\right)\frac{1}{m^2c^2r^3}S\cdot L$$
This is the \textbf{spin-orbit interaction}. $H_{so}^1$ does not any longer commute with $L$ and $S$, but it does commute with $L^2,S^2$, and $J = L+S$, the total angular momentum. Thus the eigenstates of $L^2,S^2,J^2$, and $J_z$ are now our good states. The first order correction to our energy is then $$E_{so}^1 = \frac{(E_n)^2}{mc^2}\left\{\frac{n[j(j+1)-l(l+1)-3/4]}{l(l+1/2)(l+1}\right\}$$
Adding these corrections together we obtain the fine-structure formula $$E_{fs}^1 = \frac{(E_n)^2}{2mc^2}\left(3-\frac{4n}{j+1/2}\right)$$


Combining with the Bohr formula our corrected energy levels are $$E_{nj} = -\frac{13.6\;eV}{n^2}\left[1+\frac{\alpha^2}{n^2}\left(\frac{n}{j+1/2}-\frac{3}{4}\right)\right]$$

Fine structure breaks the degeneracy in $l$, but preserves degeneracy in $j$. The ``good" quantum numbers are now $n,l,s,j$, and $m_j$.



%%%%%%%%%%%%%%%%%%%%%% Chapter 2.2
\chapter{The Variational Principle}

\section{Theory}

For any Hamiltonian $H$ of ground state energy $E_{gs}$, and any normalized wavefunction $\psi$, $$E_{gs} \leq \brkt{\psi|H|\psi} = \brkt{H}$$
This is the \textbf{variational principle}. Thus even without the solution to the time-independent Schr\"{o}dinger equation, any state gives an upper bound to the ground state energy.

Indeed, let $\psi_n$ be a complete set of solutions to $H$, with $H\psi_n = E_n\psi_n$, where $E_0 \leq E_1 \leq ...$. Then if $\psi = \sum_nc_n\psi_n$ is any state which is normalized, so $\sum_n|c_n|^2 = 1$, then $$\brkt{H} =\sum_nE_n|c_n|^2 \geq \sum_nE_0|c_n|^2 = E_0$$
where $E_0 = E_{gs}$ since the ground state energy is the smallest eigenvalue and we've ordered $E_0 \leq E_1 \leq ...$. Note that if $c_i = 0$ for $i < n$, then $\brkt{H} \geq E_n$.

How we use the variational principle in practice is take a trial wavefunction with a number of parameters, and then minimize the resulting energy so as to obtain as close an approximation to $E_{gs}$ as possible.

\section{The Ground State of Helium}

Recall the Helium atom consists of two electrons in orbit around a nucleus containing two protons and some neutrons. The Hamiltonian before corrections is $$H = -\frac{\hbar^2}{2m}\left(\nabla^2_1+\nabla^2_2\right) - \frac{e^2}{4\pi\epsilon_0}\left(\frac{2}{r_1}+\frac{2}{r_2}-\frac{1}{|r_1-r_2|}\right)$$

We want to find $E_{gs}$, which physically is the amount of energy it would take to strip off both electrons. This has no known exact solution. The issue comes from the electron-electron repulsion $$V_{ee} = \frac{e^2}{4\pi\epsilon_0}\frac{1}{|r_1-r_2|}$$
If we ignore this the exact solution to the Hamiltonian is the product of the hydrogen like wavefunctions with two protons. We use this as our trial wavefunction. This gives $\brkt{H} = -75\;eV$, while our experimental value is $\approx -79\;eV$. We use the number of protons as a parameter to simulate the shielding of the nucleus for the electrons due to their mutual repulsion. That is $$\psi_1(r_1,r_2) = \frac{Z^3}{\pi a^3}e^{-Z(r_1+r_2)/a}$$
We find that our energy is minimized with $Z = 1.69$, and this gives $\brkt{H} = -77.5\;eV$.

\section{The Hydrogen Molecule Ion}

We assume the protons are fixed in position a specified distance $R$ apart. The Hamiltonian is $$H = -\frac{\hbar^2}{2m}\nabla^2-\frac{e^2}{4\pi\epsilon_0}\left(\frac{1}{r}+\frac{1}{r'}\right)$$
Our trial wavefunction is of the form $$\psi = A\left[\frac{1}{\sqrt{\pi a^3}}e^{-r/a}+\frac{1}{\sqrt{\pi a^3}}e^{-r'/a}\right]$$
which is a linear combination of the atomic orbital wavefunctions for hydrogen. We find by normalization that $$|A|^2 = \frac{1}{2(1+I)}$$
To calculate $\brkt{H}$ we need the \textbf{direct integral} $$D = a\brkt{\psi_0(r)|1/r'|\psi_0(r)}$$
and the \textbf{exchange integral} $$X = a\brkt{\psi_0(r)|1/r|\psi_0(r')}$$
Once we add on the proton-proton repulsion energy, the total energy of the system in units of $-E_1$, and expressed as a function of $x = R/a$, has as an upper bound $$F(x) = -1+\frac{2}{x}\left\{\frac{(1-(2/3)x^2)e^{-x}+(1+x)e^{-2x}}{1+(1+x+(1/3)x^2)e^{-x}}\right\}$$
Plotting we find that bonding will occur, and in particular it will be \textbf{covalent bonding}.

\section{The Hydrogen Molecule}

Taking the protons to be at rest, the Hamiltonian is $$H = -\frac{\hbar^2}{2m}\left(\nabla^2_1+\nabla^2_2\right) + \frac{e^2}{4\pi\epsilon_0}\left(\frac{1}{r_{12}}+\frac{1}{R}-\frac{1}{r_1}-\frac{1}{r_1'}-\frac{1}{r_2}-\frac{1}{r_2'}\right)$$
For the variational wave function we associate one electron with each proton and symmetrize $$\psi_+(r_1,r_2) = A_+[\psi_0(r_1)\psi_0(r_2')+\psi_0(r_1')\psi_0(r_2)]$$
Since this spatial wave function is symmetric under interchange and electrons are fermions, the electrons must occupy the antisymmetric (singlet) spin state. We could also choose the trial wave function $$\psi_-(r_1,r_2) = A_-[\psi_0(r_1)\psi_0(r_2')-\psi_0(r_1')\psi_0(r_2)]$$
in which case the electrons would be in a symmetric (triplet) spin state. These constitute the \textbf{Heitler-London approximation}. We have that $A_{\pm} = \frac{1}{\sqrt{2(1\pm I^2)}}$, where $I$ is the overlap integral.

After computing the energies, we find that bonding only occurs if the two electrons are in a singlet configuration, i.e. if we take $\psi_+$ as our trial wavefunction. Again this bonding is covalent. The difference between the singlet and triplet energies is called the \textbf{exchange splitting} $J = \brkt{H}_+ - \brkt{H}_-$. This means a strong preference for having the electron spins anti-aligned. Note we have neglected the spin-spin interaction, which is reasonable since it is smaller by approximately 5 orders of magnitude than the exchange splitting.

Note this shows that different spin configurations can have very different energies, even when the interaction between the spins is negligible.



%%%%%%%%%%%%%%%%%%%%%% Chapter 2.3
\chapter{The WKB Approximation}


The \textbf{WKB} method is a technique for obtaining approximate solutions to the time-independent Schr\"{o}dinger equation in one dimension (it can also be applied to many other differential equations, and to the radial part of the Schr\"{o}dinger equation in three dimensions).

\begin{rmk}[Idea]
    Imagine a particle of energy $E$ moving through a region where the potential $V(x)$ is constant. If $E > V$, the wave function is of the form $$\psi(x) = Ae^{\pm ikx},\;\;\text{ with }\;\;k = \sqrt{2m(E-V)}/\hbar$$
    The plus sign indicates that the particle is traveling to the right, and the minus sign means it is going to the left (the general solution, of course, is a linear combination of the two). The wave function is oscillatory with fixed wavelength $\lambda = 2\pi/k$ and unchanging amplitude $A$. 

    Now suppose that $V(x)$ is not constant but varies slowly in comparison to $\lambda$. Then it is reasonable to suppose $\psi$ remains practically sinusoidal, except that the wavelength and the amplitude change slowly with $x$. This motivates the WKB approximation, which identifies two different levels of $x$-dependence: rapid oscillations, modulated by gradual variation in amplitude and wavelength.

    By the same token, if $E < V$, then $\psi$ is exponential: $$\psi(x) = Ae^{\pm \kappa x},\;\;\text{ with }\;\;\kappa = \sqrt{2m(V-E)}/\hbar$$
    If $V(x)$ varies slowly in comparison with $1/\kappa$, the solution remains practically exponential, except that $A$ and $\kappa$ are now slowly-varying functions of $x$. 
\end{rmk}

In the immediate vicinity of a classical \textbf{turning point}, $E \approx V$, this program is bound to fail since $\lambda$ or $1/\kappa$ go to infinity.


\section{The ``Classical" Region}

The Schr\"{o}dinger equation $$-\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2}+V(x)\psi= E\psi$$
can be rewritten as $$\frac{d^2\psi}{dx^2} = -\frac{p^2}{\hbar^2}\psi$$
where $$p(x) = \sqrt{2m(E-V(x))}$$
is the classical formula for the magnitude of the momentum of a particle with total energy $E$ and potential energy $V(x)$. In general $\psi$ is complex and can be expressed as $$\psi(x) = A(x)e^{i\phi(x)}$$
Using a prime to denote the derivative with respect to $x$, $$\frac{d^2\psi}{dx^2} = \left[A''+2iA'\phi'+iA\phi''-A(\phi')^2\right]e^{i\phi}$$
Setting this equal to $-(p^2/\hbar^2)A$ and isolating for real and imaginary parts we obtain $$A'' = A\left[(\phi')^2-\frac{p^2}{\hbar^2}\right]$$
and $$(A^2\phi')' = 0$$
The second one has solution $A = \frac{C}{\sqrt{|\phi'|}}$ for some real constant $C$. The first one cannot be solved in general. We assume that the amplitude $A$ varies slowly so $A''$ is negligible (more precisely $A''/A$ is much less than both $(\phi')^2$ and $p^2/\hbar^2$). In that case we are left with $(\phi')^2 = \frac{p^2}{\hbar^2}$, or $\frac{d\phi}{dx} = \pm\frac{p}{\hbar}$, and so $$\phi(x) = \pm\frac{1}{\hbar}\int p(x)dx$$
Then $$\psi(x) \approx \frac{C}{\sqrt{p(x)}}e^{\pm\frac{i}{\hbar}\int p(x)dx}$$
Notice that $|\psi(x)|^2 \approx \frac{|C|^2}{p(x)}$, which says that the probability of finding the particle at point $x$ is inversely proportional to its classical momentum at that point. The general approximate solution is of course a linear combination of these two.


\section{Tunneling}

So far $E > V$, so $p(x)$ is real. In the non-classical region, $E < V$, $$\psi(x) \approx \frac{C}{\sqrt{|p(x)|}}e^{\pm\frac{1}{\hbar}\int|p(x)|dx}$$

Consider the problem of scattering from a rectangular barrier with a bumpy top. To the left of the barrier $$\psi(x) = Ae^{ikx}+Be^{-ikx}$$
where $A$ is the incident amplitude, $B$ is the reflected amplitude, and $k = \sqrt{2mE}/\hbar$. To the right of the barrier $$\psi(x) = Fe^{ikx}$$
where $F$ is the transmitted amplitude, so the transmission probability is $T = |F|^2/|A|^2$. In the tunneling region the WKB approximation gives $$\psi(x) \approx \frac{C}{\sqrt{|p(x)|}}e^{\frac{1}{\hbar}\int_0^x|p(x')|dx'}+\frac{D}{\sqrt{|p(x)|}}e^{-\frac{1}{\hbar}\int_0^x|p(x')|dx'}$$
If the barrier is very high and/or very wide then the coefficient of the exponentially increasing term, $C$, must be small in fact zero. The relative amplitude of the incident and transmitted waves are determined essentially by the total decrease of the exponential over the nonclassical region $$\frac{|F|}{|A|} \sim e^{-\frac{1}{\hbar}\int_0^a|p(x')|dx'}$$
so $$T \approx e^{-2\gamma},\;\text{ with }\;\gamma = \frac{1}{\hbar}\int_0^a|p(x)|dx$$



%%%%%%%%%%%%%%%%%%%%%% Chapter 2.4
\chapter{Scattering}



%%%%%%%%%%%%%%%%%%%%%% Chapter 2.5
\chapter{Quantum Dynamics}


If we want to allow for \textbf{transitions} between energy levels we must introduce a time-dependent potential. If the time-dependent part of the Hamiltonian is small, it can be treated as a perturbation. This leads to the study of \textbf{time-dependent perturbation theory}.

\section{Two-Level Systems}

To suppose we are dealing with a two dimensional system, where we let $\psi_a$ and $\psi_b$ be the two states of the unperturbed system with energies $E_a$ and $E_b$, and which are orthonormal. Any state can then be expressed as a linear combination $$\psi(0) = c_a\psi_a + c_b\psi_b$$
In the absence of any perturbation, each component evolves with its characteristic wiggle factor $$\psi(t) = c_a\psi_ae^{-iE_at/\hbar}+c_b\psi_be^{-iE_bt/\hbar}$$
Informally, $|c_a|^2$ is the ``probability that the particle is in state $\psi_a$"---by which we really mean the probability that a measurement of the energy would yield the value $E_a$. Normalization requires $$|c_a|^2+|c_b|^2 = 1$$

\subsection{The Perturbed System}

Now we turn on a time-dependent perturbation, $H^1(t)$. Since $\psi_a$ and $\psi_b$ constitute a complete set, the wave function $\psi(t)$ can still be expressed as a linear combination of them. Now $c_a$ and $c_b$ are functions of $t$: $$\psi(t) = c_a(t) \psi_ae^{-iE_at/\hbar}+c_b(t)\psi_be^{-iE_bt/\hbar}$$
We solve for $c_a(t)$ and $c_b(t)$ by demanding $\psi(t)$ satisfy the time-dependent Schr\"{o}dinger equation. We define $$H_{ij}^1 = \brkt{\psi_i|H^1|\psi_j}$$
Note that hermiticity of $H^1$ entails $H^1_{ji} = (H^1_{ij})^*$. We conclude that $$\dot{c}_a = -\frac{i}{\hbar}\left[c_aH_{aa}^1+c_bH_{ab}^1e^{-i(E_b-E_a)t/\hbar}\right]$$
and similarly $$\dot{c}_b = -\frac{i}{\hbar}\left[c_bH_{bb}^1+c_aH_{ba}^1e^{i(E_b-E_a)t/\hbar}\right]$$

Typically $H^1_{aa} = H^1_{bb} = 0$ (i.e. the diagonal matrix elements of $H^1$ vanish). If so the equations simplify to $$\dot{c}_a = -\frac{i}{\hbar}H^1_{ab}e^{-i\omega_0t}c_b,\;\;\;\dot{c}_b = -\frac{i}{\hbar}H^1_{ba}e^{i\omega_0t}c_a$$
where $$\omega_0 = \frac{E_b-E_a}{\hbar}$$

We will assume $E_b \geq E_a$ so $\omega_0 \geq 0$.

\subsection{Time-Dependent Perturbation Theory}

If $H^1$ issmall we can solve our equations by a process of successive approximations. Suppose the particle starts out in the lower state, $c_a(0) = 1, c_b(0) = 0$.

Then to zeroth order $c_a^{(0)}(t) = 1, c_b^{(0)}(t) = 0$. To calculate the first-order approximation we insert the zeroth-order values on the right side of our previous equation: $$\frac{dc_a^{(1)}}{dt} = 0\implies c_a^{(1)}(t) = 1$$ 
and $$\frac{dc_b^{(1)}}{dt} = -\frac{i}{\hbar}H_{ba}^1e^{i\omega t}\implies c_b^{(1)} = -\frac{i}{\hbar}\int_0^tH_{ba}^1(t')e^{i\omega t'}dt'$$
We insert these expressions again back into our original equation to get the second order approximation. The error in the first-order approximation is evident in the fact that $|c_a^{(1)}(t)|^2+|c_b^{(1)}(t)|^2 \neq 1$.

We can also write $$c_b^{(1)}(t)e^{-iE_bt/\hbar} = -\frac{i}{\hbar}\int_0^te^{-iE_b(t-t')/\hbar}H_{ba}^1(t')e^{-iE_at'/\hbar}dt'$$
which suggests, reading right to left, the system remains in state $a$ from time $0$ to time $t'$, picking up the wiggle factor, makes a transition from state $a$ to state $b$ at time $t'$, and then remains in state $b$ until time $t$, picking up the other wiggle factor.

This interpretation is especially illuminating at higher orders and for multi-level systems where the expressions become complicated. 


\subsection{Sinusoidal Perturbations}

Suppose the perturbation has sinusoidal time dependence: $$H^1(r,t) = V(r)\cos(\omega t)$$
so that $$H_{ab}^1V_{ab}\cos(\omega t)$$
where $V_{ab} = \brkt{\psi_a|V|\psi_b}$. Again we assume the diagonal terms vanish. To first order we have \begin{align*}
    c_b(t) &\approx -\frac{i}{\hbar}V_{ba}\int_0^t\cos(\omega t')e^{i\omega_0t'}dt' \\
    &= -\frac{iV_{ba}}{2\hbar}\int_0^t\left[e^{i(\omega_0+\omega)t'}+e^{i(\omega_0-\omega)t'}\right]dt' \\
    &= -\frac{V_{ba}}{2\hbar}\left[\frac{e^{i(\omega_0+\omega)t}-1}{\omega_0+\omega}+\frac{e^{i(\omega_0-\omega)t}-1}{\omega_0-\omega}\right]
\end{align*}
We restrict to driving frequencies, $\omega$, that are very close to the transition frequency, $\omega_0$, so that the second term in the square brackets dominates. Specifically, we assume $$\omega_0+\omega \gg |\omega_0-\omega|$$
This gives $$c_b(t) \approx -i\frac{V_{ba}}{\hbar}\frac{\sin[(\omega_0-\omega)t/2]}{\omega_0-\omega}e^{i(\omega_0-\omega)t/2}$$
The \textbf{transition probability} is then $$P_{a\rightarrow b}(t) = |c_b(t)|^2 \approx \frac{|V_{ab}|^2}{\hbar^2}\frac{\sin^2[(\omega_0-\omega)t/2]}{(\omega_0-\omega)^2}$$
At times $t_n = 2n\pi/|\omega_0-\omega|$ the particle is certain to be back in the lower state. To maximize chances of provoking a transition it is better to turn off the perturbation after a time $\pi/|\omega_0-\omega|$, and hope to ``catch" the system in the upper state.

Note the probability of a transition is greatest when the driving frequency is close to the ``natural" frequency, $\omega_0$. 

\section{Emission and Absorption of Radiation}

\subsection{Electromagnetic Waves}

An atom in the presence of a passing light wave responds primarily to the electric component. If the wavelength is long relative to the size of the atom, we can ignore spatial variation in the field; the atom, then, is exposed to a sinusoidally oscillating elestric field $$E = E_0\cos(\omega t)\hat{k}$$
We, for now, assume the light is monochromatic and polarized along the $z$ direction. The perturbing Hamiltonian is $$H^1=-qE_0z\cos(\omega t)$$
where $q$ is the charge of the electron. Evidently $$H_{ba}^1 = -\mathcal{p}E_0\cos(\omega t),\;\;\text{ where }\;\;\mathcal{p} = q\brkt{\psi_b|z|\psi_a}$$
Typically $\psi$ is even or odd in $z$, so $z|\psi|^2$ is odd and integrates to zero. This returns our usual assumption that the diagonal entries vanish. Now we have $$V_{ba} = -\mathcal{p}E_0$$

\subsection{Absorption, Stimulated Emission, and Spontaneous Emission}

The probability of an atom starting out in the lower state $\psi_a$ transitioning to the upper state $\psi_b$ after having a polarized monochromatic beam of light shone on it is $$P_{a\rightarrow b}(t) = \left(\frac{|\mathcal{p}|E_0}{\hbar}\right)^2\frac{\sin^2[(\omega_0-\omega)t/2]}{(\omega_0-\omega)^2}$$
In this process the atom absorbs energy $E_b-E_a = \hbar\omega_0$ from the electromagnetic field. If the atom started in the upper state, the probability that it transitions down to the lower level is $$P_{b\rightarrow a}(t)=\left(\frac{|\mathcal{p}|E_0}{\hbar}\right)^2\frac{\sin^2[(\omega_0-\omega)t/2]}{(\omega_0-\omega)^2}$$
This process is called \textbf{stimulated emission}.

In this case the electromagnetic field gains energy $\hbar\omega_0$ from the atom; we say that one photon went in and two came out---the original one that caused the transition plus another one from the transition itself. This is the principle behind lasers, where we have a large number of atoms all in the higher state and triggered the stimulated emission with a single incident photon creating a chain reaction.

Another way radiation interacts with matter is \textbf{spontaneous emission}, where an atom in an excited state makes a transition downard with the release of a photon, but without any applied electromagnetic field to initiate the process. This occurs because even in the ground state at absolute zero there will be some electromagnetic radiation present. So really even spontaneous emission is a type of stimulated emission.

\subsection{Incoherent Perturbations}

We now consider the response of an atom to non-monochromatic, unpolarized, incoherent electromagnetic waves coming in from all directions. The energy density of an electromagnetic wave is $$u = \frac{\epsilon_0}{2}E_0^2$$
where $E_0$ is the amplitude of the electric field. Hence the transition probability is $$P_{b\rightarrow a}(t) = \frac{2u}{\epsilon_0\hbar^2}|\mathcal{p}|^2\frac{\sin^2[(\omega_0-\omega)t/2]}{(\omega_0-\omega)^2}$$
But this is still a monochromatic wave at a single frequency $\omega$. In the case of a whole range of frequencies we replace $u$ by $\rho(\omega)d\omega$, and the net transition probability takes the form $$P_{b\rightarrow a}(t) = \frac{2}{\epsilon_0\hbar^2}|\mathcal{p}|^2\int_0^{\infty}\rho(\omega)\left\{\frac{\sin^2[(\omega_0-\omega)t/2]}{(\omega_0-\omega)^2}\right\}d\omega$$
The term in brackets is sharply peaked about $\omega_0$, whereas $\rho(\omega)$ is ordinarily quite broad, so we can pull out $\rho(\omega)$ and $\rho(\omega_0)$, giving $$P_{b\rightarrow a}(t) \approx \frac{2}{\epsilon_0\hbar^2}|\mathcal{p}|^2\rho(\omega_0)\int_0^{\infty}\left\{\frac{\sin^2[(\omega_0-\omega)t/2]}{(\omega_0-\omega)^2}\right\}d\omega$$
Integrating we find $$P_{b\rightarrow a}(t)\approx \frac{\pi|\mathcal{p}|^2}{\epsilon_0\hbar^2}\rho(\omega_0)t$$
Note the ``flopping" phenomenon characteristic of a monochromatic perturbation gets ``washed out" when we hit the system with an incoherent spread of frequencies. In particular, the \textbf{transition rate} is now constant $$R = \frac{dP}{dt} = R_{b\rightarrow a} = \frac{\pi}{\epsilon_0\hbar^2}|\mathcal{p}|^2\rho(\omega_0)$$
To now account for radiation from all directions with all possible polarizations we need to replace $|\mathcal{p}|^2$ with the average of $|\mathbf{\mathcal{p}}\cdot\hat{n}|^2$, where $$\mathbf{\mathcal{p}} = q\brkt{\psi_b|r|\psi_a}$$
and the average is over all polarizations and all incident directions. We can equivalently integrate over all directions of $\mathbf{\mathcal{p}}$, keeping $\hat{k}$ and $\hat{n}$ fixed. This gives $$|\mathbf{\mathcal{p}}\cdot\hat{n}|^2_{ave} = \frac{1}{3}|\mathbf{\mathcal{p}}|^2$$
Thus the transition rate for stimulated emission from the state $b$ to the state $a$ under the influence of incoherent unpolarized light incident from all directions is $$R_{b\rightarrow a}=\frac{\pi}{3\epsilon_0\hbar^2}|\mathbf{\mathcal{p}}|^2\rho(\omega_0)$$
where $\mathbf{\mathcal{p}}$ is the matrix element of the electric dipole moment between the two states and $\rho(\omega_0)$ is the energy density in the fields per unit frequency, evaluated at $\omega_0 = (E_b-E_a)/\hbar$.


\section{Spontaneous Emission}


\subsection{Einstein's A and B Coefficients}

Consider a container of $N_a+N_b$ atoms, with $N_a$ of them in the lower state $\psi_a$, and $N_b$ of them in the upper state $\psi_b$. Let $A$ be the spontaneous emission rate, so that the number of particles leaving the upper state by this process per unit time is $N_bA$. The transition rate for stimulated emission, is proportional to the energy density of the electromagnetic field: $B_{ba}\rho(\omega_0)$, where $B_{ba} = \pi|\mathbf{\mathcal{p}}|^2/3\epsilon_0\hbar^2$; the number of particles leaving the upper state by this mechanism, per unit time, is $N_bB_{ba}\rho(\omega_0)$. The absorption rate is likewise, proportional to $\rho(\omega_0$, call it $B_{ab}\rho(\omega_0)$; the number of particles per unit time joining the upper level is therefore $N_aB_{ab}\rho(\omega_0)$. Then $$\frac{dN_b}{dt} = -N_bA-N_bB_{ba}\rho(\omega_0)+N_aB_{ab}\rho(\omega_0)$$
Suppose these atoms are in thermal equilibrium with the ambient field so that the number of particles in each level is constant. Then $dN_b/dt = 0$, so $$\rho(\omega_0) = \frac{A}{(N_a/N_b)B_{ab}-B_{ba}}$$
From stat mech the number of particles with energy $E$ in thermal equilibrium at temperature $T$ is proportional to the \textbf{Bolztmann factor} $\exp(-E/k_BT)$, so $$\frac{N_a}{N_b} = e^{\hbar\omega_0/k_BT}$$
so $$\rho(\omega_0) = \frac{A}{e^{\hbar\omega_0/k_BT}B_{ab}-B_{ba}}$$

Recall Planck's blackbody formula $$\rho(\omega) = \frac{\hbar}{\pi^2c^3}\frac{\omega^3}{e^{\hbar\omega/k_BT}-1}$$
Comparing these expressions $B_{ab} = B_{ba}$ and $$A = \frac{\omega_0^3\hbar}{\pi^2c^3}B_{ba}$$
Thus we recover that the transition rate for stimulated emission is the same as for absorption. Recall $$B_{ba} = \frac{\pi}{3\epsilon_0\hbar^2}|\mathbf{\mathcal{p}}|^2$$
so the spontaneous emission rate is $$A = \frac{\omega_0^3|\mathbf{\mathcal{p}}|^2}{3\pi\epsilon_0\hbar c^3}$$

\subsection{The Lifetime of an Excited State}

Suppose we have a large number of atoms in the excited state. As a result of spontaneous emission this number will decrease; specifically in a time interval $dt$ you will lose a fraction $Adt$, so $$dN_b = -AN_bdt$$
Thus $$N_b(t) = N_b(0)e^{-At}$$
so assuming no replenishing mechanism the number of excited states decrease exponentially with time constant $\tau = 1/A$. (this is the \textbf{lifetime} of the state).

If we consider more states and excited atom will typically have many different \textbf{decay modes} (that is; $\psi_b$ can decay to a large number of different lower-energy states). In that case the transition rates add, and the net lifetime is $$\tau = \frac{1}{A_1+A_2+\cdots}$$

\subsection{The Selection Rules}

Since the calculation of spontaneous emission rates has been reduced to evaluating matrix elements of the form $$\brkt{\psi_b|r|\psi_a}$$
it is important to have selection rules which can simplify these computations. Consider the system of a hydrogen atom (or more generally a centrally symmetric potential). Then we are considering $\brkt{n'l'm'|r|nlm}$. Now $r$ is a vector operator so we have from before \textbf{selection rules} $$\Delta l = \pm 1,\;\;\Delta m = 0,\pm 1$$
These conditions follow from symmetry alone. If they are not met the transition is said to be \textbf{forbidden}. Moreover, if $\Delta m = 0$ then $\brkt{n'l'm'|x|nlm} = \brkt{n'l'm'|y|nlm} = 0$, and if $\Delta m = \pm 1$ then $\brkt{n'l'm'|x|nlm} = \pm i\brkt{n'l'm'|y|nlm}$ and $\brkt{n'l'm'|z|nlm} = 0$.



\section{Fermi's Golden Rule}

We now look at transitions where $E_f$ falls in a continuum of states. If the radiation is energetic enough it can ionize the atom---the \textbf{photoelectric effect}---exciting the electron from a bound state into the continuum of scattering states. We can't consider a transition to the state $E_f$ precisely, but we can consider the probability of transitioning to a state with an energy in some range $\Delta E$ of $E_f$. That is $$P = \int_{E_f-\Delta E/2}^{E_f+\Delta E/2}\frac{|V_{in}|^2}{\hbar^2}\left\{\frac{\sin^2[(\omega_0-\omega)t/2]}{(\omega_0-\omega)^2}\right\}\rho(E_n)dE_n$$
where $\omega_0 = (E_n-E_i)/\hbar$. The quantity $\rho(E)dE$ is the number of states with energy between $E$ and $E+dE$; $\rho(E)$ is called the \textbf{density of states}.

For sufficiently large $t$ the term in brackets is peaked about $E_f = E_i+\hbar\omega$, so we can approximate $$P\approx \frac{|V_{if}|^2}{\hbar^2}\rho(E_f)\int_{-\infty}^{\infty}\frac{\sin^2[(\omega_0-\omega)t/2]}{(\omega_0-\omega)^2}dE_n = \frac{2\pi}{\hbar}\left|\frac{V_{if}}{2}\right|^2\rho(E_f)t$$
The oscillatory behaviour of $P$ has again been ``washed out" giving a constant transition rate $$R = \frac{2\pi}{\hbar}\left|\frac{V_{if}}{2}\right|^2\rho(E_f)$$
known as \textbf{Fermi's Golden Rule}.


\section{Bell's Theorem}

Einstein, Podolsky, and Rosen believed QM is incomplete in the sense that there is some other quantity $\lambda$ needed in addition to the wavefunction to characterize the state of a system fully. We call $\lambda$ the \textbf{hidden variable}. However, in 1964 J. S. Bell proved that any local hidden variable theory is incompatible with QM.

Consider an experiment where we have an entangled electron positron pair with detectors oriented along directions $a$ and $b$, respectively. Recording spins in units of $\hbar/2$, each detector registors either $+1$ or $-1$ along the direction in question. We calculate the average value of the product of the spins for a given set of detector orientations. Call this $P(a,b)$. If the detectors are parallel, $b=a$, the product is always $-1$ as one will be $+1$ and the other will be $-1$. That is $P(a,a) = -1$. Similarly, $P(a,-a) = +1$, and for general orientations QM predicts $$P(a,b) = -a\cdot b$$

Now suppose our state of the electron-positron system is characterized by hidden variable(s) $\lambda$.Suppose further that the outcome of the electron measurement is independent of the orientation of the positron detector. (This is the locality assumption). Then we have functions $A(a,\lambda)$ and $B(b,\lambda)$ for the results of the electron and proton measurements, where $A(a,\lambda) = \pm 1$ and $B(b,\lambda) = \pm 1$. Then we must have $A(a,\lambda) = -B(a,\lambda)$ regardless of $\lambda$. The average of the product of measurements is $$P(a,b) = \int \rho(\lambda)A(a,\lambda)B(b,\lambda)d\lambda$$
where $\rho(\lambda)$ is the probability density for the hidden variable. Then we can write $$P(a,b) = -\int \rho(\lambda)A(a,\lambda)A(b,\lambda)d\lambda$$
If $c$ is any other unit vector $$P(a,b) - P(a,c) = -\int\rho(\lambda)\left[A(a,\lambda)A(b,\lambda)-A(a,\lambda)A(c,\lambda)\right]d\lambda$$
Since $A(b,\lambda)^2 = 1$, $$P(a,b)-P(a,c) = -\int\rho(\lambda[1-A(b,\lambda)A(c,\lambda)]A(a,\lambda)A(b,\lambda)d\lambda$$
But $|A(a,\lambda)A(b,\lambda)| = 1$, and $\rho(\lambda)[1-A(b,\lambda)A(c,\lambda)\geq 0$, so $$|P(a,b)-P(a,c)| \leq \int\rho(\lambda)[1-A(b,\lambda)A(c,\lambda)]d\lambda$$
or more simply $$|P(a,b)-P(a,c)| \leq 1+P(b,c)$$
This is \textbf{Bell's Inequality}. It holds for any local hidden variable theory. But the QM prediction is incompatible with this inequality. Indeed, if the three vectors lie in a plane and $c$ makes a $45^{\circ}$ angle with $a$ and $b$, then $$P(a,b) = 0, P(a,c) = P(b,c) = -0.707$$
while Bell's inequality would require $0.707 \leq 1-0.707 = 0.293$, which is impossible. It has now been experimentally shown that Bell's inequality is not satisfied in general, while the predictions of QM show excellent agreement. In particular this showed that nature itself is fundamentally nonlocal.


\section{Mixed States and the Density Matrix}

\subsection{Pure States}

So far we have dealt with particles in \textbf{pure states}, $\ket{\Psi}$---a harmonic oscillator in its $n$th stationary state, for instance, or in a specific linear combination of stationary states, or a free particle in a gaussian wave packet. The expectation value of some observable $A$ is then $$\brkt{A} = \brkt{\Psi|\hat{A}|\Psi}$$
which is the average of measurements on an ensemble of identically-prepared systems, all of them in the same state $\ket{\Psi}$.

Another formulation involves the \textbf{density operator} $$\hat{\rho} = \ket{\Psi}\bra{\Psi}$$
With respect to an orthonormal basis $\{\ket{e_j}\}$ an operator is represented by a matrix; the $ij$ element of the matrix $A$ representing the operator $\hat{A}$ is $$A_{ij} = \brkt{e_i|\hat{A}|e_j}$$
In particular, the $ij$ element of the \textbf{density matrix} $\rho$ is $$\rho_{ij} = \brkt{e_i|\hat{\rho}|e_j} = \braket{e_i}{\Psi}\braket{\Psi}{e_j}$$
\begin{rmk}
    The density matrix for \textbf{pure states} has three fundamental properties: \begin{itemize}
        \item $\rho^2 = \rho$ (idempotence) 
        \item $\rho^{\text{\textdagger}} = \rho$ (hermitian) 
        \item $\text{Tr}(\rho) = \sum_i\rho_ii = 1$ (unit trace)
    \end{itemize}
\end{rmk}

The expectation value of an observable $A$ is then $$\brkt{A} = \text{Tr}(\rho A)$$
Indeed, \begin{align*}
    \text{Tr}(\rho A) &= \sum_i\braket{e_i}{\Psi}\braket{\Psi A}{e_i} \\
    &= \sum_i\braket{\Psi A}{\braket{e_i}{\Psi}e_i} \\
    &= \braket{\Psi A}{\sum_i\braket{e_i}{\Psi}e_i} \\
    &= \braket{\Psi A}{\Psi}
\end{align*}

\subsection{Mixed States}

In practice we often don't know the state of a particle. We say that the particle is in a \textbf{mixed state} if it is not represented by any single vector in the Hilbert space.

To treat such a particle we could list the probabilities, $p_k$, that it is in each possible state $\ket{\Psi_k}$. The expectation value of an observable would now be the average of measurements taken over an ensemble of systems that are not identically prepared; rather, a fraction $p_k$ of them is in each (pure) state $\ket{\Psi_k}$: $$\brkt{A} = \sum_kp_k\bra{\Psi_k}\hat{A}\ket{\Psi_k}$$
We can package this information with a density operator $$\hat{\rho} = \sum_kp_k\ket{\Psi_k}\bra{\Psi_k}$$
Like any probabilities $\sum_k p_k = 1$ and $0 \leq p_k \leq 1$.
\begin{rmk}
    The density matrix for mixed states retains most of the properties we identified for pure states:\begin{itemize}
        \item $\rho^{\text{\textdagger}} = \rho$
        \item $\text{Tr}(\rho) = 1$
        \item $\brkt{A} = \text{Tr}(\rho A)$
        \item $i\hbar\frac{d\hat{\rho}}{dt} = \left[\hat{H},\hat{\rho}\right]$, if $\frac{dp_k}{dt} = 0$ for all $k$
    \end{itemize}
    But $\rho$ is no longer idempotent. In fact, $\rho$ is idempotent, i.e. $\rho^2 = \rho$ if and only if $\rho$ represents a pure state.
\end{rmk}

\subsection{Subsystems}
 Considered an entangled state, such as the singlet spin configuration of an electron-positron pair $$\frac{1}{\sqrt{2}}(\ket{\uparrow\downarrow}-\ket{\downarrow\uparrow})$$
 Suppose we're only interested in the positron. Its state is a fifty-fifty probability of spin up or spin down. Thus even though the system is a pure state, its sybsystem, the positron, does not occupy a pure state, and has density matrix $$\rho = \begin{pmatrix} 1/2 & 0 \\ 0 & 1/2 \end{pmatrix}$$





%%%%%%%%%%%%%%%%%%%%%% Chapter 2.6
\chapter{Afterword}



%%%%%%%%%%%%%%%%%%%%%% - Appendices
\begin{appendices}


\end{appendices}


\end{document}


%%%%%% END %%%%%%%%%%%%%
