\documentclass[12pt]{article}

%------------ Packages -----------
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
%set margins to 1 inch
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}  % AMS theorem environments and proof environment -- load after amsmath
\usepackage{physics}
\usepackage{bm}
\renewcommand\qedsymbol{$\blacksquare$}
\usepackage{latexsym}
\usepackage{graphicx}    % standard LaTeX graphics tool
\usepackage{xypic}      % commutative diagrams
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{cancel}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
        colorlinks,
        citecolor=black,
        filecolor=black,
        linkcolor=black,
        urlcolor=black
}
\usepackage{multirow}
\usepackage{changepage}
\usepackage[scientific-notation=true]{siunitx}
\usepackage{pgfplots}

\pgfplotsset{every tick label/.style={inner sep=0pt,font=\scriptsize}}

%------------- Headers -----------

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Math 376 Definitions and Differentials}
\rhead{December 7 2020}
\chead{}
%\lfoot{Author's Name}
%\cfoot{}
%\rfoot{Page \thepage}


%------------- Environments -------

\newtheorem{thm}{Theorem}[section]
\newtheorem*{thm*}{Theorem}
\newtheorem{lem}[thm]{Lemma}  %%%% [thm] means number in sequence with Theorem
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{axi}[thm]{Axiom}
\newtheorem{law}[thm]{Law}
\newtheorem{pri}[thm]{Principle}
\newtheorem{for}[thm]{Formula}
\newtheorem{pro}[thm]{Property}
\newtheorem{met}[thm]{Method}
%%% definition style
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{eg}[thm]{Example}
\newtheorem{xca}[thm]{Exercise}
\newtheorem{conj}[thm]{Conjecture}
%%% remark style
\theoremstyle{remark}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{qst}{Question}
\newtheorem{obs}[thm]{Observation}
\newtheorem*{note}{Note} %%%%%%%%%% no numbering for notes
\numberwithin{equation}{section}

%------------- Macros -------------
\newcommand\C{\mathbb C}    %%%%%%%%% the set of complex numbers
\newcommand\R{\mathbb R}    %%%%%%%%% the set of real numbers
\newcommand\Z{\mathbb Z}    %%%%%%%%% the set of integers
\newcommand\N{\mathbb N}    %%%%%%%%% the natural numbers
\newcommand\Q{\mathbb Q}    %%%%%%%% the rational numbers
\newcommand\B[1]{\textbf{ #1}}
\newcommand\diriv[2]{\ensuremath{\frac{d #1}{d #2}}}
\newcommand{\parti}[2]{
        \ensuremath{\frac{\partial {#1}}{\partial {#2}}}
}
%% math operators


%------------- Begin --------------

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Math 376 Definitions and Differentials}
\author{Elijah Thompson}
\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\section{Methods}

\subsection{First Order ODEs}

\begin{met}[Integrating Factor]
        For an linear ODE of the form $y'+p(x)y=q(x)$ we can use \begin{equation}
                \mu(x) = e^{\int p(x)dx},\;\frac{d\mu}{dx} = e^{\int p(x)dx}p(x)
        \end{equation}
        And multiply the ODE by it to obtain a solution of the form:    
        \begin{equation}
                y = e^{-\int p(x) dx}\left[\int q(x)e^{\int p(x)dx}dx + C \right]
        \end{equation}
\end{met}


\vspace{1cm}


\begin{met}[Separable Equations]
        For an ODE that can be written in the form \begin{equation}
                \frac{dy}{dx}=f(x)g(y)
        \end{equation}
        we separate functions of $x$ and $y$ and integrate \begin{equation}
                \int\frac{1}{g(y)}dy = \int f(x)dx
        \end{equation}
\end{met}


\vspace{1cm}


\begin{met}[Bernoulli Equations]
        For an ODE of the form\begin{equation}
                y'(x) + p(x)y(x) = q(x)y^n
        \end{equation}
        with $n \neq 0, 1$, we substitute $z = y^{1-n}$, so $z' =(1-n)y^{-n}y$.
\end{met}


\vspace{1cm}

\begin{met}[Exact Equation]
        For an ODE of the form \begin{equation}
                M(x,y)dx + N(x,y)dy = 0
        \end{equation}
        If \begin{equation}
                \frac{\partial M}{\partial y} = \frac{\partial N}{\partial x},\;\text{or},\;M_y = N_x 
        \end{equation}
        Then we look for a integrate $M$ with respect to $x$, then differentiate with respect to $y$ to solve for our unknown potential function $F(x,y)$, giving a general solution of $F(x,y) =C$. If the condition isn't satisfied, we check if one of the following integrating factors produce an exact equation: \begin{equation}
                \frac{\mu(x)'}{\mu(x)} = \frac{M_y(x,y) - N_x(x,y)}{N(x,y)}
        \end{equation}
        \begin{equation}
                \frac{\mu(y)'}{\mu(y)} = \frac{N_x(x,y) - M_y(x,y)}{M(x,y)}
        \end{equation}
        and \begin{equation}
                \mu(x,y) = x^ny^m
        \end{equation}
\end{met}


\vspace{1cm}


\subsection{Applications}


\begin{met}[Exponential Growth]
        For a DE of the form \begin{equation}
                \frac{dN}{dt} = rN
        \end{equation}
        if $N(0) = N_0$, then $N(t) = N_0e^{rt}$.
\end{met}


\vspace{1cm}

\begin{met}[Newton's Law of Cooling]
        For a DE of the form \begin{equation}
                \diriv{T}{t} = k(T_{medium}-T(t))
        \end{equation}
        if $T(0) = T_0$, then \begin{equation}
                T(t) = T_{medium} + (T_0 - T_{medium})e^{-kt}
        \end{equation}
\end{met}


\vspace{1cm}


\begin{met}[Mixing Problems]
        For mixing problems we have \begin{equation}
                V(t) = V_0 + (r_{in} - r_{out})t
        \end{equation}
        and we obtain the DE \begin{equation}
                \diriv{Q}{t} = c_{in}r_{in} -c_{out}r_{out} = c_{in}r_{in} - \frac{Q(t)r_{in}}{V_0+(r_{in}-r_{out})t} 
        \end{equation}
        where the solution method for the linear DE depends on the values of $r_{in}$ and $r_{out}$.
\end{met}


\vspace{1cm}

\subsection{Second Order ODEs}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Solutions for the linear homogeneous DEs with constant coefficients, $y'' + by' + c = 0$.}
        \begin{tabular}{c|c}
                Roots of the & General \\ 
                Characteristic Equation & Solution \\ \hline
                $\lambda_1 \neq \lambda_2$ (real) & $C_1e^{\lambda_1 x}+C_2e^{\lambda_2 x}$ \\
                $\lambda_1 = \lambda_2 = \lambda$ (real) & $(C_1+C_2x)e^{\lambda x}$ \\
                $\lambda_{1,2} = \alpha \pm \beta i$ (complex) & $e^{\alpha x}[C_1cos(\beta x) + C_2sin(\beta x)]$ \\
        \end{tabular}
\end{table}
\egroup

\vspace{1cm}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{(Method of UC) Particular solutions of the second order linear ODE $y''(x)+b(x)y'(x)+c(x)y=f(x)$.}
        \begin{tabular}{c|c|c}
                Right Hand & $\lambda$ is a root of & Particular \\
                Side & multiplicity m & Solution \\ \hline
                $P_n(x)$ & $\lambda = 0$ & $x^mQ_n(x)$ \\
                $e^{kx}P_n(x)$ & $\lambda = k$ & $x^me^{kx}Q_n(x)$ \\
                $e^{kx}\cos(\beta x)P_n(x)$ or $e^{kx}\sin(\beta x)P_n(x)$ & $\lambda = k \pm \beta i$ & $x^me^{kx}[Q_n(x)\cos(\beta x) + R_n(x)\sin(\beta x)]$ \\
        \end{tabular}
\end{table}
\egroup


\vspace{1cm}

\begin{met}[Variation of Parameters]
        To find the particular solutions of an ODE \begin{equation}
                y''(x) + p(x)y'(x) + q(x)y(x) = f(x)
        \end{equation} 
        we look for a solution of the form \begin{equation}
                y_{part} = C_1(x)y_1(x) + C_2(x)y_2(x)
        \end{equation}
        where we assume $C_1'y_1 + C_2'y_2 = 0$. This gives the sistem \begin{equation}
                \begin{matrix} C_1'y_1 + C_2'y_2 =0 \\ C_1'y_1' + C_2'y_2' = f(x) \end{matrix}
        \end{equation}
        and solution \begin{equation}
                C_1 = -\int\frac{y_2f}{W[y_1\;y_2]}dx,\;C_2 = \int\frac{y_1f}{W[y_1\;y_2]}dx
        \end{equation}
\end{met}


\vspace{1cm}

\begin{met}[Cauchy-Euler Equation]
        For an ODE of the form $x^2y'' + bxy' + cy = 0$, we substitute $x = e^z$, or $z = \ln(x)$ ($x > 0$) \begin{equation}
                y''_z + (b-1)y'_z +cy = 0
        \end{equation}
        which gives the characteristic equation\begin{equation}
                \lambda(\lambda - 1) + b\lambda + c = \lambda^2 + (b-1)\lambda +c = 0
        \end{equation}
\end{met}

\vspace{1cm}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Solutions for the 2nd order homogeneous Cauchy-Euler DE, $x^2y''+bxy'+cy=0$.}
        \begin{tabular}{c|c|c}
                Roots of the & \multirow{2}{*}{$y(z)$} & Fundamental \\
                Characteristic Equation &  & Solutions \\ \hline
                $\lambda_1 \neq \lambda_2$ & $e^{\lambda_1 z}$, $e^{\lambda_2 z}$ & $x^{\lambda_1}$, $x^{\lambda_2}$ \\
                $\lambda_1=\lambda_2=\lambda$ & $e^{\lambda z}$, $ze^{\lambda z}$ & $x^{\lambda}$, $x^{\lambda}\ln(x)$ \\
                $\lambda_{1,2} = \alpha \pm \beta i$ & $e^{\alpha z}\cos(\beta z)$ or $e^{\alpha z}\sin(\beta z)$ & $x^{\alpha}\cos(\beta\ln(x))$ or $x^{\alpha}\sin(\beta\ln(x))$ \\
        \end{tabular}
\end{table}
\egroup

\vspace{1cm}


\subsection{Systems of First Order Linear ODEs}

\begin{met}[Homogeneous Systems]
        The standard method for solving a homogeneous system of first order DEs is to separate the system into individual equations, differentiate an equivalent number of times to the order of the system, and substitute so you obtain a single DE of order n. Then, express the other components of the solution vector as linear combinations of your first solution and its derivatives.
\end{met}



\vspace{1cm}


\begin{met}[Method of Undetermined Coefficients]
        If we are given an inhomogeneous system of the form \begin{equation}
                \B{X}'(t) = \B{A}\B{X}(t) + P_n(t)e^{\alpha t}\cos(\beta t)\B{v}\;\text{or}\;\B{X}'(t) = \B{A}\B{X}(t) + P_n(t)e^{\alpha t}\sin(\beta t)\B{v}
        \end{equation}
        we can solve it using the \B{Method of Undetermined Coefficients}, which gives a solution of the form \begin{equation}
                t^me^{\alpha t}[Q_n(t)\cos(\beta t)\B{v}_1 + R_n(t)\sin(\beta t)\B{v}_2]
        \end{equation}
\end{met}

\vspace{1cm}


\begin{met}[Variation of Parameters]
        First, let the solution of $\B{X}'(t) = \B{A}\B{X}(t)$ be $\B{X}(t) = \boldsymbol{\Phi}(t)\B{C}$. We then look for a solution of $\B{X}'(t) = \B{A}\B{X}(t) + \B{B}(t)$ in the form $\B{X}(t) = \boldsymbol{\Phi}(t)\B{C}(t)$. We then use the relationship, $\boldsymbol{\Phi}'(t)\B{C}(t) = \B{A}\boldsymbol{\Phi}(t)$, which gives us \begin{equation}
                \boldsymbol{\Phi}(t)\B{C}'(t) = \B{B}(t)
        \end{equation}
\end{met}


\subsection{Series Solutions of ODEs}

\begin{met}[Ratio Test]
        For a series $\sum\limits_{n=1}^{\infty}a_n$, if the limit \begin{equation}
                \lim\limits_{n\rightarrow \infty}\left|\frac{a_{n+1}}{a_n}\right| = r
        \end{equation}
        and $r < 1$, then the series converges (absolutely if $a_n$ have different signs). If $r > 1$ then the series diverges, and if $r=1$ then the test is inconclusive.
\end{met}


\vspace{1cm}


\begin{met}[Integral Test]
        If the terms $a_n$ of a series are positive and decreasing, then the series $\sum\limits_{n=0}^{\infty}a_n$ converges if and only if the integral \begin{equation}
                \int\limits_{0}^{\infty}f(x)dx
        \end{equation}
        converges, where $f(x)$ is a positive decreasing function coinciding with $a_n$ at integer points, $f(n) = a_n$
\end{met}

\vspace{1cm}

\begin{met}[Divergence Test]
        If $\lim\limits_{n\rightarrow \infty}a_n \neq 0$ for the terms of some series, then that series diverges.
\end{met}

\vspace{1cm}


\begin{met}[Leibniz Test for Alternating Series]
        Let $\{a_n\}$ be a positive decreasing sequence. If $\lim\limits_{n\rightarrow \infty}a_n = 0$ then the alternating series \begin{equation}
                \sum\limits_{n=0}^{\infty}(-1)^na_n
        \end{equation}
        converges.
\end{met}

\vspace{1cm}


\begin{met}[Common Taylor Series]
        Below we have four of the most common power series \begin{align}
                \frac{a_0}{1-q} &= \sum\limits_{n=0}^{\infty}a_nq^n \\
                e^x &= \sum\limits_{n=0}^{\infty}\frac{x^n}{n!} \\
                \cos(x) &= \sum\limits_{n=0}^{\infty}\frac{(-1)^n}{(2n)!}x^{2n} \\
                \sin(x) &= \sum\limits_{n=0}^{\infty}\frac{(-1)^n}{(2n+1)!}x^{2n+1}
        \end{align}
\end{met}


\vspace{1cm}

\begin{met}[Domain of Convergence]
        We have convergence until the denominator of one of the component functions vanishes in the complex domain.
\end{met}

\vspace{1cm}


\begin{met}[Ordinary Point Solution]
        If all component functions of our linear ODE are analytic in the neighbourhood of an initial point $x_0$, then there is a Taylor Series solution to our ODE in the form \begin{equation}
                y = \sum\limits_{n=0}^{\infty}a_n(x-x_0)^n
        \end{equation}
\end{met}


\vspace{1cm}


\begin{met}[Frobenius Method]
        Let $r_1$ and $r_2$ be roots of the indicial equation, $F(r) = r(r-1) + p_0r+q_0 = 0$, and $r_1 \geq r_2$. Then one solution of our ODE will always be \begin{equation}
                y_1(x) = x^{r_1}\left(1 + \sum\limits_{n=1}^{\infty}a_n(r_1)x^n\right)
        \end{equation}
        \begin{enumerate}
                \item If $r_1 - r_2 \neq n$ for any $n \in \N$, then $r_1 \neq r_2 + n$ for any $n \in \N$, so $F(r_2 + n) \neq 0$ for any $n \in \N$ and the recurrence relation \begin{equation}
                a_n(r_2) = -\frac{\sum\limits_{k=0}^{n-1}a_k(r_2)(p_{n-k}(r+k)+q_{n-k})}{F(r_2+n)}
                \end{equation}
                is well-defined, giving a second solution of \begin{equation}
                        y_2(x) = x^{r_2}\left(1 + \sum\limits_{n=1}^{\infty}a_n(r_2)x^n\right)
                \end{equation}
                \item If $r_1 = r_2$, then our second solution is \begin{equation}
                                y_2(x) = y_1(x)ln(x) + x^{r_1}\sum\limits_{n=1}^{\infty}a_n'(r_1)x^n                      
                        \end{equation}
                \item If $r_1 - r_2 = n$ for some $n \in \N$, then our second solution becomes \begin{equation}
                                y_2(x) = ay_1(x)ln(x) + x^{r_2}\left(1 + \sum\limits_{n=1}^{\infty}c_n(r_2)x^n\right)
                        \end{equation}
                        where \begin{equation}
                                a =\lim\limits_{r \rightarrow r_2}(r-r_2)a_N(r)\;\text{and}\;c_n(r_2)=\frac{d}{dr}\left[(r-r_2)a_n(r)\right]\bigg\rvert_{r=r_2}
                        \end{equation}
        \end{enumerate}
\end{met}

\vspace{1cm}

\subsection{Laplace Transform Solutions to ODEs}

\begin{met}[Properties]
        To see properties of the Laplace Transform, go to the Properties subsection of the Laplace Transform section.
\end{met}


\vspace{1cm}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Functions and their respective Laplace Transforms}
        \begin{tabular}{c|c}
                Function, $f(t)$ & Laplace Transform, $\mathcal{L}[f(t)](s)$ \\ \hline
                $1$ & $\frac{1}{s}$, $s > 0$ \\
                $e^{at}$ & $\frac{1}{s-a}$, $s > a$ \\
                $\cos(bt)$ or $\sin(bt)$ & $\frac{s}{s^2+b^2}$ or $\frac{b}{s^2+b^2}$ \\
                $t^n$ & $\frac{n!}{s^{n+1}}$ \\
                $e^{at}\cos(bt)$ or $e^{at}\sin(bt)$ & $\frac{s-a}{(s-a)^2+b^2}$ or $\frac{b}{(s-a)^2+b^2}$ \\
                $u_a(t)$ & $\frac{e^{-as}}{s}$ \\
        \end{tabular}
\end{table}
\egroup


\vspace{1cm}

\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Functions and their respective Inverse Laplace Transforms}
        \begin{tabular}{c|c|c}
                $F(s) =\mathcal{L}[f(t)](s)$ & Formula to Use & $f(t) = \mathcal{L}^{-1}[F(s)](t)$ \\ \hline
                $\frac{A_1}{s-a}$ & $\mathcal{L}[e^{at}](s)=\frac{1}{s-a}$ &  $A_1e^{at}$ \\
                $\frac{A_2}{(s-a)^2}$ & $\mathcal{L}[te^{at}](s)=\frac{1}{(s-a)^2}$ &  $A_2te^{at}$ \\
                $\frac{A_k}{(s-a)^k}$ & $\mathcal{L}[t^{k-1}e^{at}](s)=\frac{(k-1)!}{(s-a)^k}$ &  $A_k\frac{t^{k-1}e^{at}}{(k-1)!}$ \\
                $\frac{s-a}{(s-a)^2+b^2}$ & $\mathcal{L}[e^{at}\cos(bt)](s) = \frac{s-a}{(s-a)^2+b^2}$ & $e^{at}\cos(bt)$ \\
                $\frac{b}{(s-a)^2+b^2}$ & $\mathcal{L}[e^{at}\sin(bt)](s) = \frac{b}{(s-a)^2+b^2}$ & $e^{at}\sin(bt)$ \\
                $e^{-as}F(s)$ & $\mathcal{L}[u_a(t)f(t-a)](s) = e^{-as}F(s)$ & $u_a(t)f(t-a)$ \\
                $\frac{1}{s^n}$ & $\mathcal{L}[t^n] = \frac{n!}{s^{n+1}}$ & $\frac{t^{n-1}}{(n-1)!}$ \\
        \end{tabular}
\end{table}
\egroup

\vspace{1cm}

\begin{met}[Convolution]
        For an integral of the form \begin{equation}
                (f*g)(t) = \int\limits_0^tf(\tau)g(t-\tau)d\tau = (g*f)(t)
        \end{equation}
        we note that the laplace transform is given by \begin{equation}
                \mathcal{L}[f*g](s) = \mathcal{L}[f](s)\mathcal{L}[g](s)
        \end{equation}
\end{met}


\vspace{1cm}



\clearpage

\section{Basic Concepts}


\subsection{Definitions}

\begin{defn}[Differential Equation]
        A \B{differential equation} (DE) is an equation connecting an unknown function with some of its derivatives. In general, a DE is of the form \begin{equation}
                F(x,y(x),y'(x),...y^{(n)}(x)) = 0
        \end{equation}
\end{defn}

\vspace{1cm}

\begin{defn}[Order]
        The \B{order} of a DE is the order of the highest derivative it contains/
\end{defn}

\vspace{1cm}


\begin{defn}[Dependent and Independent Variables]
        The unknown function in a DE is called the \B{dependent variable}, with the variablesw on which it depends being called the \B{independent variables}.
\end{defn}


\vspace{1cm}

\begin{defn}[Solution]
        A \B{solution} of a DE is a function that satisfies the equation on some open interval $(a,b)$. The graph of a solution to a DE is called a \B{solution curve}.
\end{defn}


\vspace{1cm}



\begin{defn}[Graphs]
        A curve $C$ is said to be an \B{integral curve} of a DE if every function $y = y(x)$ whose graph is a segment of $C$ is a solution of the DE.
\end{defn}


\vspace{1cm}


\begin{defn}[IVP]
        An \B{initial value problem} (IVP) of an nth order DE requires y and its first $n-1$ derivatives to have specified values at some point $x_0$.
\end{defn}


\vspace{1cm}



\begin{defn}[Validity]
        The largest open interval $(a,b)$ that contains $x_0$, on which $y$ is defined and satisfies the DE is the \B{interval of validity} of $y$.
\end{defn}



\vspace{1cm}


\begin{defn}[Linear]
        A DE is \B{linear} if it is linear with respect to the unknown function and its derivatives.
\end{defn}


\vspace{1cm}


\begin{defn}[Homogeneous]
        A DE is \B{homogeneous} if it can be written in the form \begin{equation}
                F(y,y',...,y^{(n)}) = 0
        \end{equation}
        Otherwise, the DE is \B{inhomogeneous}.
\end{defn}


\vspace{1cm}


\begin{defn}[Parameter]
        An arbitrary constant in a DE is called a \B{parameter} and a solution of a DE with a single parameter edefines a \B{one-parameter family of functions}.
\end{defn}




\clearpage 


\section{First Order ODEs}



\subsection{Linear First Order ODEs}

\begin{defn}[Direction Field]
        For a first order DE that can be written in the form \begin{equation}
                y' = f(x,y)
        \end{equation}
        At each point in the $(x,y)$ plane we place an arrow with slope equivalent to the value of $f(x,y)$.
\end{defn}


\vspace{1cm}


\begin{defn}[Linear First Order ODE]
        A \B{linear first order ODE} is a DE of the form \begin{equation}
                y'(x) + p(x)y(x) = q(x) \label{eq:1st}
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{defn}[Homogeneous First Order ODE]
        A \B{homogeneous linear first order ODE} is a DE of the form \begin{equation}
                y'(x) + p(x)y(x) = 0
        \end{equation}
\end{defn}



\vspace{1cm}


\begin{defn}[First Order Integrating Factor Solution]
        For a linear first order homogeneous differential equation \ref{eq:1st}, we have an \B{integrating factor} of the form \begin{equation}
                \mu(x) = e^{\int p(x)dx},\;\frac{d\mu}{dx} = e^{\int p(x)dx}p(x)
        \end{equation}
        We multiply \ref{eq:1st} by this integrating factor and use the product law of differentiation to obtain the solution
        \begin{equation}
                y = e^{-\int p(x) dx}\left[\int q(x)e^{\int p(x)dx}dx + C \right]
        \end{equation}
        Note that the constant of integration for the integrating factor will cancel, so we use the simplest integration constant of 0.
\end{defn}



\vspace{1cm}


\begin{defn}[Autonomous]
        A first order ODE is called \B{autonomous} if it does not include the independent variable explicitly \begin{equation}
                y' = f(y)
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{eg}[Logistic Equation]
        The \B{logistic equation} is a first order autonomous DE of the form \begin{equation}
                \frac{dN}{dt} = rN\left(1 - \frac{N}{\kappa}\right)
        \end{equation}
        where $\kappa = $ the carrying capacity, and $r = $ the growth rate.
\end{eg}




\vspace{1cm}

\subsection{Separable Equations}

\begin{defn}[Separable Equations]
        We call a first order DE that can be written in the form \begin{equation}
                \frac{dy}{dx}=f(x)g(y)
        \end{equation}
        \B{separable}. To solve we separate functions of $x$ and $y$ and integrate \begin{equation}
                \int\frac{1}{g(y)}dy = \int f(x)dx
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{defn}[Singular Solutions]
        When dividing by $g(y)$ in solving the separable DE above, we lose the \B{singular solution} $g(y) \equiv 0$.
\end{defn}


\vspace{1cm}

\subsection{Bernoulli Equations}

\begin{defn}[Bernoulli Equations]
        Consider a first order DE of the following form: \begin{equation}
                y'(x) + p(x)y(x) = q(x)y^n
        \end{equation}
        If $n = 0$ the DE is linear, if $n = 1$ the DE is linear homogeneous and separable, and if $n \neq 0, 1$ then we have a \B{Bernoulli equation}. To solve a Bernoulli equation we divide by $y^{n}$ and substitute $z = y^{1-n}$, so $z' =(1-n)y^{-n}y
        $.
\end{defn}


\vspace{1cm}

\subsection{Exact Equations}


\begin{defn}[Exact Equation]
        If for the equation \begin{equation}
                M(x,y)dx + N(x,y)dy = 0
        \end{equation}
        we can find a function $F(x,y)$ (called a \B{potential function}) so that \begin{equation}
                \frac{\partial F}{\partial x} = M(x,y)\;\text{and}\;\frac{\partial F}{\partial y} = N(x,y)
        \end{equation}
        then we say that the DE is \B{exact}, and its general solution is \begin{equation}
                F(x,y) = C
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{defn}[Conditions]
        The equation \begin{equation}
                M(x,y)dx + N(x,y)dy = 0
        \end{equation}
        is exact if and only if \begin{equation}
                \frac{\partial M}{\partial y} = \frac{\partial N}{\partial x},\;\text{or},\;M_y = N_x 
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{thm}[Integrating Factors]
        Let \begin{equation}
                M(x,y)dx + N(x,y)dy = 0
        \end{equation}
        be not exact. Then integrating factors $\mu(x)$, $\mu(y)$, or $\mu(x,y)$ of the form \begin{equation}
                \frac{\mu(x)'}{\mu(x)} = \frac{M_y(x,y) - N_x(x,y)}{N(x,y)}
        \end{equation}
        \begin{equation}
                \frac{\mu(y)'}{\mu(y)} = \frac{N_x(x,y) - M_y(x,y)}{M(x,y)}
        \end{equation}
        and \begin{equation}
                \mu(x,y) = x^ny^m
        \end{equation}
        may make the equation exact.
\end{thm}


\vspace{1cm}


\subsection{Existence And Uniqueness}


\begin{thm}[Existence]
        Consider the IVP \begin{equation}
                y' = f(x,y),\;y(x_0) =y_0
        \end{equation}
        If there exists an open rectangle \begin{equation}
                R = \{(x,y):a < x < b, c< y < d\}
        \end{equation}
        that contains the point $(x_0,y_0)$, such that $f(x,y)$ is continuous in R, then there is an interval $(a_0,b_0)$ such that a solution of our IVP exists in $(a_0,b_0)$ containing $x_0$.
\end{thm}

\vspace{1cm}



\begin{thm}[Uniqueness]
        If in addition to the conditions of existence we have that $\frac{\partial f}{\partial y}$ is continuous on $R$, the solution is also unique.
\end{thm}


\vspace{1cm}


\begin{rmk}[Linear Equivalent]
        A solution of the IVP for a linear ODE \begin{equation}
                y' + p(x)y = q(x),\;y(x_0)=y_0
        \end{equation}
        has a unique solution on $(a,b)$ containing $x_0$ where $p(x)$ and $q(x)$ are continuous.
\end{rmk}

\vspace{1cm}



\subsection{Applications}


\begin{defn}[Exponential Growth]
        Exponential growth is characterized by a DE of the form \begin{equation}
                \frac{dN}{dt} = rN
        \end{equation}
        with exponential growth for $r > 0$ and exponential decay for $r < 0$. If $N(0) = N_0$, then $N(t) = N_0e^{rt}$.
\end{defn}


\vspace{1cm}

\begin{defn}[Newton's Law of Cooling]
        Let $T(t) = $ the temperature of an object at time t, and $T_{medium} = $ the temperature of the medium. Then \B{Newton's Law of Cooling} states that \begin{equation}
                \diriv{T}{t} = k(T_{medium}-T(t))
        \end{equation}
        This equation is separable and linear, and gives us a general solution of the form: \begin{equation}
                T(t) = T_{medium} + (T_0 - T_{medium})e^{-kt}
        \end{equation}
        where $T(0) = T_0$
\end{defn}


\vspace{1cm}


\begin{defn}[Mixing Problems]
        Let $V(t) = $ the volume of liquid at time t, $Q(t) = $ the amount of substance desolved in the solution, $c_{in} = $ the inflow of concentration, $c_{out} = $ the outflow concentration, $r_{in} = $ the inflow rate, and $r_{out} = $ is the outflow rate. If well mixed, $c_{out} = \frac{Q(t)}{V(t)}$. Let $V(0) = V_0$, so we have the equation \begin{equation}
                V(t) = V_0 + (r_{in} - r_{out})t
        \end{equation}
        and we obtain the differential equation \begin{equation}
                \diriv{Q}{t} = c_{in}r_{in} -c_{out}r_{out} = c_{in}r_{in} - \frac{Q(t)r_{in}}{V_0+(r_{in}-r_{out})t} 
        \end{equation}
\end{defn}

\vspace{1cm}








\clearpage

\section{Second Order Linear ODEs}

\subsection{Constant Coefficients}

\begin{defn}[Characteristic Equation]
        Given the following linear second order homogeneous DE with constant coefficients \begin{equation}
                y'' + by' + cy = 0 
        \end{equation}
        we have the \B{characteristic equation} \begin{equation}
                \lambda^2 + b\lambda + c = 0
        \end{equation}
        This equation results from looking for a solution in the form $y = e^{\lambda x}$.
\end{defn}



\vspace{1cm}

\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Solutions for the linear homogeneous DE with constant coefficients, $y'' + by' + c = 0$.}
        \begin{tabular}{c|c}
                Roots of the & General \\ 
                Characteristic Equation & Solution \\ \hline
                $\lambda_1 \neq \lambda_2$ (real) & $C_1e^{\lambda_1 x}+C_2e^{\lambda_2 x}$ \\
                $\lambda_1 = \lambda_2 = \lambda$ (real) & $(C_1+C_2x)e^{\lambda x}$ \\
                $\lambda_{1,2} = \alpha \pm \beta i$ (complex) & $e^{\alpha x}[C_1cos(\beta x) + C_2sin(\beta x)]$ \\
        \end{tabular}
\end{table}
\egroup



\vspace{1cm}


\subsection{General Theory}

Consider the linear second order ODE \begin{equation}
        y''(x)+b(x)y'(x)+c(x)y=f(x)
\end{equation}


\begin{thm}[Existence and Uniqueness]
         If $b(x)$, $c(x)$, and $f(x)$ are continuous in the above ODE, in an open interval $I$, containing $x_0$, then the solution of this ODE with an initial condition $y(x_0) = y_0$ exists and is unique on the open interval $I$ for any $y_0$.
\end{thm}

\vspace{1cm}


\begin{thm}[Superposition Principle]
        If $y_1$ and $y_2$ are solutions of the complementary homogeneous ODE to our above ODE, then for any $C_1$ and $C_2$, the \B{linear combination} $y = C_1y_1+C_2y_2$ is also a solution.
\end{thm}


\vspace{1cm}


\begin{defn}[Wronskian]
        The value \begin{equation}
                W[y_1\;y_2](x) = \begin{vmatrix}y_1(x) & y_2(x) \\
                y'_1(x) & y'_2(x) \end{vmatrix} = y_1(x)y'_2(x) - y_2(x)y'_1(x) 
        \end{equation}
        is called the \B{Wronskian} of the functions $y_1$ and $y_2$. Two functions $y_1$ and $y_2$ are \B{linearly independent on I} if the identity $C_1y_1(x) + C_2y_2(x) = 0$ on $I$ (for any $x \in I$) is satisfied for only $C_1 = C_2 = 0$. If $W[y_1\;y_2](x)\neq 0$ for all $x \in I$, then $y_1$ and $y_2$ are linearly independent.
\end{defn}

\vspace{1cm}


\begin{thm}[Abel's Formula]
        If $y_1$ and $y_2$ are solutions of our complementary homogeneous equation, then their Wronskian is \begin{equation}
                W(x) = W(x_0)e^{-\int\limits_{x_0}^xb(t)dt}
        \end{equation}
        where $x_0$ is the value for our initial condition.
\end{thm}




\vspace{1cm}

\begin{thm}[The General Solution (Homogeneous)]
        If $y_1$ and $y_2$ are two solutions of our homogeneous complementary equation such that $W[y_1\;y_2](x)\neq 0$ at some $x$, then the general solution of the homogeneous DE is a linear combination $y(x) = C_1y_1(x) + C_2y_2(x)$.
\end{thm}


\vspace{1cm}


\begin{thm}[The General Solution (Inhomogeneous)]
        A general solution of our ODE is a sum of the general solution of the complementary homogeneous equation and a particular solution \begin{equation}
                y_{gen} = y_{part} + y_{hom}
        \end{equation}
\end{thm}

\vspace{1cm}


\subsection{Method of Undetermined Coefficients}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Particular solutions of the second order linear ODE $y''(x)+b(x)y'(x)+c(x)y=f(x)$.}
        \begin{tabular}{c|c|c}
                Right Hand & $\lambda$ is a root of & Particular \\
                Side & multiplicity m & Solution \\ \hline
                $P_n(x)$ & $\lambda = 0$ & $x^mQ_n(x)$ \\
                $e^{kx}P_n(x)$ & $\lambda = k$ & $x^me^{kx}Q_n(x)$ \\
                $e^{kx}\cos(\beta x)P_n(x)$ or $e^{kx}\sin(\beta x)P_n(x)$ & $\lambda = k \pm \beta i$ & $x^me^{kx}[Q_n(x)\cos(\beta x) + R_n(x)\sin(\beta x)]$ \\
        \end{tabular}
\end{table}
\egroup


\vspace{1cm}

\begin{eg}[Resonance]
        We say that an ODE of the form \begin{equation}
                y'' + \omega^2y = 0
        \end{equation}
        is a \B{harmonic oscillator}. Suppose we have an inhomogeneous equation $$y''+9y=-18\cos(3t)$$ with a solution $$-3t\sin(t)$$ Observe that the external driving force is bound, but our solution is unbounded: this is an example of the phenomenon of \B{resonance}.
\end{eg}

\vspace{1cm}


\subsection{Variation of Parameters}


\begin{defn}[Variation of Parameters]
        The method of \B{Variation of Parameters} aims to find a particular solution to ODEs of the form \begin{equation}
                y''(x) + p(x)y'(x) + q(x)y(x) = f(x)
        \end{equation} 
        of the form \begin{equation}
                y_{part} = C_1(x)y_1(x) + C_2(x)y_2(x)
        \end{equation}
        where $C_1(x)$ and $C_2(x)$ are unknown function to be determined, and $y_1(x)$ and $y_2(x)$ are solutions of the complementary homogeneous DE. In our solution we assume $C_1'y_1 + C_2'y_2 = 0$. Our end result is a system of the form \begin{equation}
                \begin{matrix} C_1'y_1 + C_2'y_2 =0 \\ C_1'y_1' + C_2'y_2' = f(x) \end{matrix}
        \end{equation}
        Solving the system or using Cramer's rule gives solutions for the unknown functions of \begin{equation}
                C_1 = -\int\frac{y_2f}{W[y_1\;y_2]}dx,\;C_2 = \int\frac{y_1f}{W[y_1\;y_2]}dx
        \end{equation}
\end{defn}




%\clearpage

\section{Higher Order Linear ODEs}


\subsection{General Theory}

\begin{defn}[n-th Order Linear ODE]
        We consider an n-th order linear ODE \begin{equation}
                L[y]=y^{(n)}+a_{n-1}(x)y^{(n-1)}+\hdots +a_1(x)y'+a_0(x)y =f(x)
        \end{equation}
        with a corresponding homogeneous equation \begin{equation}
                L[y] = 0
        \end{equation}
        For both ODEs we can describe the initial conditions as \begin{equation}
                y(x_0) = y_0,\;y'(x_0) = y_0',...,y^{(n-1)}(x_0)=y_0^{(n-1)}
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{thm}[Existence and Uniqueness]
        If the functions $a_{n-1},...,a_1,a_0,f$ are continuous in an open interval $(\alpha,\beta)$ containing $x_0$, then the IVP has a unique solution on $(\alpha,\beta)$.
\end{thm}


\vspace{1cm}


\begin{thm}[Superposition Principle]
        If $y_1,y_2,...,y_k$ are solutions of the homogeneous ODE $L[y] = 0$, then for any constants $C_1,C_2,...,C_k$, a linear combination \begin{equation}
                y = C_1y_1+C_2y_2+\hdots + C_ky_k
        \end{equation}
        is also a solution. The solutions $y_1,y_2,...,y_k$ are \B{linearly independent} on $(\alpha,\beta)$ if $$C_1y_1(x)+\hdots+C_ky_k(x) = 0$$ for any $x \in (\alpha,\beta)$ only for $C_1=...=C_k=0$.
\end{thm}


\vspace{1cm}


\begin{defn}[Fundamental Set]
        If solutions $y_1,...,y_n$ are linearly independent, they form a \B{fundamental set} $\{y_1,...,y_n\}$.
\end{defn}


\vspace{1cm}


\begin{thm}[General Solution]
        If $\{y_1,...,y_n\}$ is a fundamental set (linearly independent) set of solutions of $L[y] = 0$, then the general solution is \begin{equation}
                y = C_1y_1 + \hdots + C_ny_n
        \end{equation}
        Solutions are linearly independent if the Wronskian \begin{equation}
                W[y_1,y_2,...,y_n](x) = \begin{vmatrix} y_1 & y_2 & \hdots & y_n \\ y_1' & y_2' & \hdots & y_n' \\ \hdots & \hdots & \hdots & \hdots \\ y_1^{(n-1)} & y_2^{(n-1)} & \hdots & y_n^{(n-1)} \end{vmatrix} \neq 0
        \end{equation}
        for some $x$ in the interval where we consider our homogeneous DE.
\end{thm}



\vspace{1cm}


\begin{thm}[Abel's Theorem]
        The Wronskian of our linear homogeneous DE can be represented by \begin{equation}
                W(x) = W(x_0)e^{-\int\limits_{x_0}^xa_{n-1}(x)dx}
        \end{equation}
        Thus, W is either identically equal to zero or does not vanish.
\end{thm}


\vspace{1cm}


\begin{thm}[Inhomogeneous General Solution]
        If $y_{part}$ is a solution of our inhomogeneous linear DE and $y_{hom}$ is the general solution of the homogeneous linear DE, then the general solution of our inhomogeneous linear DE is \begin{equation}
                y = y_{hom} + y_{part}
        \end{equation}
\end{thm}


\vspace{1cm}


\subsection{Constant Coefficients}


\begin{defn}[Constant Coefficients]
        In this section we consider the linear homogeneous DE with constant coefficients \begin{equation}
                y^{(n)}+a_{n-1}y^{(n-1)}+\hdots+a_1y'+a_0y = 0
        \end{equation}
        and the linear inhomogeneous DE with constant coefficients \begin{equation}
                y^{(n)}+a_{n-1}y^{(n-1)}+\hdots+a_1y'+a_0y = f(t)
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{defn}[Characteristic Equation]
        For the homogeneous linear DE with constant coefficients, we look for solutions of the form $y = e^{\lambda x}$, which gives the \B{characteristic equation} \begin{equation}
                \lambda^n+a_{n-1}\lambda^{n-1}+...+a_1\lambda+a_0 = 0
        \end{equation}
\end{defn}


\vspace{1cm}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Solutions for the linear homogeneous DE with constant coefficients, $y^{(n)}+a_{n-1}y^{(n-1)}+\hdots+a_1y'+a_0y = 0$.}
        \begin{tabular}{c|c}
                Roots of the & Fundamental \\ 
                Characteristic Equation & Solutions \\ \hline
                $\lambda_1 \neq \lambda_2\neq ...$ & $e^{\lambda_1 x},e^{\lambda_2 x},e^{\lambda_3 x},...$ \\
                $\lambda_1 = \lambda_2 =...=\lambda_m= \lambda$ & $e^{\lambda x},xe^{\lambda x},x^2e^{\lambda x},...,x^{m-1}e^{\lambda x}$ \\
                & $e^{\alpha x}cos(\beta x)$, $e^{\alpha x}C_2sin(\beta x),$ \\
                $\lambda = \alpha \pm \beta i$& $xe^{\alpha x}cos(\beta x)$, $xe^{\alpha x}C_2sin(\beta x),$ \\
                m times (2m roots) & $xe^{\alpha x}cos(\beta x)$, $xe^{\alpha x}C_2sin(\beta x),$ \\
                & $\hdots\hdots\hdots$ \\
                & $x^{m-1}e^{\alpha x}cos(\beta x)$, $x^{m-1}e^{\alpha x}C_2sin(\beta x),$
        \end{tabular}
\end{table}
\egroup


\vspace{1cm}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Solutions for the linear inhomogeneous DE with constant coefficients, $y^{(n)}+a_{n-1}y^{(n-1)}+\hdots+a_1y'+a_0y = f(x)$.}
        \begin{adjustwidth}{-0.7cm}{}
                \begin{tabular}{c|c|c}
                        Right Hand & $\lambda$ is a root of & Particular \\
                        Side & multiplicity m & Solution \\ \hline
                        $P_n(x)e^{kx}$ & $\lambda = k$ & $x^mQ_n(x)e^{kx}$ \\
                        $(k=0 \implies P_n(x))$ & ($m = 0$ if not a root) & \\
                        $e^{kx}\cos(\beta x)P_n(x)$ or $e^{kx}\sin(\beta x)P_n(x)$ & $\lambda = k \pm \beta i$ & $x^me^{kx}[Q_n(x)\cos(\beta x) + R_n(x)\sin(\beta x)]$ \\
                \end{tabular}
        \end{adjustwidth}
\end{table}
\egroup

\vspace{1cm}

\subsection{Variation of Parameters}

\begin{defn}[Variation of Parameters]
        The method of \B{Variation of Parameters} aims to find a particular solution to the linear ODEs of the form \begin{equation}
                y^{(n)}+a_{n-1}y^{(n-1)}+\hdots+a_1y'+a_0y = 0
        \end{equation} 
        of the form \begin{equation}
                y_{part} = C_1(x)y_1(x) + C_2(x)y_2(x)+...+C_n(x)y_n(x)
        \end{equation}
        where $C_1(x), C_2(x), ..., C_n(x)$ are unknown function to be determined, and $\{y_1(x), y_2(x),...,y_n(x)\}$ is a fundamental set of solutions for the complementary homogeneous DE. Our end result is a system of the form \begin{equation}
                \begin{matrix}
                        C_1'y_1& +& C_2'y_2&+&\hdots&+&C_n'y_n& =& 0 \\
                        C_1'y_1'& +& C_2'y_2'&+&\hdots&+&C_n'y_n' &=& 0 \\
                        \hdots&&\hdots&&\hdots&&\hdots&&\hdots \\
                        C_1'y_1^{(n-1)}& + &C_2'y_2^{(n-1)}&+&\hdots&+&C_n'y_n^{(n-1)} &=& f(x)
                \end{matrix}
        \end{equation}
\end{defn}

\vspace{1cm}

\subsection{Cauchy-Euler Equations}


\begin{defn}[Cauchy-Euler Equation]
        We say that a linear homogeneous ODE is a homogeneous \B{Cauchy-Euler equation} if it can be written in the form \begin{equation}
                x^ny^{(n)}+a_{n-1}x^{n-1}y^{(n-1)}+...+a_1xy'+a_0y=0
        \end{equation}
        where $a_{n-1},...,a_1$ are constants.
\end{defn}


\vspace{1cm}


\begin{defn}[Method of Solution]
        For a general Cauchy-Euler equation we substitute $x = e^z$ or $z = \ln(x)$ ($x > 0$) and then obtain an associated characteristic equation of \begin{equation}
                \lambda(\lambda - 1)(\lambda - 2)...(\lambda - (n-1)) + a_{n_1}\lambda(\lambda - 1)(\lambda - 2)...(\lambda - (n-2)) + ... + a_1\lambda + a_0 = 0
        \end{equation}
        We then use the methods described previously for the roots of this characteristic equation for $z$ to obtain a solution in terms of $z$, then substitute back in $x$. The inhomogeneous case is handled with either Variation of Parameters or the Method of Undetermined Coefficients.
\end{defn}


\vspace{1cm}


\begin{eg}[Second Degree Case]
        We assume $x = e^{z}$, or $z = \ln(x)$ $(x > 0)$. We use the chain rule to find that $$\diriv{y}{x} = \diriv{y}{z}\diriv{z}{x} = y'_z\frac{1}{x}$$
        and $$\frac{d^2y}{dx^2} = \frac{d}{dx}\left(y'_z\frac{1}{x}\right) = y''_z\frac{1}{x^2} - y'_z\frac{1}{x^2}$$ After substituting we get the equation \begin{equation}
                y''_z + (b-1)y'_z +cy = 0
        \end{equation}
        which gives the characteristic equation\begin{equation}
                \lambda(\lambda - 1) + b\lambda + c = \lambda^2 + (b-1)\lambda +c = 0
        \end{equation}
\end{eg}

\vspace{1cm}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Solutions for the 2nd order homogeneous Cauchy-Euler DE, $x^2y''+bxy'+cy=0$.}
        \begin{tabular}{c|c|c}
                Roots of the & \multirow{2}{*}{$y(z)$} & Fundamental \\
                Characteristic Equation &  & Solutions \\ \hline
                $\lambda_1 \neq \lambda_2$ & $e^{\lambda_1 z}$, $e^{\lambda_2 z}$ & $x^{\lambda_1}$, $x^{\lambda_2}$ \\
                $\lambda_1=\lambda_2=\lambda$ & $e^{\lambda z}$, $ze^{\lambda z}$ & $x^{\lambda}$, $x^{\lambda}\ln(x)$ \\
                $\lambda_{1,2} = \alpha \pm \beta i$ & $e^{\alpha z}\cos(\beta z)$ or $e^{\alpha z}\sin(\beta z)$ & $x^{\alpha}\cos(\beta\ln(x))$ or $x^{\alpha}\sin(\beta\ln(x))$ \\
        \end{tabular}
\end{table}
\egroup

\vspace{1cm}

\begin{rmk}[Inhomogeneous Case]
        For inhomogeneous Cauchy-Euler equations we use either Variation of Parameters, or Undetermined Coefficients applied to $y(z)$.
\end{rmk}



\clearpage

\section{Systems of First Order Linear ODEs}


\subsection{Definitions and Notation}

\begin{defn}[System of n Unknown Functions]
        Suppose we have n unknown functions, $x_1(t),x_2(t),...,x_n(t)$, satisfying \begin{equation}
                \begin{matrix}
                        x'_1(t)=&a_{11}(t)x_1(t)+&a_{12}(t)x_2(t)+&\hdots+&a_{1n}(t)x_n(t)+&b_1(t) \\
                        x'_2(t)=&a_{21}(t)x_1(t)+&a_{22}(t)x_2(t)+&\hdots+&a_{2n}(t)x_n(t)+&b_2(t) \\
                        \hdots&\hdots&\hdots&\hdots&\hdots&\hdots \\
                        x'_n(t)=&a_{n1}(t)x_1(t)+&a_{n2}(t)x_2(t)+&\hdots+&a_{nn}(t)x_n(t)+&b_n(t) 
                \end{matrix}
        \end{equation}
        where here $a_{ij}$ and $b_{i}$ are given functions.
\end{defn}

\vspace{1cm}

\begin{defn}[Solution]
        A solution of such a system is a \B{vector function} \begin{equation}
                \B{X}(t) = \begin{bmatrix}x_1(t) \\ x_2(t) \\ \vdots \\ x_n(t) \end{bmatrix}
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{rmk}[Notation]
        We introduce the column vector \begin{equation}
                \B{B}(t) = \begin{bmatrix}b_1(t) \\ b_2(t) \\ \vdots \\ b_n(t) \end{bmatrix}
        \end{equation}
        and the matrix of coefficients \begin{equation}
                \B{A}(t) = \begin{bmatrix}a_{11}(t) & a_{12}(t) & \hdots & a_{1n}(t) \\ a_{21}(t) & a_{22}(t) & \hdots & a_{2n}(t)\\ \vdots & \vdots & \vdots & \vdots \\ a_{n1}(t) & a_{n2}(t) & \hdots & a_{nn}(t) \end{bmatrix}
        \end{equation}
        which lets us right our system in matrix form \begin{equation}
                \B{X}'(t) = \B{A}(t)\B{X}(t)+\B{B}(t)
        \end{equation}
\end{rmk}


\vspace{1cm}


\begin{obs}[n-th Order Linear ODEs]
        Given an n-th order linear ODE of the form $$y^{(n)}+a_{n-1}y^{(n-1)}+...+a_1y'+a_0y=b(t)$$ we can represent it as an n-dimensional system given by \begin{equation}
                \begin{bmatrix}x_1'(t) \\ x_2'(t) \\ \vdots \\ x_n'(t) \end{bmatrix} = \begin{bmatrix} 0 & 1 & 0 & \hdots & 0 \\ 0 & 0 & 1 & \hdots & 0 \\ \vdots & \vdots & \vdots & \vdots \\ -a_0 & -a_1 & -a_2 & \hdots & -a_{n-1} \end{bmatrix}\begin{bmatrix}x_1(t) \\ x_2(t) \\ \vdots \\ x_n(t) \end{bmatrix}+\begin{bmatrix}0 \\ 0 \\ \vdots \\ b(t) \end{bmatrix}
        \end{equation}
\end{obs}


\vspace{1cm}

\subsection{General Theory}


We consider a system of n equations of the form \begin{equation}
        \B{X}'(t) = \B{A}(t)\B{X}(t)+\B{B}(t) 
\end{equation}
and the associated homogeneous system \begin{equation}
        \B{X}'(t) = \B{A}(t)\B{X}(t) 
\end{equation}

\vspace{1cm}


\begin{thm}[Existence And Uniqueness]
        If $I$ is an open interval containing $t_0$ and all the component functions $a_{ij}(t)$, $b_i(t)$ are continuous on $I$, then the IVP $\B{X}(t_0) = \B{X}_0$ has a unique solution on $I$.
\end{thm}


\vspace{1cm}


\begin{thm}[Superposition Principle]
        If $\B{X}_1(t), \B{X}_2(t),...,\B{X}_k(t)$ are solutions of the homogeneous system, then for any constants $C_1,C_2,...,C_k$, the linear combination \begin{equation}
                C_1\B{X}_1(t)+ C_2\B{X}_2(t)+...+C_k\B{X}_k(t)
        \end{equation}
        is also a solution of the homogeneous system.
\end{thm}

\begin{note}[In General]
        We need n constants to satisfy \emph{any} initial conditions.
\end{note}

\vspace{1cm}

\begin{thm}[General Solution]
        If $\B{X}_1,\B{X}_2,...,\B{X}_n$ are linearly independent solutions of our homogeneous system, then the general solution is \begin{equation}
                \B{X}(t) = C_1\B{X}_1(t)+C_2\B{X}_2(t)+...+C_n\B{X}_n(t)
        \end{equation}
        In particular, the solutions $\B{X}_1,\B{X}_2,...,\B{X}_n$ are linearly independent if the \B{fundamental matrix} with $\B{X}_1,\B{X}_2,...,\B{X}_n$ as columns, \begin{equation}
                \boldsymbol{\Phi}(t) = \left[\B{X}_1\;\B{X}_2\;...\;\B{X}_n\right]
        \end{equation}
        has a non-zero determinant at a point, t, in the interval where we consider the system.
\end{thm}


\vspace{1cm}

\begin{thm}[Abel's Theorem]
        For any solutions $\B{X}_1,\B{X}_2,...,\B{X}_n$ of our homogeneous system, and $\boldsymbol{\Phi}(t) = \left[\B{X}_1\;\B{X}_2\;...\;\B{X}_n\right]$, we have that \begin{equation}
                \det\boldsymbol{\Phi}(t) = \boldsymbol{\Phi}(t_0)e^{\int_{t_0}^t\tr\B{A}(t)dt}
        \end{equation}
\end{thm}


\vspace{1cm}



\begin{thm}[Inhomogeneous General Solution]
        If $\B{X}_{part}$ is a particular solution of our inhomogeneous system and $\{\B{X}_1,...,\B{X}_n\}$ is a fundamental set of our homogeneous system, then the general solution for our inhomogeneous system is \begin{equation}
                \B{X}(t) = C_1\B{X}_1(t) + \hdots +  C_n\B{X}_n(t) + \B{X}_{part}
        \end{equation}
\end{thm}

\vspace{1cm}

\subsection{Methods of Solution}


\begin{defn}[Constant Matrix]
        If $\B{A}(t)$ is a constant matrix, then each $\B{X}_1,\B{X}_2,...,\B{X}_n$ has the form \begin{equation}
                t^me^{\alpha t}\cos(\beta t)\B{v}\;\text{or}\;t^me^{\alpha t}\sin(\beta t)\B{v}
        \end{equation}
        where $\B{v}$ is a column vector. If $\boldsymbol{\Phi}(t)$ is a \B{fundamental matrix}, then our solution will be of the form \begin{equation}
                \B{X}(t) = \boldsymbol{\Phi}(t)\B{C}
        \end{equation}
        where \begin{equation}
                \B{C} = \begin{bmatrix} C_1 \\ C_2 \\ \vdots \\ C_n \end{bmatrix}
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{defn}[Method of Undetermined Coefficients]
        If we are given an inhomogeneous system of the form \begin{equation}
                \B{X}'(t) = \B{A}\B{X}(t) + P_n(t)e^{\alpha t}\cos(\beta t)\B{v}\;\text{or}\;\B{X}'(t) = \B{A}\B{X}(t) + P_n(t)e^{\alpha t}\sin(\beta t)\B{v}
        \end{equation}
        we can solve it using the \B{Method of Undetermined Coefficients}, which gives a solution of the form \begin{equation}
                t^me^{\alpha t}[Q_n(t)\cos(\beta t)\B{v}_1 + R_n(t)\sin(\beta t)\B{v}_2]
        \end{equation}
\end{defn}

\vspace{1cm}


\begin{defn}[Variation of Parameters]
        First, let the solution of $\B{X}'(t) = \B{A}\B{X}(t)$ be $\B{X}(t) = \boldsymbol{\Phi}(t)\B{C}$. We then look for a solution of $\B{X}'(t) = \B{A}\B{X}(t) + \B{B}(t)$ in the form $\B{X}(t) = \boldsymbol{\Phi}(t)\B{C}(t)$. We then have that $$\B{X}'(t) = \boldsymbol{\Phi}'(t)\B{C}(t) + \boldsymbol{\Phi}(t)\B{C}'(t)\;\text{and}\;\B{X}'(t) = \B{A}\boldsymbol{\Phi}(t)\B{C}(t)+\B{B}(t)$$ but $\boldsymbol{\Phi}'(t) = \B{A}\boldsymbol{\Phi}(t)$, so $\boldsymbol{\Phi}'(t)\B{C}(t) = \B{A}\boldsymbol{\Phi}(t)$, which gives us \begin{equation}
                \boldsymbol{\Phi}(t)\B{C}'(t) = \B{B}(t)
        \end{equation}
\end{defn}







\clearpage

\section{Series Solutions of ODEs}


\subsection{Definitions and Convergence}

\begin{defn}[Series]
        The infinite sum \begin{equation}
                \sum\limits_{n=1}^{\infty}a_n
        \end{equation}
        where $a_n$ are numbers, is called a \B{series}.
\end{defn}

\vspace{1cm}


\begin{defn}[Converges]
        A series \B{converges} if the limit of partial sums \begin{equation}
                \lim\limits_{N\rightarrow \infty}S_N = \lim\limits_{N\rightarrow \infty}\sum\limits_{n=1}^Na_n
        \end{equation}
        exists and is finite.
\end{defn}


\vspace{1cm}


\begin{defn}[Absolute Convergence]
        If $a_n$ can be of different signs and $\sum\limits_{n=1}^{\infty}|a_n|$ converges, then we say that $\sum\limits_{n=1}^{\infty}a_n$ is \B{absolutely convergent}.
\end{defn}


\vspace{1cm}

\begin{defn}[Geometric Series]
        For the \B{geometric series} $\sum\limits_{n=0}^{\infty}a_nq^n$, if $|q| \geq 1$ then the series diverges. If $|q| < 1$ then  the series converges and we have that \begin{equation}
                \sum\limits_{n=0}^{\infty}a_nq^n = \frac{a_0}{1-q}
        \end{equation}
\end{defn}


\vspace{1cm}



\begin{thm}[Ratio Test]
        The \B{ratio test} states that for a series $\sum\limits_{n=1}^{\infty}a_n$, if the limit \begin{equation}
                \lim\limits_{n\rightarrow \infty}\left|\frac{a_{n+1}}{a_n}\right| = r
        \end{equation}
        and $r < 1$, then the series converges (absolutely if $a_n$ have different signs). If $r > 1$ then the series diverges, and if $r=1$ then the test is inconclusive.
\end{thm}


\vspace{1cm}


\begin{thm}[Integral Test]
        The \B{integral test} states that if the terms $a_n$ of a series are positive and decreasing, then the series $\sum\limits_{n=0}^{\infty}a_n$ converges if and only if the integral \begin{equation}
                \int\limits_{0}^{\infty}f(x)dx
        \end{equation}
        converges, where $f(x)$ is a positive decreasing function coinciding with $a_n$ at integer points, $f(n) = a_n$.
\end{thm}


\begin{figure}[H]
        \centering
        \begin{tikzpicture}
                \begin{axis}[
                                axis lines=middle,
                                samples=200,
                                xtick={0,...,6},
                                ytick={-3,...,9}
                                ]
                        \addplot[blue,domain=0.15:6] {1/x};
                        \addplot+[ybar interval,mark=no] plot coordinates { (0.2,5) (0.5, 2) (1, 1) (2, 0.5) (3, 1/3) (4, 1/4) (5, 1/5) };
                \end{axis}
        \end{tikzpicture}
\end{figure}


\vspace{1cm}

\begin{thm}[Divergence Test]
        If $\lim\limits_{n\rightarrow \infty}a_n \neq 0$ for the terms of some series, then that series diverges.
\end{thm}

\vspace{1cm}


\begin{thm}[Leibniz Test for Alternating Series]
        Let $\{a_n\}$ be a positive decreasing sequence. If $\lim\limits_{n\rightarrow \infty}a_n = 0$ then the alternating series \begin{equation}
                \sum\limits_{n=0}^{\infty}(-1)^na_n
        \end{equation}
        converges.
\end{thm}

\vspace{1cm}


\subsection{Power Series}


\begin{defn}[Power Series]
        A series of the form \begin{equation}
                \sum\limits_{n=0}^{\infty}a_n (x-c)^n
        \end{equation}
        is called a \B{power series} centered at $c$. By the ration test, the series converges if \begin{equation}
                |x-c| < \lim\limits_{n\rightarrow \infty}\left|\frac{a_n}{a_{n+1}}\right|
        \end{equation}
        The value $R = \lim\limits_{n\rightarrow \infty}\left|\frac{a_n}{a_{n+1}}\right|$ is called the \B{radius of convergence} for the power series.
\end{defn}


\vspace{1cm}

\begin{defn}[Interval of Convergence]
        Given a power series $\sum\limits_{n=0}^{\infty}a_n (x-c)^n$, its \B{interval of convergence} is the interval, centered at $c$, in which the series converges.
\end{defn}



\vspace{1cm}

\begin{rmk}[Radius and Convergence]
        If the radius of convergence for a power series centered at $c$ is finite and $R \neq 0$, then the series converges absolutely for $x$ in the interval $(c - R, c + R)$. If $R = 0$, then the power series only converges for $x = c$. Finally, if $R = \infty$, then the power series converges everywhere.
\end{rmk}


\vspace{1cm}

\begin{obs}[Facts about Power Series]
        \begin{enumerate}
                \item Inside the interval of convergence, a power series converges absolutely.
                \item We can add, subtract, and multiply power series in their common interval of convergence.
                \item We can differentiate and integrate power series inside their interval of convergence.
        \end{enumerate}
\end{obs}


\vspace{1cm}

\subsection{Taylor Series}


\begin{defn}[Taylor Series]
        Given a function $f(x)$, its \B{Taylor Series} centered at $c$ is given by \begin{equation}
                f(x) \sim \sum\limits_{n=0}^{\infty}\frac{f^{(n)}(c)}{n!}(x-c)^n
        \end{equation}
\end{defn}

\vspace{1cm}

\begin{thm}[Power Series]
        If a power series converges to a function, then the series is the Taylor Series for that function.
\end{thm}


\vspace{1cm}


\begin{thm}[Common Taylor Series]
        Below we have four of the most common power series \begin{align}
                \frac{a_0}{1-q} &= \sum\limits_{n=0}^{\infty}a_nq^n \\
                e^x &= \sum\limits_{n=0}^{\infty}\frac{x^n}{n!} \\
                \cos(x) &= \sum\limits_{n=0}^{\infty}\frac{(-1)^n}{(2n)!}x^{2n} \\
                \sin(x) &= \sum\limits_{n=0}^{\infty}\frac{(-1)^n}{(2n+1)!}x^{2n+1}
        \end{align}
\end{thm}


\vspace{1cm}


\subsection{Definitions for Convergence of Solutions}

\begin{defn}[Analytic]
        A function is called \B{analytic} in a domain $D$ if it is infinitely differentiable and its power series converges to it within $D$.
\end{defn}


\vspace{1cm}


\begin{thm}[Analytic Functions]
        \begin{enumerate}
                \item All polynomials are analytic on the entirety of their domain.
                \item The exponential function is analytic on the entirety of its domain.
                \item Trigonometric functions are analytic on their domains.
                \item The logarithm is analytic on any open set in its domain.
                \item All power functions are analytic in open sets in their domain.
        \end{enumerate}
\end{thm}


\vspace{1cm}


\begin{thm}[Convergence of Solutions to Linear ODEs]
        Consider the linear ODE given by \begin{equation}
                y^{(n)}+a_{n-1}(x)y^{(n-1)}+...+a_1(x)y'+a_0(x)y=f(x)
        \end{equation}
        If all component functions, $a_{n-1}(x),...,a_0(x),f(x)$, are analytic on some domain $D$ containing an initial point, then in this domain the series solution of the ODE should converge to a solution of the IVP.
\end{thm}



\vspace{1cm}


\begin{thm}[Domain of Convergence]
        We have convergence until the denominator of one of the component functions vanishes in the complex domain.
\end{thm}
\begin{eg}
        For example, if one of the component functions is $p(x) = \frac{1}{x^2+25}$, then since the denominator is zero for $x = \pm 5i$, we have that the radius of convergence will be $5$.
\end{eg}



\vspace{1cm}


\begin{defn}[Ordinary Point]
        If all component functions of our linear ODE are analytic in the neighbourhood of an initial point $x_0$, then $x_0$ is an \B{ordinary point}, and there is a Taylor Series solution to our ODE in the form \begin{equation}
                y = \sum\limits_{n=0}^{\infty}a_n(x-x_0)^n
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{defn}[Singular Points]
        If the component functions for our linear ODE are not analytic in any neighbourhood of $x_0$, then $x_0$ is \B{singular}. Moreover, if all functions $x^na_0(x), x^{n-1}a_1(x),...,xa_{n-1}(x)$ are analytic in some neighbourhood of $x_0$ then $x_0$ is \B{regular singular}. Otherwise, if at least one of $x^na_0(x), x^{n-1}a_1(x),...,xa_{n-1}(x)$ is not analytic in any neighbourhood of $x_0$, then $x_0$ is called \B{irregular singular}.
\end{defn}




\vspace{1cm}


\subsection{Frobenius Method}

\begin{defn}[Cauchy-Euler Extension]
        We now consider second order linear ODEs of the form \begin{equation}
                x^2y''+xp(x)y'+q(x)y = 0
        \end{equation}
        where $x=0$ is a regular singular point and \begin{equation}
                p(x) = \sum\limits_{n=0}^{\infty}p_nx^n\;\text{and}\;q(x) = \sum\limits_{n=0}^{\infty}q_nx^n
        \end{equation}
        are analytic at $x = 0$, we seek solutions to the ODE of the form \begin{equation}
                y = x^r\sum\limits_{n=0}^{\infty}a_nx^n
        \end{equation}
\end{defn}


\vspace{1cm}

\begin{defn}[Indicial Equation]
        We solve for $r$ in the above solution by solving for the roots of the \B{indicial equation} \begin{equation}
                F(r) = r(r-1)+p_0r+q_0 = 0
        \end{equation}
        The solutions to this equation are called the \B{exponents of singularity}.
\end{defn}


\vspace{1cm}



\begin{thm}[Series Form of the ODE]
        When we substitute in our series solution we obtain an ODE of the form \begin{equation}
                x^r\left[a_0F(r)+\sum\limits_{n=1}^{\infty}\left(a_nF(r+n)+\sum\limits_{k=0}^{n-1}a_k(p_{n-k}(r+k)+q_{n-k})\right)x^n\right] = 0
        \end{equation}
        This gives us the following recurrence relation for $n \geq 1$: \begin{equation}
                a_n(r) = -\frac{\sum\limits_{k=0}^{n-1}a_k(r)(p_{n-k}(r+k)+q_{n-k})}{F(r+n)}
        \end{equation}
\end{thm}


\vspace{1cm}

\begin{thm}[Cases for the Roots of the Indicial Equation]
        Let $r_1$ and $r_2$ be roots of the indicial equation and $r_1 \geq r_2$. Then one solution of our ODE will always be \begin{equation}
                y_1(x) = x^{r_1}\left(1 + \sum\limits_{n=1}^{\infty}a_n(r_1)x^n\right)
        \end{equation}
        \begin{enumerate}
                \item If $r_1 - r_2 \neq n$ for any $n \in \N$, then $r_1 \neq r_2 + n$ for any $n \in \N$, so $F(r_2 + n) \neq 0$ for any $n \in \N$ and the recurrence relation \begin{equation}
                a_n(r_2) = -\frac{\sum\limits_{k=0}^{n-1}a_k(r_2)(p_{n-k}(r+k)+q_{n-k})}{F(r_2+n)}
                \end{equation}
                is well-defined, giving a second solution of \begin{equation}
                        y_2(x) = x^{r_2}\left(1 + \sum\limits_{n=1}^{\infty}a_n(r_2)x^n\right)
                \end{equation}
                \item If $r_1 = r_2$, then our second solution is \begin{equation}
                                y_2(x) = y_1(x)ln(x) + x^{r_1}\sum\limits_{n=1}^{\infty}a_n'(r_1)x^n                      
                        \end{equation}
                \item If $r_1 - r_2 = n$ for some $n \in \N$, then our second solution becomes \begin{equation}
                                y_2(x) = ay_1(x)ln(x) + x^{r_2}\left(1 + \sum\limits_{n=1}^{\infty}c_n(r_2)x^n\right)
                        \end{equation}
                        where \begin{equation}
                                a =\lim\limits_{r \rightarrow r_2}(r-r_2)a_N(r)\;\text{and}\;c_n(r_2)=\frac{d}{dr}\left[(r-r_2)a_n(r)\right]\bigg\rvert_{r=r_2}
                        \end{equation}
        \end{enumerate}
\end{thm}




\clearpage

\section{Laplace Transform}


\subsection{Base Definitions}

\begin{defn}[Operator]
        An \B{operator} is a function that maps functions from a certain class into another function.
\end{defn}
\begin{eg}
        For example, two common operators (which are in fact linear operators) are Differentiation:$f\rightarrow f'$, and Integration:$f\rightarrow \int\limits_{0}^xf(t)dt$.
\end{eg}


\vspace{1cm}


\begin{defn}[Piece-wise Continuous]
        A function $f$ is \B{piecewise continuous} on an open interval if the interval can be broken into a finite number of subintervals on which the function is continuous on each open subinterval and has a finite limit at the endpoints of each subinterval.
\end{defn}


\vspace{1cm}

\begin{defn}[Laplace Transform]
        Let $f$ be a piecewise continuous function such that $|f(t)| \leq Me^{bt}$ for some $b,M > 0$. Then, \begin{equation}
                F(s) = \mathcal{L}[f(t)](s) = \int\limits_{0}^{\infty}e^{-st}f(t)dt
        \end{equation} 
        is defined (usually $s > 0$), and is called the \B{Laplace Transform} of $f$.
\end{defn}


\vspace{1cm}


\begin{defn}[Unit Step Function]
        We define the \B{unit step function} as \begin{equation}
                u_a(t) = \left\{\begin{array}{cc} 0, & 0\leq t < a \\ 1, & t \geq a \end{array}\right. (a \geq 0)
        \end{equation}
        We also notate the unit step function as $step_a(t)$ or $h(t-a)$ (to denote the \B{Heaviside function})
\end{defn}

\vspace{1cm}

\subsection{Properties}

Recall that $\mathcal{L}[f(t)](s) = F(s)$.

\begin{pro}[Linearity]
        The Laplace Transform is a linear operator. That is for any functions $f(t)$ and $g(t)$ for which the laplace transform exists, and any constants $a,b \in \R$, we have that \begin{equation}
                \mathcal{L}[af(t)+bg(t)](s)=a\mathcal{L}[f(t)](s)+b\mathcal{L}[g(t)](s)
        \end{equation}
\end{pro}


\vspace{1cm}


\begin{pro}[Defined]
        If $f$ is a piecewise continuous function and $|f(t)| \leq Me^{bt}$ for some $b,M > 0$, then $\mathcal{L}[f(t)](s)$ is defined for $s > b$.
\end{pro}


\vspace{1cm}


\begin{pro}[First Differentiation Formula]
        Given a function $f$ that is n-th differentiable, and has a defined laplace transform, we have that \begin{equation}
                \mathcal{L}[f'(t)](s) = s\mathcal{L}[f(t)](s) - f(0) 
        \end{equation}
        \begin{equation}
                \mathcal{L}[f''(t)](s) = s^2\mathcal{L}[f(t)](s) - sf(0) - f'(0)
        \end{equation}
        and in general,  \begin{equation}
                \mathcal{L}[f^{(n)}(t)](s) = s^n\mathcal{L}[f(t)](s) - s^{n-1}f(0) - s^{n-2}f'(0) - ... - f^{(n-1)}(0) 
        \end{equation}
\end{pro}


\vspace{1cm}


\begin{pro}[Second Differentiation Formula]
        Given a function $f$ that has a defined laplace transform, we have that \begin{equation}
                \mathcal{L}[tf(t)](s) = -\frac{d}{ds}\left(\mathcal{L}[f(t)](s)\right)
        \end{equation}
        and in general, \begin{equation}
                \mathcal{L}[t^nf(t)](s) = (-1)^n\frac{d^n}{ds^n}\left(\mathcal{L}[f(t)](s)\right)
        \end{equation}
        Equivalently we have that \begin{equation}
                \mathcal{L}^{-1}\left[\frac{d^nF(s)}{ds^n}\right] = (-1)^nt^nf(t)
        \end{equation}
\end{pro}


\vspace{1cm}


\begin{pro}[First Shift Formula]
        Observe that given a function $f$ that has a defined laplace transform, we see that \begin{equation}
                \mathcal{L}[e^{at}f(t)](s) = \int\limits_0^{\infty}e^{-st}e^{at}f(t)dt = \int\limits_0^{\infty}e^{-(s-a)}f(t)dt = \mathcal{L}[f(t)](s-a)
        \end{equation}
        or simply \begin{equation}
                \mathcal{L}[e^{at}f(t)](s) = \mathcal{L}[f(t)](s-a)
        \end{equation}
        Equivalently, we have that \begin{equation}
                \mathcal{L}^{-1}[F(s)](t) = e^{at}\mathcal{L}^{-1}[F(s+a)](t)
        \end{equation}
\end{pro}


\vspace{1cm}


\begin{pro}[Integration Formulas]
        Given a function $f$ that has a defined laplace transform, we have that \begin{equation}
                \mathcal{L}\left[\int\limits_0^tf(r)dr\right](s) = \frac{1}{s}\mathcal{L}[f(t)](s)
        \end{equation}
        or equivalently we have that \begin{equation}
                \mathcal{L}^{-1}\left[\frac{1}{s}F(s)\right] = \int\limits_0^tf(r)dr
        \end{equation}
        Additionally, we have that \begin{equation}
                \mathcal{L}\left[\frac{f(t)}{t}\right](s) = \int\limits_s^{\infty}\mathcal{L}[f(t)](r)dr
        \end{equation}
\end{pro}


\vspace{1cm}


\begin{pro}[Step Function]
        We have that the laplace transform of the unit step function $u_a(t)$ is \begin{equation}
                \mathcal{L}[u_a(t)](s) = \frac{e^{-as}}{s}, s > 0
        \end{equation}
\end{pro}


\vspace{1cm}

\begin{pro}[Second Shift Formula]
        Given a function $f$ that has a defined laplace transform, we have that \begin{align*}
                \mathcal{L}[u_a(t)f(t)](s) &= \int\limits_{0}^{\infty}e^{-st}u_a(t)f(t)dt \\
                &= \int\limits_{a}^{\infty}e^{-st}f(t)dt\tag{take $v = t - a$, $dv=dt$} \\
                &= \int\limits_{0}^{\infty}e^{-s(v+a)}f(v+a)dv\tag{shift $v \rightarrow t$} \\
                &= e^{-as}\int\limits_{0}^{\infty}e^{-st}f(t+a)dt \\
                &= e^{-as}\mathcal{L}[f(t+a)](s)
        \end{align*}
        or succinctly, \begin{equation}
                \mathcal{L}[u_a(t)f(t)](s) = e^{-as}\mathcal{L}[f(t+a)](s)
        \end{equation}
        Equivalently, we have that \begin{equation}
                \mathcal{L}^{-1}[e^{-as}F(s)](t) = u_a(t)\mathcal{L}^{-1}[F(s)](t-a) = u_a(t)f(t-a)
        \end{equation}
\end{pro}



\vspace{1cm}



\subsection{Tables of Laplace Transforms and Inverse Laplace Transforms}


\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Functions and their respective Laplace Transforms}
        \begin{tabular}{c|c}
                Function, $f(t)$ & Laplace Transform, $\mathcal{L}[f(t)](s)$ \\ \hline
                $1$ & $\frac{1}{s}$, $s > 0$ \\
                $e^{at}$ & $\frac{1}{s-a}$, $s > a$ \\
                $\cos(bt)$ or $\sin(bt)$ & $\frac{s}{s^2+b^2}$ or $\frac{b}{s^2+b^2}$ \\
                $t^n$ & $\frac{n!}{s^{n+1}}$ \\
                $e^{at}\cos(bt)$ or $e^{at}\sin(bt)$ & $\frac{s-a}{(s-a)^2+b^2}$ or $\frac{b}{(s-a)^2+b^2}$ \\
                $u_a(t)$ & $\frac{e^{-as}}{s}$ \\
        \end{tabular}
\end{table}
\egroup


\vspace{1cm}

\bgroup
\def\arraystretch{1.5}
\begin{table}[H]
        \centering
        \caption{Functions and their respective Inverse Laplace Transforms}
        \begin{tabular}{c|c|c}
                $F(s) =\mathcal{L}[f(t)](s)$ & Formula to Use & $f(t) = \mathcal{L}^{-1}[F(s)](t)$ \\ \hline
                $\frac{A_1}{s-a}$ & $\mathcal{L}[e^{at}](s)=\frac{1}{s-a}$ &  $A_1e^{at}$ \\
                $\frac{A_2}{(s-a)^2}$ & $\mathcal{L}[te^{at}](s)=\frac{1}{(s-a)^2}$ &  $A_2te^{at}$ \\
                $\frac{A_k}{(s-a)^k}$ & $\mathcal{L}[t^{k-1}e^{at}](s)=\frac{(k-1)!}{(s-a)^k}$ &  $A_k\frac{t^{k-1}e^{at}}{(k-1)!}$ \\
                $\frac{s-a}{(s-a)^2+b^2}$ & $\mathcal{L}[e^{at}\cos(bt)](s) = \frac{s-a}{(s-a)^2+b^2}$ & $e^{at}\cos(bt)$ \\
                $\frac{b}{(s-a)^2+b^2}$ & $\mathcal{L}[e^{at}\sin(bt)](s) = \frac{b}{(s-a)^2+b^2}$ & $e^{at}\sin(bt)$ \\
                $e^{-as}F(s)$ & $\mathcal{L}[u_a(t)f(t-a)](s) = e^{-as}F(s)$ & $u_a(t)f(t-a)$ \\
                $\frac{1}{s^n}$ & $\mathcal{L}[t^n] = \frac{n!}{s^{n+1}}$ & $\frac{t^{n-1}}{(n-1)!}$ \\
        \end{tabular}
\end{table}
\egroup


\vspace{1cm}

\subsection{Solving ODEs with the Laplace Transform}

\begin{defn}[Scheme for Linear ODEs]
        Given a linear ODE, we perform the following steps:
        \begin{enumerate}
                \item Take the laplace transform of both sides of the differential equation using the first differentiation property, and let $Y(s) = \mathcal{L}[y(t)](s)$.
                \item Solve for $Y(s)$ from the new equation.
                \item Compute the inverse laplace transform for $Y(s)$ to obtain \begin{equation}
                                y(t) = \mathcal{L}^{-1}[Y(s)](t)
                \end{equation}
        \end{enumerate}
\end{defn}

\vspace{1cm}


\begin{defn}[Scheme for Systems of DEs]
        Consider a system \begin{equation}
                \B{X}' = \B{A}\B{X}+\B{B}
        \end{equation}
        where $\B{A}$ is a matrix of constants. As before we take the laplace transform of both sides to obtain \begin{equation}
                s\mathcal{L}[\B{X}](s)-\B{X}(0) = \B{A}\mathcal{L}[\B{X}](s) + \mathcal{L}[\B{B}](s)
        \end{equation}
        We solve for $\mathcal{L}[\B{X}](s)$ and then take the inverse laplace transform to find $\B{X}$.
\end{defn}


\vspace{1cm}


\subsection{Convolution}

\begin{defn}[Convolution]
        Let $f$ and $g$ be piecewise continuous functions. The integral \begin{equation}
                (f*g)(t) = \int\limits_0^tf(\tau)g(t-\tau)d\tau
        \end{equation}
        is called the \B{convolution of $f$ and $g$}. Moreover, the convolution is commutative, so \begin{equation}
                (f*g)(t) = (g*f)(t)
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{pro}[Laplace Transform of the Convolution]
        The laplace transform of the convolution is given by \begin{equation}
                \mathcal{L}[f*g](s) = \mathcal{L}[f](s)\mathcal{L}[g](s)
        \end{equation}
\end{pro}




\clearpage

\section{Fourier Series}


\subsection{Orthogonality}

\begin{defn}[Orthogonal]
        We say that two integrable functions $f$ and $g$ are \B{orthogonal} on an interval $[a,b]$ if \begin{equation}
                \int\limits_a^bf(x)g(x)dx = 0
        \end{equation}
        More generally we say that the functions $\phi_1,\phi_2,...,\phi_n,...$ (finitely or infinitely many) are orthogonal on $[a,b]$ if \begin{equation}
                \int\limits_a^b\phi_i(x)\phi_j(x)dx = 0\;\text{whenever}\;i\neq j
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{thm}[Orthogonal Series Coefficients]
        Suppose the functions $\phi_1,\phi_2,\phi_3,...$, are orthogonal on $[a,b]$ and \begin{equation}
                \int\limits_a^b\phi_n^2(x)dx \neq 0,\;\;n=1,2,3,...
        \end{equation}
        Let $c_1,c_2,c_3,...$ be constants such that the partial sums $f_N(x) = \sum_{m=1}^Nc_m\phi_m(x)$ satisfy the inequalities \begin{equation}
                |f_N(x)| \leq M.\;\;a \leq x \leq b,\;\;N=1,2,3,...
        \end{equation}
        for some constant $M < \infty$. Suppose also that the series \begin{equation}
                f(x) = \sum\limits_{m=1}^{\infty}c_m\phi_m(x)
        \end{equation}
        converges and is integrable on $[a,b]$. Then \begin{equation}
                c_n = \frac{\int\limits_a^bf(x)\phi_n(x)dx}{\int\limits_a^b\phi_n^2(x)dx},\;\;n=1,2,3,...
        \end{equation}
\end{thm}


\vspace{1cm}


\subsection{Fourier Expansions}

\begin{defn}[Fourier Expansion]
        Suppose $\phi_1,\phi_2,...,\phi_n,...$ are orthogonal on $[a,b]$ and $\int_a^b\phi_n2(x)\neq 0$, $n=1,2,3,...$. Let $f$ be integrable on $[a,b]$, and define \begin{equation}
                c_n = \frac{\int\limits_a^bf(x)\phi_n(x)dx}{\int\limits_a^b\phi_n^2(x)dx},\;\;n=1,2,3,...
        \end{equation}
        Then the infinite series $\sum_{n=1}^{\infty}c_n\phi_n(x)$ is called the \B{Fourier Expansion} of $f$ in terms of the orthogonal set $\{\phi_n\}_{n=1}^{\infty}$, and $c_1,c_2,...,c_n,...$ are called the \B{Fourier Coefficients} of $f$ with respect to $\{\phi_n\}_{n=1}^{\infty}$. We indicate the relationship between $f$ and its Fourier Expansion by \begin{equation}
                f(x) \sim \sum\limits_{n=1}^{\infty}c_n\phi_n(x),\;\;a\leq x\leq b
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{defn}[Fourier Series]
        If $f$ is integrable on $[-L,L]$, its Fourier expansion in terms of the orthogonal functions \begin{equation}
                1, \cos\frac{\pi x}{L}, \sin\frac{\pi x}{L},\cos\frac{2\pi x}{L},\sin\frac{2\pi x}{L},...,\cos\frac{n\pi x}{L},\sin\frac{n\pi x}{L},...
        \end{equation}
        is called the \B{Fourier Series} of $f$ on $[-L,L]$. In particular, it is of the form \begin{equation}
                a_0 + \sum\limits_{n=1}^{\infty}\left(a_n\cos\frac{n\pi x}{L}+b_n\sin\frac{n\pi x}{L}\right)
        \end{equation}
        Moreover, since \begin{equation*}
                \int\limits_{-L}^L1^2dx = 2L
        \end{equation*}
        \begin{equation*}
                \int\limits_{-L}^L\cos^2\frac{n\pi x}{L}dx = L
        \end{equation*}
        and \begin{equation*}
                \int\limits_{-L}^L\sin^2\frac{n\pi x}{L}dx = L
        \end{equation*}
        we get that \begin{equation}
                a_0 = \frac{1}{2L}\int\limits_{-L}^Lf(x)dx
        \end{equation}
        \begin{equation}
                a_n = \frac{1}{L}\int\limits_{-L}^Lf(x)\cos\frac{n\pi x}{L}dx,\;\;\text{and}\;\;b_n = \frac{1}{L}\int\limits_{-L}^Lf(x)\sin\frac{n\pi x}{L}dx, n \geq 1
        \end{equation}
\end{defn}


\vspace{1cm}


\begin{defn}[Piecewise Smooth]
        A function $f$ is said to be piecewise smooth on $[a,b]$ if:\begin{enumerate}
                \item $f$ has at most finitely many points of discontinuity in $(a,b)$;
                \item $f'$ exists and is continuous except possibly at finitely many points in $(a,b)$;
                \item $f(x_0+)=\lim\limits_{x\rightarrow x_0^+}f(x)$ and $f'(x_0+)=\lim\limits_{x\rightarrow x_0^+}f'(x)$ exist if $a \leq x_0 < b$;
                \item $f(x_0-)=\lim\limits_{x\rightarrow x_0^-}f(x)$ and $f(x_0-)=\lim\limits_{x\rightarrow x_0^-}f(x)$ exist if $a < x_0 \leq b$
        \end{enumerate}
\end{defn}


\vspace{1cm}

\begin{thm}[Convergence of Fourier Series]
        If $f$ is piecewise smooth on $[-L,L]$, then the Fourier series \begin{equation}
                F(x) = a_0 + \sum\limits_{n=1}^{\infty}\left(a_n\cos\frac{n\pi x}{L}+b_n\sin\frac{n\pi x}{L}\right)
        \end{equation}
        of $f$ on $[-L,L]$ converges for all $x$ in $[-L,L]$; moreover, \begin{equation}
                F(x) = \left\{\begin{array}{cc} \frac{f(x+)+f(x-)}{2} & \text{if}\;-L < x < L \\ \frac{f(-L+)+f(L-)}{2} & \text{if}\;x=\pm L \end{array}\right.
        \end{equation}
\end{thm}

\vspace{3cm}


\begin{center}
        \begin{Huge}
                \textit{\B{Fin.}}
        \end{Huge}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%-------------- End ---------------

\end{document}
