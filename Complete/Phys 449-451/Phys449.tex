\documentclass[12pt, a4paper, oneside, openright, titlepage]{book}
\usepackage[utf8]{inputenc}
\raggedbottom
\usepackage{import}


\input{../book_packages}

%%% Specific Macros %%%


%%%%%% BEGIN %%%%%%%%%%


\begin{document}

%%%%%% TITLE PAGE %%%%%

\begin{titlepage}
    \centering
    \scshape
    \vspace*{\baselineskip}
    \rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt}
    \rule{\textwidth}{0.4pt}
    
    \vspace{0.75\baselineskip}
    
    {\LARGE Statistical Mechanics: A Complete Guide}
    
    \vspace{0.75\baselineskip}
    
    \rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt}
    \rule{\textwidth}{1.6pt}
    
    \vspace{2\baselineskip}
    Phys 449 \\
    \vspace*{3\baselineskip}
    \monthdayyeardate\today \\
    \vspace*{5.0\baselineskip}
    
    {\scshape\Large E/Ea Thompson(they/them), \\ Physics and Math Honors\\}
    
    \vspace{1.0\baselineskip}
    \textit{Solo Pursuit of Learning}
    \vfill
    \enlargethispage{1in}
    \begin{figure}[b!]
    \makebox[\textwidth]{\includegraphics[width=\paperwidth, height =10cm]{../Crab.jpg}}
    \end{figure}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Part 1
\part{Thermodynamics}

%%%%%%%%%%%%%%%%%%%%%% Chapter 1.1
\chapter{Energy in Thermal Physics}

%%%%%%%%%%%%%%%%%%%% Section 1.1.1
\section{Basic Notation and Work}

\begin{defn}
    Thermodynamics is a \Emph{phenomenological} description of properties of \Emph{macroscopic systems} in \Emph{thermal equilibrium}.
\end{defn}

By a phenomenological description we mean a discription based on observations and direct experience of the experimenter with the system, considered as a ``black box" (a system whose internal structure is unknown, or is just not considered). The task of thermodynamics is to define appropriate physical quanitities, \Emph{state quantities}, which characterize macroscopic properties of matter, that is \Emph{macrostates}, in a way which is as unambiguous as possible, and to relate these quantities by means of universally valid equations.

\begin{defn}[Systems]
    There are a number of different thermodynamic systems. In particular we define a thermodynamic system, in general, to be an arbitrary amount of matter, the properties of which can be uniquely and completely described by specifying certain macroscopic parameters. We summarize them as follows: \begin{itemize}
        \item \Emph{Thermodynamic or Macroscopic System:} A system consisting of a large number of constituents. For example, a mole of gas (approximately $10^{23}$ particles) can be considered as a macroscopic system.
        \item \Emph{Isolated Thermodynamic System:} A system which exhibits no exchange of any type with the surroundings; no exchange of work, heat, matter, etc. The total energy (mechanical, electrical, etc.) is a conserved quantity for such a system, and can thus be used to characterize the macrostate.
        \item \Emph{Closed Theormodynamic System:} A system which exhibits no exchange of matter with its surroundings. Hence, energy exchange is allowed, so energy is no longer a conserved quantity and can fluctuate due to exchange with the surroundings. The termperature, particle number, and volume of the system can characterize the macrostate.
        \item \Emph{Open Thermodynamic System:} A system for which it is possible for exchange of any type with the surroundings (work, heat, matter, etc.). Energy and particle number are both not conserved. But, temperature and chemical potential can still be used to characterize a macrostate.
    \end{itemize}
\end{defn}

If the properties of a system are the same for any part of it, one calls such a system \Emph{homogeneous}. On the other hand, if the properties change discontinuously at certain marginal surfaces, the system is \Emph{heterogeneous}, with the homogeneous portions of the system called \Emph{phases} and the separating surfaces \Emph{phase boundaries}. 

\begin{defn}
    The macroscopic quantities which describe a system are called state quantities.
\end{defn}

\begin{eg}
    Examples of state quantities are the system's energy $E$, its volume $V$, its particle number $N$, its entropy $S$, its temperature $T$, the pressure $P$, the chemical potential $\mu$, the charge $q$, the dipole momentum, the refractive index, the viscosity, the chemical composition, and the size of phase boundaries.
\end{eg}

However, microscopic quantities do not fall under the umbrella of state quantities.

\begin{defn}[Equilibrium]
    Two thermodynamic systems are in \Emph{equilibrium} if and only if they are in contact such that they can exchange a given conserved quantity (for example particles) and they are relaxed to a state in which there is no average net transfer of that quantity between them anymore. A thermodynamic system $S$ is in equilibrium with itself, if and only if, all its subsystems are in equilibrium with each other. In this case, $S$ is called an \Emph{equilibrium} system.
\end{defn}

From this definition we find that different types of exchanged quanitities can lead to different types of equilibria: 

\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        \hline
        Exchanged Quantity & Type of Equilibrium \\ \hline \hline
        Particles/Matter & Diffusive Equilibrium \\ 
        Work & Mechanical Equilibrium \\
        Heat & Thermal Equilibrium \\ \hline
    \end{tabular}
\end{table}

\begin{defn}
    \Emph{Complete Thermodynamic Equilibrium} corresponds to a state where all the conserved fluxes between two coupled thermodynamic systems vanish.
\end{defn}

A way of testing if a given system is in a complete thermodynamic equilibrium is if the properties of the system do not change appreciably over the observation time, which is to say the properties reflect the true asymptotic long-term properties after any initial relaxation time is over.

The state of thermodynamic systems in complete thermodynamic equilibrium can be described by a set of independent \Emph{thermodynamic coordinates} or \Emph{state variables}; this fact is based on empirical observations. 

\subsection{Boyle-Mariotte's Experiment}

\begin{law}
    For a given mass of gas at a constant temperature $T$, the volume $V$ is inverseley proportional to the pressure $P$: $V \propto P^{-1}$.
\end{law}

We consider Robert Boyle's (1627-1691) experiment, conducted at room temperature, which should remain constant during the time of the experiment. We use a vertical tube with markings to indicate the volume of gas, and oil at the bottom. By applying pressure on the oil, we also exert a pressure on the air in the tube above the oil, causing the volume to decrease. Drawing the graph of volume versus $P^{-1}$ we obtain a straight line, so $V\cdot P = C$ for some constant $C$, given a fixed temperature. As further measurements show, the constant is proportional to the temperature $T$, so $V\cdot P \propto T$. More precisely, the full equation of a state of simple low-density gase is given by \begin{equation}
    \boxed{PV = Nk_BT}
\end{equation}
where $N$ is the number of gas particles and $k_B \approx 1.381\times 10^{-23}\;J/K$ is a natural constant called the \Emph{Boltzmann's constant}. The above equation is called the \Emph{ideal gas law} or the \Emph{equation of state for an ideal gas} since it relates the three state variables, or thermodynamic coordinates, $P$, $V$, and $T$ at equilibrium.

\subsubsection{Mechanical Derivation}

We now sketch a derivation of the ideal gas law using an atomistic theory governed by Newton's Laws. First assume there is a single molecule or atom bouncing around in a volume $V = AL$, where $L$ is the length along the $x$-direction and $A$ is the area of a side parallel to the $yz$-plane:

\begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1.4,xscale=1.4]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Cube [id:dp41058044355698486] 
\draw   (130,134) -- (154,110) -- (240,110) -- (240,166) -- (216,190) -- (130,190) -- cycle ; \draw   (240,110) -- (216,134) -- (130,134) ; \draw   (216,134) -- (216,190) ;
%Straight Lines [id:da3652266253176417] 
\draw    (132,200) -- (208,200) ;
\draw [shift={(210,200)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (6.56,-1.97) .. controls (4.17,-0.84) and (1.99,-0.18) .. (0,0) .. controls (1.99,0.18) and (4.17,0.84) .. (6.56,1.97)   ;
\draw [shift={(130,200)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (6.56,-1.97) .. controls (4.17,-0.84) and (1.99,-0.18) .. (0,0) .. controls (1.99,0.18) and (4.17,0.84) .. (6.56,1.97)   ;
%Straight Lines [id:da08447016298808463] 
\draw    (166.89,172.06) -- (190.59,155.21) ;
\draw [shift={(192.22,154.06)}, rotate = 504.61] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (6.56,-1.97) .. controls (4.17,-0.84) and (1.99,-0.18) .. (0,0) .. controls (1.99,0.18) and (4.17,0.84) .. (6.56,1.97)   ;
%Curve Lines [id:da14521036207082538] 
\draw    (268.22,143.72) .. controls (250.49,160.14) and (256.69,125.13) .. (232.99,150.84) ;
\draw [shift={(231.89,152.06)}, rotate = 311.76] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (6.56,-1.97) .. controls (4.17,-0.84) and (1.99,-0.18) .. (0,0) .. controls (1.99,0.18) and (4.17,0.84) .. (6.56,1.97)   ;
%Straight Lines [id:da5265921627377463] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (130,190) -- (141.22,190) ;
\draw [shift={(143.22,190)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (4.37,-1.32) .. controls (2.78,-0.56) and (1.32,-0.12) .. (0,0) .. controls (1.32,0.12) and (2.78,0.56) .. (4.37,1.32)   ;
%Straight Lines [id:da8512625721822618] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (130,190) -- (137.8,182.35) ;
\draw [shift={(139.22,180.94)}, rotate = 495.52] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (4.37,-1.32) .. controls (2.78,-0.56) and (1.32,-0.12) .. (0,0) .. controls (1.32,0.12) and (2.78,0.56) .. (4.37,1.32)   ;
%Straight Lines [id:da2755149870872269] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (130,190) -- (130,181.94) ;
\draw [shift={(130,179.94)}, rotate = 450] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (4.37,-1.32) .. controls (2.78,-0.56) and (1.32,-0.12) .. (0,0) .. controls (1.32,0.12) and (2.78,0.56) .. (4.37,1.32)   ;

% Text Node
\draw (167,198) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$L$};
% Text Node
\draw (166.89,172.06) node [anchor=center][inner sep=0.75pt]  [font=\LARGE,color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$\cdot $};
% Text Node
\draw (170.56,151.96) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$\vec{v}$};
% Text Node
\draw (269.56,135.96) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$A$};
% Text Node
\draw (142.56,189.84) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$x$};
% Text Node
\draw (140.89,176.51) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$y$};
% Text Node
\draw (124.89,173.84) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$z$};


\end{tikzpicture}
\end{center}

After many perfectly ellastic collisions against the wall, the average pressure directly on the wall is \begin{equation*}
    \overline{p} = \frac{\overline{F}_{x,on\;wall}}{A} = -\frac{\overline{F}_{x,on\;molecule}}{A} = -\frac{m\overline{\frac{\Delta v_x}{\Delta t}}}{A} 
\end{equation*}
The time it takes for a full round trip is $\Delta t = 2L/v_x$. When it undergoes one elastic collision, its change in velocity is $\Delta v_x = -2v_x$. Together these give \begin{equation*}
    \overline{p} = -\frac{m(-2v_x/(2L/v_x))}{A} = \frac{mv_x^2}{LA} = \frac{mv_x^2}{V}
\end{equation*}
Now, if we extend to $N >> 1$ non-interacting molecules, each colliding with the walls totally elastically, then we can forgoe the averageness of the pressure to obtain \begin{equation*}
    pV = m\sum_{i=1}^N(v_x^2)_i = mN\overline{v_x^2} = Nk_BT
\end{equation*}
using the ideal gas law at the end, which gives the relation \begin{equation*}
    \overline{K}_x = \overline{\frac{1}{2}mv_x^2} = \frac{1}{2}k_BT
\end{equation*}
Then we find that the total mean kinetic energy is \begin{equation}
    \overline{K} = \frac{1}{2}m\overline{v_x^2}+\frac{1}{2}m\overline{v_y^2}+\frac{1}{2}m\overline{v_z^2} = \frac{3}{2}k_BT
\end{equation}

\begin{defn}
    The \Emph{Root-mean square speed} of each atom/molecule is defined as \begin{equation*}
        v_{RMS} = \sqrt{\overline{v^2}}
    \end{equation*}
\end{defn}

In general the RMS speed is often a close approximation of the average speed. In this case we have $v_{RMS} = \sqrt{3k_BT/m}$.


\subsection{Work}

\begin{defn}
    Given a vector force field $\vec{F}$ defined along a path $\gamma$, the \Emph{work} $W$ is of $\vec{F}$ along $\gamma$ is defined to be: \begin{equation}
        W = \int_{\gamma}\vec{F}\cdot d\vec{r}
    \end{equation}
\end{defn}

\begin{rec}
    Recall that pressure is force per unit area, so we have that \begin{equation*}
        P = \frac{F}{A}
    \end{equation*}
    where $F$ is the normal component of the force to the area $A$. More precisely, pressure is the proportionality constant that relates the force and normal vectors: \begin{equation*}
        d\vec{F}_n = -pd\vec{A}
    \end{equation*}
    where $\vec{F}_n$ denotes the normal component of the force vector $\vec{F}$.
\end{rec}


\begin{eg}
    Consider the compression of a gas by a piston with a constant force of magnitude $F$.
    \begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1.2,xscale=1.2]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Rectangle [id:dp9125760638904235] 
\draw   (100,110) -- (200,110) -- (200,160) -- (100,160) -- cycle ;
%Straight Lines [id:da19743423815417427] 
\draw    (200,110) -- (230,110) ;
%Straight Lines [id:da17599416360044806] 
\draw    (200,160) -- (230,160) ;
%Straight Lines [id:da37025754978607806] 
\draw [color={rgb, 255:red, 255; green, 27; blue, 0 }  ,draw opacity=1 ]   (200,110) -- (200,160) ;
%Straight Lines [id:da5752016333947421] 
\draw [color={rgb, 255:red, 255; green, 27; blue, 0 }  ,draw opacity=1 ]   (200,135) -- (250,135) ;
%Straight Lines [id:da4588247522685671] 
\draw    (230,127) -- (222,127) ;
\draw [shift={(220,127)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (4.37,-1.32) .. controls (2.78,-0.56) and (1.32,-0.12) .. (0,0) .. controls (1.32,0.12) and (2.78,0.56) .. (4.37,1.32)   ;
%Curve Lines [id:da22459360243367987] 
\draw [line width=0.75]    (281.89,123.39) .. controls (280.27,107.22) and (224.4,110.81) .. (204.92,121.69) ;
\draw [shift={(203.22,122.72)}, rotate = 326.56] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (6.56,-1.97) .. controls (4.17,-0.84) and (1.99,-0.18) .. (0,0) .. controls (1.99,0.18) and (4.17,0.84) .. (6.56,1.97)   ;
%Straight Lines [id:da04861500853364609] 
\draw    (200,170) -- (192,170) ;
\draw [shift={(190,170)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (4.37,-1.32) .. controls (2.78,-0.56) and (1.32,-0.12) .. (0,0) .. controls (1.32,0.12) and (2.78,0.56) .. (4.37,1.32)   ;
%Straight Lines [id:da2692578781738837] 
\draw    (200,167) -- (200,173) ;

% Text Node
\draw (231,122.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny,color={rgb, 255:red, 246; green, 12; blue, 12 }  ,opacity=1 ]  {$\vec{F}$};
% Text Node
\draw (237,125) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {(const.)};
% Text Node
\draw (270.89,125.89) node [anchor=north west][inner sep=0.75pt]  [font=\tiny] [align=left] {Piston area:};
% Text Node
\draw (311.89,125.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny,color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$\mathcal{A}$};
% Text Node
\draw (108.56,114.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (128.56,134.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (128.56,134.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (139.89,121.07) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (159.89,141.07) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (127.56,112.07) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (161.56,117.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (181.56,137.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (110.22,132.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (179.22,114.73) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (142.89,141.4) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (168.22,127.73) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (148.22,112.73) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (123.56,124.07) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (111.56,142.73) node [anchor=north west][inner sep=0.75pt]  [font=\small]  {$\cdot $};
% Text Node
\draw (192,173.4) node [anchor=north west][inner sep=0.75pt]  [font=\tiny]  {$\Delta x$};


\end{tikzpicture}
\end{center}
    so the work is \begin{equation*}
        W = \int_{\gamma}\vec{F}\cdot d\vec{r} = F\int_{\gamma}dr = F\Delta x = PA\Delta x = -P\Delta V
    \end{equation*}
    where $P$ is the pressure and $\Delta V$ is the change in volume. Since the change in volume is negative, the work $W$ done on the system (here the gas) is positive: Energy is added to the system by a force (macroscopic) process (here the piston). $W< 0$ if energy is removed from the system.
\end{eg}


\begin{defn}
    The generalized differential form for work is given by \begin{equation}
        \delta W = \sum_{i=1}^mJ_idq_i
    \end{equation}
    where $q_i$ are the generalized coordinates and the $J_i$ are the conjugate generalized forces such that $J_idq_i$ has units of energy. This is possible when the system is undergoing an infinitesimal quasi-static transformation.
\end{defn}

Here are a few examples of generalized coordinates and their corresponding generalized conjugate forces:

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        & Generalized Force $J_i$ & Generalized Coordinate $dq_i$ \\ \hline \hline
        Pressure & $-P$ & $dV$ (change in volume) \\ 
        Surface tension & $\sigma$ & $dS$ (change in surface area) \\
        Magnetic field & $\vec{B}_0$ & $d\vec{m}$ (change in magnetic moment) \\
        Electric field & $\vec{E}$ & $d\vec{P}$ (change in electric dipole moment) \\\hline
    \end{tabular}
\end{table}

\begin{defn}
    Differential changes in a systems property are said to be \Emph{quasi-static} if the changes occur on time scales much longer than the relaxation time such that the system remains in equilibrium at all times.
\end{defn}

To ensure thermodynamics is a self-consistent description of macroscopic systems in thermal equilibrium we must assume that the differential changes $\delta W$ are quasi-static. This also ensures that we can describe the state of such a thermodynamic system by a set of thermodynamic coordinates at all times, which is to state at any stage of the process the thermodynamic coordinates of the system exist and can in principle be computed. 

Depending on the macroscopic generalized force, the work necessary to transfer a thermodynamic system in complete thermodynamic equilibrium from state $A$ to state $B$ might or might not depend on the path taken. For example, if $\gamma_1$ and $\gamma_2$ are two paths from state $A$ to state $B$, it is possible that $\int_{\gamma_1}\delta W \neq \int_{\gamma_2}\delta W$. (Note that this justifies the use of the notation $\delta W$ over $dW$, which would denote an exact differential)


\begin{defn}
    State quantities which are proportional to the amount of matter in a system are called \Emph{extensive}, and are consequently additive when looking at subsystems.
\end{defn}

\begin{defn}
    State quantities which are independent of the amount of matter in the system are called \Emph{intensive}, and are not additive for the particular phases of the system.
\end{defn}

Intensive quantities are indicators of equilibrium. 


\subsection{Conservative Forces}

\begin{rec}
    Recall that a conservative force is a force $\vec{F}$ with an associated potential energy function $E$ such that $\vec{F} = \nabla E$.
\end{rec}

\begin{prop}
    If the macroscopic generalized force $\vec{J}$, depending on $m$ generalized coordinates $q_i$, is conservative with potential energy $E_{pot}$ which is twice continuously differentiable and the domain of integration is simply connected, then the following are equivalent:
    \begin{itemize}
        \item $\vec{J}(\vec{q}) = -\nabla E_{pot}(\vec{q})$
        \item $dW$ is an exact differential form, so $$\int_{\gamma}dW = \int_{\gamma}\vec{J}\cdot d\vec{r}$$ depends only on the endpoints of $\gamma$.
        \item For any simple closed path $\gamma$, $$\oint_{\gamma}dW = \oint_{\gamma}\vec{J}\cdot d\vec{r} = 0$$
        \item The curl of $\vec{J}$ is trivial over the domain of integration $$\nabla \times \vec{J} = 0$$ provided that $m = 3$
        \item For all $i,j \in \{1,2,...,m\}$, we have that \begin{equation*}
                \frac{\partial J_i}{\partial q_j} - \frac{\partial J_j}{\partial q_i} = 0
        \end{equation*}
        \item The differential $dW$ is exact, and we have that \begin{equation*}
                dW = -\nabla E_{pot}\cdot d\vec{r} = -\sum_{i=1}^m\frac{\partial E_{pot}}{\partial q_i}dq_i = -dE_{pot}
        \end{equation*}
    \end{itemize}
\end{prop}


\begin{defn}
    For a function $A(q_1,q_2,...,q_m)$ the \Emph{total differential} or \Emph{exact differential} of $A$ is given by \begin{equation*}
        dA = \sum_{i=1}^m\frac{\partial A}{\partial q_i}dq_i
    \end{equation*}
    This corresponds to a generalized chain rule: \begin{equation*}
        \frac{dA}{dt} = \nabla A\cdot \frac{d}{dt}\vec{q} = \sum_{i=1}^m\frac{\partial A}{\partial q_i}\frac{dq_i}{dt}
    \end{equation*}
    called the \Emph{total derivative} of $A$ with respect to $t$. An exact differential corresponds to an \Emph{integrable differential form}: \begin{equation*}
        \int_{\gamma}\sum_{i=1}^m\frac{\partial A}{\partial q_i}dq_i = \int_{\gamma}dA = A(\vec{q}_f) - A(\vec{q}_i)
    \end{equation*}
    where $\vec{q}_i$ and $\vec{q}_f$ are the initial and final point of the path $\gamma$, respectively.
\end{defn}

Consequently, an integrable, or exact, differential form does not depend on the path taken and in physics we would consider $A$ to be a potential function. 

\begin{note}
    For non-conservative forces, $\delta W$ is an \Emph{inexact differential form}.
\end{note}

\begin{thm}
    A differential form $$\delta A \equiv \sum_{i=1}^ma_i(q_1,q_2,...,q_m)dq_i$$ for functions $a_i:D \subseteq \R^m\rightarrow \R$ is exact if and only if \begin{equation*}
        \frac{\partial a_i}{\partial q_j} = \frac{\partial a_j}{\partial q_i}
    \end{equation*}
    for all $i,j \in \{1,2,...,m\}$.
\end{thm}

\begin{defn}
    An \Emph{integrating factor} $\mu$ is a factor that makes an inexact differential form exact upon multiplication.
\end{defn}

\begin{thm}
    For $m = 2$, an integrating factor always exists. Specifically, for $\delta A = a_1dx_1+a_2dx_2$, we can define $df:= \mu \delta A = (\mu a_1)dx_1 + (\mu a_2)dx_2$, with $\mu$ determined non-uniquely by the equation \begin{equation*}
        \frac{\partial (\mu a_1)}{\partial x_2} = \frac{\partial (\mu a_2)}{\partial x_1}
    \end{equation*}
\end{thm}


%%%%%%%%%%%%%%%%%%%% Section 1.1.2
\section{Equipartition Theorem}

\begin{namthm}[Equipartition Theorem]
    Every degree of freedom of a system in thermodynamic equilibrium, which appear only quadratically in the total energy, constributes an average energy of $\frac{1}{2}Nk_BT$ to the total energy of the system.
\end{namthm}

First, what exactly is a `degree of freedom'? In colloquil terms, a degree of freedom tells you about what the atom or molecule can actually do - maybe it can move around (translate), or vibrate, or rotate, or produce sound, or emit light, or a host of other things. Each of these capabilities contributes an energy of $\frac{1}{2}k_BT$ to the system's energy. 

Now, recall that the kinetic energy of a rotating body given a diagonalized inertial basis $I_{ii}$, is \begin{equation}
    K_{rot} = \frac{1}{2}(I_{xx}w_x^2+I_{yy}w_y^2+I_{zz}w_z^2)
\end{equation}
and $w_i$ are the angular frequences associated with the angular velocity of the body in this basis. Each of these terms is quadratic, so each one corresponds to a degree of freedom. 

We can conclude that the total energy of the system in some limit is \begin{equation*}
    \frac{f}{2}Nk_BT
\end{equation*}
where $f$ is the number of degrees of freedom.

%%%%%%%%%%%%%%%%%%%% Section 1.1.3
\section{Heat and the 1st Law of Thermodynamics}

\begin{defn}
    Recall that work corresponds to the change in energy of a thermodynamic system by a \Emph{macroscopically forced process}. On the other hand the \Emph{heat} $Q$ is the energy added to or removed from a thermodynamic system by a \Emph{spontaneous process} (that is not forced).
\end{defn}

Heat can be also stated as the amount of energy that spontaneously flows between two systems: heat moves from the system at higher temperature to the system at lower temperature (2nd law of thermodynamics to be covered later). Note we can define the temperature through the equipartition theorem as a measure of the total energy of the system $E$: $T \approx 2E/fk_B$, where $f$ is the number of degrees of freedom.

We note that negative $Q$ implies removing energy while positive $Q$ implies adding energy.

For example, consider the energy transfer between a cooking plate and a pot of water. The origin of this type of process lies in the underlying microscopic dynamics, which we will explore using Statistical Mechanics. 

\begin{law}[First Law of Thermodynamics]
    For an isolated thermodynamic system, the total \Emph{internal energy} $U$ is a constant and $dU = 0$. By the definition of internal energy, $dU$ is an exact differential; consequently, $U$ should be unique for a given state. For various systems we have the following: \begin{itemize}
        \item For a closed system, $dU = \delta Q + \delta W$. 
        \item For an open system, $dU = \delta Q + \delta W + \delta E_c$, with $$\delta E_c := \sum_{i=1}^{\alpha}\mu_idN_i$$ where $N_i$ is the number of particles of type $i$ and $\mu_i$ is the \Emph{chemical potential} associated with particles of type $i$ for all $1 \leq i \leq \alpha$ with $\alpha$ being the number of different particle types. The chemical potential corresponds to the energy needed to add a particle of type $i$ to a given thermodynamic system while no other type of energy is exchanged, which is to say $\delta W = 0$ and $\delta Q = 0$.
    \end{itemize}
\end{law}

In summary, the empirical first law is a reformulation of the conservation of energy and requires the inclusion of heat. 





%%%%%%%%%%%%%%%%%%%%%% Chapter 1.2
\chapter{Thermodynamical Systems}


%%%%%%%%%%%%%%%%%%%% Section 1.2.1
\section{The 0th Law of Thermodynamics}

We now wish to define the concept of temperature from a \Emph{macroscopic} perspective, to ensure our theoretical framework of thermodynamics is a self-consistent description of macroscopic systems in thermal equilibrium. The empirical principal we proceed with in our construction is that two systems which are in thermal equilibrium with one another should share the same ``temperature." Before we showed a derivation of the temperature in terms of kinetic energy from a microscopic perspective of sparse (non-dense) gas - this belongs to the area of Statistical mechanics.

Now, the $0$th law of thermodynamics describes the transitivity of the equilibrium relation for systems:

\begin{law}[0th Law of Thermodynamics]
    If two systems, $A$ and $B$, are separately in equilibrium with a third system, $C$, then they are also in thermal equilibrium with one another.
\end{law}

Indeed we can depict this as follows:

\begin{center}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Rectangle [id:dp7440238011885612] 
\draw   (100,120) -- (200,120) -- (200,173.5) -- (100,173.5) -- cycle ;
%Shape: Rectangle [id:dp00894291260356117] 
\draw   (242,120) -- (342,120) -- (342,173.5) -- (242,173.5) -- cycle ;
%Shape: Rectangle [id:dp3516092606802361] 
\draw   (100,210) -- (342,210) -- (342,282.5) -- (100,282.5) -- cycle ;
%Straight Lines [id:da2634357285779194] 
\draw    (292,175.08) -- (292,208) ;
\draw [shift={(292,210)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw [shift={(292,173.08)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da874447176315235] 
\draw    (525.67,146.8) -- (563.67,146.8) ;
\draw [shift={(565.67,146.8)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw [shift={(523.67,146.8)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da5304557286713043] 
\draw    (150,175.08) -- (150,208) ;
\draw [shift={(150,210)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw [shift={(150,173.08)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da12969934099335645] 
\draw    (359.4,198.5) -- (404.18,198.5)(359.4,201.5) -- (404.18,201.5) ;
\draw [shift={(412.18,200)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw [shift={(351.4,200)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp038213352648847954] 
\draw   (423,120) -- (523,120) -- (523,173.5) -- (423,173.5) -- cycle ;
%Shape: Rectangle [id:dp8987344627936489] 
\draw   (565,120) -- (665,120) -- (665,173.5) -- (565,173.5) -- cycle ;
%Shape: Rectangle [id:dp787394422240979] 
\draw   (423,210) -- (665,210) -- (665,282.5) -- (423,282.5) -- cycle ;
%Straight Lines [id:da9453296500738602] 
\draw    (615,175.08) -- (615,208) ;
\draw [shift={(615,210)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw [shift={(615,173.08)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da5113397508185176] 
\draw    (473,175.08) -- (473,208) ;
\draw [shift={(473,210)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw [shift={(473,173.08)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

% Text Node
\draw (109.78,130.07) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$A$};
% Text Node
\draw (252.44,123.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$B$};
% Text Node
\draw (208.44,229.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$C$};
% Text Node
\draw (432.78,130.07) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$A$};
% Text Node
\draw (575.44,123.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$B$};
% Text Node
\draw (531.44,229.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$C$};
% Text Node
\draw (128.67,184.22) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\fontfamily{ptm}\selectfont heat}};
% Text Node
\draw (296.67,186.22) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\fontfamily{ptm}\selectfont heat}};
% Text Node
\draw (451.33,186.22) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\fontfamily{ptm}\selectfont heat}};
% Text Node
\draw (534.67,132.89) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\fontfamily{ptm}\selectfont heat}};
% Text Node
\draw (620,185.56) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {{\fontfamily{ptm}\selectfont heat}};


\end{tikzpicture}
\end{center}

where the double-sided arrows labeled ``heat" correspond to equilibrium between the systems. This law leads to existence of the state function $\Theta$, known as the \Emph{empirical temperature}, such that systems in equilibrium are at the same temperature:

\begin{cor}
    The $0$th law of thermodynamics (transitivity of equilibria) implies the existence of temperature (state function).
\end{cor}
\begin{proof}
    Assume that systems $A$ and $C$ are allowed to exchange heat and that they are in thermal equilibrium with each other. The state of system $A$ is described by thermodynamic coordinates $A_1,A_2,...$ and that of $C$ by $C_1,C_2,...$. If now, for example, $A_1$ is changed, then this will potentially lead to changes in $A_2,A_3,...$ and $C_1,C_2,...$. Mathematically, this means that a function $f_{AC}$ exists such that \begin{equation*}
        f_{AC}(A_1,A_2,...,C_1,C_2,...) = 0,
    \end{equation*}
    which is to say the function is constant for all thermodynamic coordinates that correspond to a thermal equilibrium between $A$ and $C$. This has the form of a \Emph{constraint}. Similarly, we have another function $f_{BC}$ with \begin{equation*}
        f_{BC}(B_1,B_2,...,C_1,C_2,...) = 0,
    \end{equation*}
    describing the thermal equilibrium between systems $B$ and $C$, where the state coordinates of $B$ are $B_1,B_2,...$. For such physical constraints, the implicit function theorem applies and we can solve the above equations for any specific parameter, for example \begin{equation*}
        C_1 = F_{AC}(A_1,A_2,...,C_2,C_3,...)
    \end{equation*}
    and \begin{equation*}
        C_1 = F_{BC}(B_1,B_2,...,C_2,C_3,...)
    \end{equation*}
    for thermodynamic coordinates $A_1,A_2,...,B_1,B_2,...,C_2,C_3,...$ such that the constraints hold, so the systems are in thermodynamic equilibrium. Thus, as $C$ is separately in equilibrium with $A$ and $B$, this implies that \begin{equation*}
        F_{AC}(A_1,A_2,...,C_2,...) = F_{BC}(B_1,B_2,...,C_2,...)
    \end{equation*}
    Now the $0$th law ensures that $A$ and $B$ are also in thermal equilibrium so there is a function $f_{AB}$ such that \begin{equation*}
        f_{AB}(A_1,A_2,...,B_1,B_2,...) = 0
    \end{equation*}
    is the constraint describing the equilibrium. For any set of state parameters satisfying this equation, when substituting into $F_{AC} = F_{BC}$ the equality must hold quite independently of the state variables for $C$. Moving along the level surface described by this constraint, $F_{AC} = F_{BC}$ will remain valid irrespective of the state of $C$. This implies that we can cancel the $C_2,C_3,...$ state variables in the equation $F_{AC} = F_{BC}$ on the manifold described by $f_{AB} = 0$. Hence, we have after cancelling the $C_2,C_3,...$'s out of the equation, \begin{equation*}
        \Theta_A(A_1,A_2,...) = \Theta_B(B_1,B_2,...)
    \end{equation*}
    and this characterizes the equilibrium previously characterized by $f_{AB} = 0$. Thus, we have the existence of a universal property of systems in thermal equilibrium, namely the existence of a state function $\Theta$ that takes on identical values if systems are in thermal equilibrium, and this function specifies the \Emph{equation of state}. Hence, it fulfils the basic property of what one would naturally consider a temperature. We define $\Theta$ as the \Emph{empirical temperature}, which is scalar and not unique - we have many possible choices for $\Theta$ at his point. For a given fixed temperature $\Theta$, all allowed thermodynamic states of a system, say $A$, can be obtained by $\Theta_A(A_1,A_2,...) = \Theta$. These states are called the \Emph{isotherms}.
\end{proof}

We note that due to this, the zeroth law constrains the form of the constraint equation describing the equilibrium between two bodies such that it can be organized into an equality of two empirical temperature functions. 

\begin{rmk}
    \Emph{Isotherms} can be considered as manifolds, or level-surfaces, of the empirical temperature function in the space of state variables for a system.
\end{rmk}

\begin{defn}
    A \Emph{thermometer} is a system with some convenient macroscopic property that changes in a simple way as the equilibriumm macrostate changes.
\end{defn}


%%%%%%%%%%%%%%%%%%%% Section 1.2.2
\section{Heat Capacities}

We move to the question of how much heat (energy added through a spontaneous not-forced process) is needed to raise the temperature of a system by a given amount. Heat is often an inexact differential, $\delta Q$, (which is to say it is not-path independent), and hence not a function of the state variables (i.e. not a \Emph{state function}). Consequently, one needs to specify the path by which heat is supplied:

\begin{defn}
    The \Emph{heat capacities} are defined as $C_{\vec{X}} = \left(\frac{\partial Q}{dt}\right)_{\vec{X}}$, where $\vec{X}$ is the set of thermodynamic coordinates which are held constant during the heat supply $\delta Q$. The \Emph{specific heats} are defined as $c_{\vec{X}} = \left(\frac{\delta Q}{MdT}\right)_{\vec{X}}$ where $M$ is the mass of the system.
\end{defn}

Consequently, the heat capacity of a system is how much heat energy is needed to raise its temperature by one degree. The specific heat is the heat capacity per unit mass. 


\begin{eg}
    Examples of common specific heats are as follows: \begin{itemize}
        \item Water: $c_P \approx 4180\;J/(kg\;K)$ and $c_V \approx c_P$ at $T = 25^{\circ}C$, $p = 100\;kPa$, and $N$ fixed.
        \item Air: $c_P \approx 1010\; J/(kg\;K)$ and $c_V \approx 720\;J/(kg\;K)$ at $T = 25^{\circ}C$, $p = 100\;kPa$, and $N$ fixed.
    \end{itemize}
\end{eg}

\subsection{Closed Systems (Heat Capacities)}

Assume that the internal energy of a system is given by $U = U(T,q_1,...,q_m)$, which is to say the state of the system can be uniquely determined by the temperature and $\vec{q}$. Starting from the 1st law we have that \begin{equation*}
    \delta Q = dU - \delta W = dU - \sum_{i=1}^mJ_idq_i = \frac{\partial U}{\partial T}dT + \sum_{i=1}^m\left[\frac{\partial U}{\partial q_i} - J_i\right]dq_i
\end{equation*}
We consider as a first special case the absence of any external work, so $\delta W = 0$ and equivalently $\vec{q}$ is constant (for example, the volume for a gas being constant), so we have that \begin{equation*}
    C_{\vec{q}} = \left(\frac{\delta Q}{dT}\right)_{\vec{q}} = \left(\frac{\partial U}{\partial T}\right)_{\vec{q}}
\end{equation*}
As a second special case let us consider when all macroscopic generalized forces are constant, so $\vec{J}$ is constant (for example, the pressure for a gas is constant). We use the state functions $J_i=J_i(q_1,...,q_m,T)$, for $1 \leq i \leq m$, and we invert them to obtain $q_i = q_i(J_1,...,J_m,T)$ for $1 \leq i \leq m$ and make a change of variables: \begin{equation*}
    dq_i = \sum_{j=1}^m\frac{\partial q_i}{\partial J_j}dJ_j + \frac{\partial q_i}{\partial T}dT = \frac{\partial q_i}{\partial T}dT
\end{equation*}
using the fact that $\vec{J}$ is constant. Thus, for this special case of constant generalized force we have the heat capacity: \begin{align*}
    C_{\vec{J}} &= \left(\frac{\delta Q}{dT}\right)_{\vec{J}} \\
    &= \left(\frac{\partial U}{\partial T}\right)_{\vec{q}} + \sum_{i=1}^m\left[\frac{\partial U}{\partial q_i} - J_i\right]\left(\frac{dq_i}{dT}\right)_{\vec{J}} \\
    &= \left(\frac{\partial U}{\partial T}\right)_{\vec{q}} + \sum_{i=1}^m\left[\frac{\partial U}{\partial q_i} - J_i\right]\left(\frac{\partial q_i}{\partial T}\right)_{\vec{J}} 
\end{align*}
using the expressions for $\delta Q$ and $dq_i$ from above.

\begin{eg}[Gas]
    For a simple dilute gas, we have $m = 1$ with state variable $q = V$ and generalized force $J = -p$. Hence: \begin{equation*}
        C_V = \left(\frac{\partial U}{\partial T}\right)_V
    \end{equation*}
    and \begin{equation*}
        C_p = \left(\frac{\partial U}{\partial T}\right)_V + \left[\left(\frac{\partial U}{\partial V}\right)_{T} + p\right]\left(\frac{\partial V}{\partial T}\right)_p
    \end{equation*}
    Thus, we have \begin{equation*}
        C_p - C_V = \left[\left(\frac{\partial U}{\partial V}\right)_{T} + p\right]\left(\frac{\partial V}{\partial T}\right)_p
    \end{equation*}
    For an ideal gas, we can simplify further by using the equation of state $pV = Nk_BT$. Specifically, we have $\frac{\partial V}{\partial T} = \frac{Nk_B}{p}$. From experiments, we also have $U = U(T)$. Thus, $C_p - C_v = Nk_B$.
\end{eg}

\begin{rmk}
    Most real-world objects prefer to expand when heated, keeping their pressure constants (this effect is less noticable in solids and liquids, and quite important in gases).
\end{rmk}


\begin{eg}[Magnet]
    For a magnet we have $m = 1$ generalized coordinate, the magnetic moment $m$, and we have the generalized force $J = B_0 = \mu_0H$. Hence, \begin{equation*}
        C_m = \left(\frac{\partial U}{\partial T}\right)_m
    \end{equation*}
    and \begin{equation*}
        C_H = C_m + \left[\left(\frac{\partial U}{\partial m}\right)_T - \mu_0H\right]\left(\frac{\partial m}{\partial T}\right)_H
    \end{equation*}
\end{eg}



%%%%%%%%%%%%%%%%%%%% Section 1.2.3
\section{Adiabats and Isotherms}

\subsection{Fire Syringe}

A syringe with air and cotton wool at the bottom is the setup for the experiment. When the Piston is pushed down quickly, the compression of the air on top of the wool ignites it. Compressing the air forces the particles to collide closer together, causing the kinetic energy of the particles to quickly increase. 

From the quick compression we reach the auto-ignition temperature for cotton, which is in the range of $400^{\circ}C$. If the compression is done too slowly, there is time for heat to escape into the air so there won't be enough to ignite the cotton. The compression in which no/minimal heat loss occurs due to how rapid the compression is is called an \Emph{adiabatic compression}. These observations lead to the following definition 

\begin{defn}
    An \Emph{adiabat} is a change in the state of a system such that $\delta Q = 0$. Hence, the $1$st law simplifies to $dU = \delta W$ for adiabats.
\end{defn}

\subsection{Closed Systems (Adiabats)}


We now consider the properties of adiabats for closed systems. In the case that $U = U(T,q_1,...,q_m)$, we can use the formulation of the first law $dU = \delta W + \delta Q$ and apply $\delta Q = 0$ to obtain \begin{equation*}
    \frac{\partial U}{\partial T}(dT)_{ad} = \sum_{i=1}^m\left[J_i - \frac{\partial U}{\partial q_i}\right](dq_i)_{ad}
\end{equation*}
where the subscript $ad$ indicates that this expression is only valid for adiabats. 

\begin{eg}[Gas]
    For a simple gas, we have $m = 1$ with generalized coordinate $q = V$ and generalized force $J = -p$. Hence: \begin{equation*}
        \frac{\partial U}{\partial T}(dT)_{ad} = \left[-p - \frac{\partial U}{\partial V}\right](dV)_{ad}
    \end{equation*}
    This implies that \begin{equation*}
        \left(\frac{dT}{dV}\right)_{ad} = -\frac{p+\frac{\partial U}{\partial V}}{C_V}
    \end{equation*}
    recalling that \begin{equation*}
        C_V = \frac{\partial U}{\partial T}
    \end{equation*}
    so this expression represents the change in temperature during an adiabatic process with respect to the change in volume. For an ideal gas, which has $U = U(t)$, this expression simplifies further to \begin{equation*}
        \left(\frac{dT}{dV}\right)_{ad} = -\frac{p}{C_V} = -\frac{Nk_BT}{C_VV} = -\frac{(C_p-C_V)}{C_V}\frac{T}{V} = -(\gamma - 1)\frac{T}{V}
    \end{equation*}
    where we define $\gamma := \frac{C_p}{C_V} \geq 1$, and where we have used the previously derived equality $C_p - C_V = Nk_B$ for an ideal gas. Under the assumption that $\gamma$ is constant, this ordinary differential equation can be easily solved to obtain \begin{equation*}
        TV^{\gamma-1} = const.
    \end{equation*}
    Using the equation of state for an ideal gas one can get the following two equivalent formulations: \begin{align*}
        pV^{\gamma} &= const. \\
        T^{\gamma}p^{1-\gamma} &= const.
    \end{align*}
    We can then draw the following graphs:
    \begin{figure}[H]
        \centering
        \includegraphics[scale = 0.4]{Images/AdiabaticIdealGas.png}
        \caption{Relationships for temperature, pressure, and volume for an adiabatic process involving an ideal gas.}
    \end{figure}
\end{eg}

Some specific applications of adiabatic processes are as follows: \begin{itemize}
    \item Diesel engine (spontaneous explosion due to heating; no spark plugs)
    \item Chinooks (air pressure increases due to lower altitude)
\end{itemize}



\begin{eg}[Black-body radiation]
    We now deal with electromagnetic radiation in a cavity in thermal equilibrium, which is emitted by black walls. Here ``black" means all radiation is absorbed:
    \begin{center}
        \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Rectangle [id:dp6362389483265465] 
\draw  [fill={rgb, 255:red, 253; green, 0; blue, 0 }  ,fill opacity=1 ][line width=2.25]  (100,126) -- (460,126) -- (460,306) -- (100,306) -- cycle ;
%Shape: Rectangle [id:dp03100105207175874] 
\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ][line width=2.25]  (190,171) -- (370,171) -- (370,261) -- (190,261) -- cycle ;

% Text Node
\draw (345,183.07) node [anchor=north west][inner sep=0.75pt]    {$V$};
% Text Node
\draw (443,280.07) node [anchor=north west][inner sep=0.75pt]    {$T$};
% Text Node
\draw (193,242.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {Cavity};
% Text Node
\draw (102,286.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {Heat Bath};


\end{tikzpicture}
    \end{center}
    Experiments show that $U = U(T,V) = V\epsilon(T)$, where $\epsilon$ is the energy density per volume. From classical electromagnetism, we also have $p = \frac{1}{3}\epsilon(T)$ for isotropic radiation (uniform radiation). The existence of a pressure from the classical perspective is similar to what we have for a gas. We can also borrow the wave-particle dualism from Quantum Mechanics. This suggests that we might be able to interpret or model black-body radiation as a \Emph{photon gas} (at least in a first approximation). Using our previous derivation for a gas we have \begin{equation*}
        C_V = \frac{\partial U}{\partial T} = V\frac{d\epsilon}{dT}
    \end{equation*}
    and for adiabats we find then \begin{equation*}
        \left(\frac{dT}{dV}\right)_{ad} = -\frac{p + \frac{\partial U}{\partial V}}{C_V} = -\frac{\frac{4}{3}\epsilon(T)}{V\frac{d\epsilon}{dT}}
    \end{equation*}
    We can rewrite this to obtain the simple ODE \begin{equation*}
        -\frac{d\epsilon}{\epsilon} = \frac{4}{3}\frac{dV}{V}
    \end{equation*}
    The solution of this ODE can be written as \begin{equation*}
        \epsilon V^{4/3} = const.
    \end{equation*}
    which has the equivalent form \begin{equation*}
        pV^{4/3} = const.
    \end{equation*}
    given the relationship between $p$ and $\epsilon$ for black-body radiation from above.
\end{eg}


Since no heat is flowing in or out of the system in an adiabatic process, in the absence of any friction we can completely recover the original state of the system by reversing the procedure. Such a process also converses entropy, as we will see later. 

\begin{defn}
    A thermodynamic process which is both adiabatic and reversible is said to be an \Emph{isentropic process}.
\end{defn}




\subsection{Isotherms}

\begin{defn}
    An \Emph{isotherm} is a change in the state of a system such that $dT = 0$.
\end{defn}

This is not to be confused with an adiabatic process in which there is no heat trnasfer between the surroundings and the system. $dT = 0$ relates to a level surface, or manifold, associated to a constant empirical temperature $\Theta$, which we use to define $T$.


%%%%%%%%%%%%%%%%%%%%%% Chapter 1.3
\chapter{Thermodynamical Engines and Refrigerators}


%%%%%%%%%%%%%%%%%%%% Section 1.3.1
\section{The 2nd Law of Thermodynamics}

Recall from the first law we know that energy is conserved during all thermodynamic processes. This leaves the question as to whether all thermodynamic processes that conserve energy can actually occur. To answer this we focus on the conversion of heat to work in terms of engines:

\begin{defn}
    An idealized \Emph{heat engine} is a thermodynamic system that runs in a \Emph{cycle} (i.e. it returns to the same internal thermodynamic states) taking in a certain amount of heat $Q_h$ from a heat bath with temperature $T_h$, converting a portion of it to work $W > 0$ and dumping the remaining heat $Q_c$ into a heat bath with $T_c < T_h$. The \Emph{efficiency} of a heat engine is given by \begin{equation}
        \eta = \frac{W}{Q_h} = \frac{Q_h-Q_c}{Q_h} = 1 - \frac{Q_c}{Q_h} \leq 1
    \end{equation}
\end{defn}

\begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,288); %set diagram left start at 0, and has height of 288

%Shape: Circle [id:dp4831299922922003] 
\draw   (210,135) .. controls (210,110.15) and (230.15,90) .. (255,90) .. controls (279.85,90) and (300,110.15) .. (300,135) .. controls (300,159.85) and (279.85,180) .. (255,180) .. controls (230.15,180) and (210,159.85) .. (210,135) -- cycle ;
%Straight Lines [id:da9977923048639901] 
\draw    (255,50) -- (255,88) ;
\draw [shift={(255,90)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da745735751950716] 
\draw    (255,180) -- (255,218) ;
\draw [shift={(255,220)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4282340901069286] 
\draw    (300,135) -- (368,135) ;
\draw [shift={(370,135)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp12016935374222504] 
\draw  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ] (110,18) .. controls (110,13.58) and (113.58,10) .. (118,10) -- (392,10) .. controls (396.42,10) and (400,13.58) .. (400,18) -- (400,42) .. controls (400,46.42) and (396.42,50) .. (392,50) -- (118,50) .. controls (113.58,50) and (110,46.42) .. (110,42) -- cycle ;
%Rounded Rect [id:dp5253667581959272] 
\draw  [color={rgb, 255:red, 0; green, 139; blue, 255 }  ,draw opacity=1 ] (110,228) .. controls (110,223.58) and (113.58,220) .. (118,220) -- (392,220) .. controls (396.42,220) and (400,223.58) .. (400,228) -- (400,252) .. controls (400,256.42) and (396.42,260) .. (392,260) -- (118,260) .. controls (113.58,260) and (110,256.42) .. (110,252) -- cycle ;

% Text Node
\draw (380,12.4) node [anchor=north west][inner sep=0.75pt]    {$T_{h}$};
% Text Node
\draw (241,112.4) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$E$};
% Text Node
\draw (257,62.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$Q_{h}$};
% Text Node
\draw (258,191.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 135; blue, 255 }  ,opacity=1 ]  {$Q_{c}$};
% Text Node
\draw (331,113.4) node [anchor=north west][inner sep=0.75pt]    {$W$};
% Text Node
\draw (187,22) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont {\large Heat Bath (source) }}};
% Text Node
\draw (380,222.4) node [anchor=north west][inner sep=0.75pt]    {$T_{c}$};
% Text Node
\draw (194,232) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont {\large Heat Bath (sink) }}};


\end{tikzpicture}
\end{center}

\begin{defn}
    An idealized \Emph{refrigerator} can be considered as an idealized heat engine running backwards, such that it uses work $W$ to extract heat $Q_c$ (e.g. from an ice box) and dump the heat $Q_h$ at higher $T_h$ (e.g. through an exhaust). The \Emph{performance} of a refrigerator is given by \begin{equation}
        w = \frac{Q_c}{W} = \frac{Q_c}{Q_h-Q_c}
    \end{equation}
\end{defn}

\begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,288); %set diagram left start at 0, and has height of 288

%Shape: Circle [id:dp4831299922922003] 
\draw   (210,135) .. controls (210,110.15) and (230.15,90) .. (255,90) .. controls (279.85,90) and (300,110.15) .. (300,135) .. controls (300,159.85) and (279.85,180) .. (255,180) .. controls (230.15,180) and (210,159.85) .. (210,135) -- cycle ;
%Straight Lines [id:da9977923048639901] 
\draw    (255,52) -- (255,90) ;
\draw [shift={(255,50)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da745735751950716] 
\draw    (255,182) -- (255,220) ;
\draw [shift={(255,180)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4282340901069286] 
\draw    (140,135) -- (208,135) ;
\draw [shift={(210,135)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp12016935374222504] 
\draw  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ] (110,18) .. controls (110,13.58) and (113.58,10) .. (118,10) -- (392,10) .. controls (396.42,10) and (400,13.58) .. (400,18) -- (400,42) .. controls (400,46.42) and (396.42,50) .. (392,50) -- (118,50) .. controls (113.58,50) and (110,46.42) .. (110,42) -- cycle ;
%Rounded Rect [id:dp5253667581959272] 
\draw  [color={rgb, 255:red, 0; green, 139; blue, 255 }  ,draw opacity=1 ] (110,228) .. controls (110,223.58) and (113.58,220) .. (118,220) -- (392,220) .. controls (396.42,220) and (400,223.58) .. (400,228) -- (400,252) .. controls (400,256.42) and (396.42,260) .. (392,260) -- (118,260) .. controls (113.58,260) and (110,256.42) .. (110,252) -- cycle ;
%Straight Lines [id:da01660900085043382] 
\draw    (140,130) -- (140,140) ;

% Text Node
\draw (380,12.4) node [anchor=north west][inner sep=0.75pt]    {$T_{h}$};
% Text Node
\draw (241,112.4) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$R$};
% Text Node
\draw (257,62.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$Q_{h}$};
% Text Node
\draw (258,191.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 135; blue, 255 }  ,opacity=1 ]  {$Q_{c}$};
% Text Node
\draw (152,113.4) node [anchor=north west][inner sep=0.75pt]    {$W$};
% Text Node
\draw (187,22) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont {\large Heat Bath (exhaust) }}};
% Text Node
\draw (380,222.4) node [anchor=north west][inner sep=0.75pt]    {$T_{c}$};
% Text Node
\draw (164,230) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont {\large Heat Bath (cold resevoir) }}};


\end{tikzpicture}
\end{center}

We will now give two formulations of the second law of thermodynamics:

\begin{law}[2nd Law of Thermodynamics (Kelvin)]
    A thermodynamic machine or process that runs in a cycle and whose sole result is the complete conversion of heat from a single heat bath or reservoir into work is not possible.
\end{law}

\begin{law}[2nd Law of Thermodynamics (Clausius)]
    There is no thermodynamics machine or process that runs in a cycle and whose sole result is the transfer of heat from a colder to a hotter body.
\end{law}

These two formulations are in fact equivalent, as we shall now prove:

\begin{proof}
    To prove their equivalence, we shall show ``not Kelvin" implies ``not Clausius" and vice-versa. First, suppose that there exists a thermodynamic machine that runs in a cycle and whose sole result is the complete conversion of heat from a single heat bath or reservoir into work. Let $T_h$ correspond to the reservoir of interest. Then, consider a refrigerator from a reservoir $T_c < T_h$ to $T_h$, and have the work come from our engine. Then, after the system as balanced, looking at the engine and refrigerator as a single machine we have that heat $Q_c$ comes from the cold reservoir, and heat $Q_h - Q_h'$ is deposited in the hot reservoir, without any external work. Thus, such a machine violates Clausius, as desired, and as shown in the following diagram:
        \begin{center}
            \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,288); %set diagram left start at 0, and has height of 288

%Shape: Circle [id:dp4831299922922003] 
\draw   (300.67,134.33) .. controls (300.67,109.48) and (320.81,89.33) .. (345.67,89.33) .. controls (370.52,89.33) and (390.67,109.48) .. (390.67,134.33) .. controls (390.67,159.19) and (370.52,179.33) .. (345.67,179.33) .. controls (320.81,179.33) and (300.67,159.19) .. (300.67,134.33) -- cycle ;
%Straight Lines [id:da9977923048639901] 
\draw    (345.67,51.33) -- (345.67,89.33) ;
\draw [shift={(345.67,49.33)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da745735751950716] 
\draw    (345.67,181.33) -- (345.67,219.33) ;
\draw [shift={(345.67,179.33)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4282340901069286] 
\draw    (230.67,134.33) -- (298.67,134.33) ;
\draw [shift={(300.67,134.33)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp12016935374222504] 
\draw  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ] (110,18) .. controls (110,13.58) and (113.58,10) .. (118,10) -- (392,10) .. controls (396.42,10) and (400,13.58) .. (400,18) -- (400,42) .. controls (400,46.42) and (396.42,50) .. (392,50) -- (118,50) .. controls (113.58,50) and (110,46.42) .. (110,42) -- cycle ;
%Rounded Rect [id:dp5253667581959272] 
\draw  [color={rgb, 255:red, 0; green, 139; blue, 255 }  ,draw opacity=1 ] (110,228) .. controls (110,223.58) and (113.58,220) .. (118,220) -- (392,220) .. controls (396.42,220) and (400,223.58) .. (400,228) -- (400,252) .. controls (400,256.42) and (396.42,260) .. (392,260) -- (118,260) .. controls (113.58,260) and (110,256.42) .. (110,252) -- cycle ;
%Straight Lines [id:da01660900085043382] 
\draw    (230.67,129.33) -- (230.67,139.33) ;
%Shape: Circle [id:dp9583179343344637] 
\draw   (140.67,134.33) .. controls (140.67,109.48) and (160.81,89.33) .. (185.67,89.33) .. controls (210.52,89.33) and (230.67,109.48) .. (230.67,134.33) .. controls (230.67,159.19) and (210.52,179.33) .. (185.67,179.33) .. controls (160.81,179.33) and (140.67,159.19) .. (140.67,134.33) -- cycle ;
%Straight Lines [id:da5396194960167366] 
\draw    (185,50) -- (185,88) ;
\draw [shift={(185,90)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp6223483168411648] 
\draw  [color={rgb, 255:red, 85; green, 199; blue, 11 }  ,draw opacity=1 ] (110,102) .. controls (110,89.85) and (119.85,80) .. (132,80) -- (398,80) .. controls (410.15,80) and (420,89.85) .. (420,102) -- (420,168) .. controls (420,180.15) and (410.15,190) .. (398,190) -- (132,190) .. controls (119.85,190) and (110,180.15) .. (110,168) -- cycle ;

% Text Node
\draw (380,12.4) node [anchor=north west][inner sep=0.75pt]    {$T_{h}$};
% Text Node
\draw (331.67,111.73) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$R$};
% Text Node
\draw (347.67,61.73) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$Q_{h}$};
% Text Node
\draw (348.67,190.73) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 135; blue, 255 }  ,opacity=1 ]  {$Q_{c}$};
% Text Node
\draw (261,142.4) node [anchor=north west][inner sep=0.75pt]    {$W$};
% Text Node
\draw (181,22) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont {\large Heat Bath (hot resevoir) }}};
% Text Node
\draw (380,222.4) node [anchor=north west][inner sep=0.75pt]    {$T_{c}$};
% Text Node
\draw (164,230) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont {\large Heat Bath (cold resevoir) }}};
% Text Node
\draw (187,62.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$Q'_{h}$};
% Text Node
\draw (171,103.4) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$\overline{K}$};
% Text Node
\draw (250,83.4) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$\overline{C}$};


\end{tikzpicture}
        \end{center}
        Next, suppose there exists a thermodynamic machine that runs in a cycle and whose sole result is the transfer of heat, $Q$, from a colder to a hotter body. Then, connect an engine that takes heat $Q_h$ from the hotter body, does work $W$, and deposits heat $Q_c$ to the colder body. It follows that the combined machine has input $Q_h - Q$ from the hot body, work output $W$, and heat output $Q_c - Q$. We then adjust the output of the heat engine such that $Q_c = Q$. Then we have obtained a machine violating Kelvin, as seen in the diagram:

        \begin{center}
            \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,288); %set diagram left start at 0, and has height of 288

%Shape: Circle [id:dp4831299922922003] 
\draw   (300.67,134.33) .. controls (300.67,109.48) and (320.81,89.33) .. (345.67,89.33) .. controls (370.52,89.33) and (390.67,109.48) .. (390.67,134.33) .. controls (390.67,159.19) and (370.52,179.33) .. (345.67,179.33) .. controls (320.81,179.33) and (300.67,159.19) .. (300.67,134.33) -- cycle ;
%Straight Lines [id:da9977923048639901] 
\draw    (345.67,49.33) -- (345.67,87.33) ;
\draw [shift={(345.67,89.33)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da745735751950716] 
\draw    (185,182) -- (185,220) ;
\draw [shift={(185,180)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4282340901069286] 
\draw    (390.67,134.33) -- (458.67,134.33) ;
\draw [shift={(460.67,134.33)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp12016935374222504] 
\draw  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ] (110,18) .. controls (110,13.58) and (113.58,10) .. (118,10) -- (392,10) .. controls (396.42,10) and (400,13.58) .. (400,18) -- (400,42) .. controls (400,46.42) and (396.42,50) .. (392,50) -- (118,50) .. controls (113.58,50) and (110,46.42) .. (110,42) -- cycle ;
%Rounded Rect [id:dp5253667581959272] 
\draw  [color={rgb, 255:red, 0; green, 139; blue, 255 }  ,draw opacity=1 ] (110,228) .. controls (110,223.58) and (113.58,220) .. (118,220) -- (392,220) .. controls (396.42,220) and (400,223.58) .. (400,228) -- (400,252) .. controls (400,256.42) and (396.42,260) .. (392,260) -- (118,260) .. controls (113.58,260) and (110,256.42) .. (110,252) -- cycle ;
%Straight Lines [id:da01660900085043382] 
\draw    (230.67,129.33) -- (230.67,139.33) ;
%Shape: Circle [id:dp9583179343344637] 
\draw   (140.67,134.33) .. controls (140.67,109.48) and (160.81,89.33) .. (185.67,89.33) .. controls (210.52,89.33) and (230.67,109.48) .. (230.67,134.33) .. controls (230.67,159.19) and (210.52,179.33) .. (185.67,179.33) .. controls (160.81,179.33) and (140.67,159.19) .. (140.67,134.33) -- cycle ;
%Straight Lines [id:da5396194960167366] 
\draw    (185,52) -- (185,90) ;
\draw [shift={(185,50)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp6223483168411648] 
\draw  [color={rgb, 255:red, 85; green, 199; blue, 11 }  ,draw opacity=1 ] (110,102) .. controls (110,89.85) and (119.85,80) .. (132,80) -- (398,80) .. controls (410.15,80) and (420,89.85) .. (420,102) -- (420,168) .. controls (420,180.15) and (410.15,190) .. (398,190) -- (132,190) .. controls (119.85,190) and (110,180.15) .. (110,168) -- cycle ;
%Straight Lines [id:da8097853550719598] 
\draw    (345.67,179.33) -- (345.67,217.33) ;
\draw [shift={(345.67,219.33)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

% Text Node
\draw (380,12.4) node [anchor=north west][inner sep=0.75pt]    {$T_{h}$};
% Text Node
\draw (331.67,111.73) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$E$};
% Text Node
\draw (347.67,61.73) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$Q_{h}$};
% Text Node
\draw (188,191.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 135; blue, 255 }  ,opacity=1 ]  {$Q$};
% Text Node
\draw (401,113.4) node [anchor=north west][inner sep=0.75pt]    {$W$};
% Text Node
\draw (181,22) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont {\large Heat Bath (hot resevoir) }}};
% Text Node
\draw (380,222.4) node [anchor=north west][inner sep=0.75pt]    {$T_{c}$};
% Text Node
\draw (164,230) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont {\large Heat Bath (cold resevoir) }}};
% Text Node
\draw (187,62.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ]  {$Q$};
% Text Node
\draw (171,103.4) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$\overline{C}$};
% Text Node
\draw (250,83.4) node [anchor=north west][inner sep=0.75pt]  [font=\huge]  {$\overline{K}$};
% Text Node
\draw (351,192.4) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 135; blue, 255 }  ,opacity=1 ]  {$Q$};


\end{tikzpicture}
        \end{center}
\end{proof}




%%%%%%%%%%%%%%%%%%%% Section 1.3.2
\section{Carnot Engines}

We now investigate the idea of the most efficient heat engine. 

\begin{defn}
    A \Emph{Carnot engine} is any heat engine that is reversible and runs in a cycle along two isotherms at temperatures $T_h$ and $T_c$ and adiabats. A \Emph{reversible} process is one that can be run backward in time by simply reversing its inputs and outputs. 
\end{defn}

\begin{eg}[Carnot Engine with an Ideal Gas as the Internal Working Substance]
    Here we have adiabatic compression of the gas, causing $\Delta T = T_h - T_c > 0$ ($A\rightarrow B$), then we have isothermal expansion with $\Delta Q_1 = Q_h > 0$ ($B\rightarrow C$), next we have adiabatic expansion which causes $\Delta T = T_c - T_h < 0$ ($C\rightarrow D$), and finally isothermal compression with $\Delta Q_2 = -Q_c < 0$ ($D\rightarrow A$).
    \begin{center}
        \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,288); %set diagram left start at 0, and has height of 288

%Shape: Right Angle [id:dp843558691918219] 
\draw   (370,220) -- (160,220) -- (160,40) ;
\draw   (156.71,47) -- (160.19,40.22) -- (163.67,47) ;
\draw   (363.3,216.63) -- (370.08,220.11) -- (363.3,223.59) ;
%Curve Lines [id:da8766430669684191] 
\draw    (209.33,109.33) .. controls (219.83,118.08) and (254.83,150.08) .. (309.83,150.08) ;
\draw [shift={(256.04,139.25)}, rotate = 203.76] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da47282664716897016] 
\draw    (226.33,175.08) .. controls (236.83,183.83) and (275.83,199.58) .. (330.83,199.58) ;
\draw [shift={(276.99,194.27)}, rotate = 12.33] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da19308737717293245] 
\draw  [dash pattern={on 4.5pt off 4.5pt}]  (206.83,79.75) .. controls (206.83,111.75) and (219.33,165.92) .. (240.83,213.92) ;
\draw [shift={(217.79,148.67)}, rotate = 75.43] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da4455351667362961] 
\draw  [dash pattern={on 4.5pt off 4.5pt}]  (302.83,108.75) .. controls (302.83,140.75) and (316.33,166.92) .. (337.83,214.92) ;
\draw [shift={(315.18,163.42)}, rotate = 248.82999999999998] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da7064223462049448] 
\draw    (245.6,121.88) -- (237.34,134.97)(243.07,120.28) -- (234.8,133.36) ;
\draw [shift={(232.33,140.08)}, rotate = 302.28] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (8.74,-3.92) .. controls (5.56,-1.84) and (2.65,-0.53) .. (0,0) .. controls (2.65,0.53) and (5.56,1.84) .. (8.74,3.92)   ;
%Straight Lines [id:da05642770961721699] 
\draw [line width=0.75]    (273.1,185.88) -- (264.84,198.97)(270.57,184.28) -- (262.3,197.36) ;
\draw [shift={(259.83,204.08)}, rotate = 302.28] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (8.74,-3.92) .. controls (5.56,-1.84) and (2.65,-0.53) .. (0,0) .. controls (2.65,0.53) and (5.56,1.84) .. (8.74,3.92)   ;
%Curve Lines [id:da10018836511051932] 
\draw  [dash pattern={on 4.5pt off 4.5pt}]  (189.83,73.75) .. controls (204.33,135.75) and (298.33,161.25) .. (345.33,146.75) ;
%Curve Lines [id:da2569141398168828] 
\draw  [dash pattern={on 4.5pt off 4.5pt}]  (192.83,146.75) .. controls (233.33,200.25) and (319.33,205.75) .. (385.33,196.25) ;

% Text Node
\draw (145.33,35.73) node [anchor=north west][inner sep=0.75pt]    {$P$};
% Text Node
\draw (373.33,215.23) node [anchor=north west][inner sep=0.75pt]    {$V$};
% Text Node
\draw (246.83,112.73) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$Q_{h}$};
% Text Node
\draw (274.33,177.73) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$Q_{c}$};
% Text Node
\draw (214.83,175.07) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$A$};
% Text Node
\draw (198.33,108.07) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$B$};
% Text Node
\draw (310.83,135.57) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$C$};
% Text Node
\draw (340.33,203.57) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$D$};
% Text Node
\draw (184.83,65.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q=0$};
% Text Node
\draw (278.83,93.9) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q=0$};
% Text Node
\draw (344.83,139.9) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$T_{h}$};
% Text Node
\draw (385.83,188.9) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$T_{c}$};

\draw   (209.36, 109.08) circle [x radius= 5, y radius= 5]   ;
\draw   (310.43, 150.62) circle [x radius= 5, y radius= 5]   ;
\draw   (238.15, 130.88) circle [x radius= 5, y radius= 5]   ;
\end{tikzpicture}
    \end{center}
    Over one cycle, the 1st law gives us \begin{equation*}
        0 = \oint dU = \Delta Q_1 + \Delta Q_2 + \Delta W
    \end{equation*}
    We calculate $\Delta W$ by considering the individual contributions: \begin{itemize}
        \item $A\rightarrow B$: adiabatic process, so $\delta Q = 0$, and $\delta W = dU$. Hence, $\Delta W_{AB} = U(T_h) - U(T_c)$, where we have used that $U = U(T)$ for an ideal gas.
        \item $B\rightarrow C$: isomethermal process, so $dT = 0$. Hence, $p = p(V)$ for an ideal gas in a closed system setting ($pV = Nk_BT$) as temperature is a constant, and so $$\Delta W_{BC} = -\int_B^Cp(V)dV = -Nk_BT_h\int_{V_B}^{V_C}\frac{dV}{V} = -Nk_BT_h\ln\frac{V_C}{V_B}$$
        \item $C\rightarrow D$: another adiabatic process, so $\Delta W_{CD} = U(T_c) - U(T_h)$ as in $A\rightarrow B$
        \item $D\rightarrow A$: another isothermal process, so $\Delta W_{DA} = -Nk_BT_c\ln\frac{V_A}{V_D}$ as in $B\rightarrow C$
    \end{itemize}
    This gives the total work: \begin{equation*}
        \Delta W = \Delta W_{AB} + \Delta W_{BC} + \Delta W_{CD} + \Delta W_{DA} = -Nk_B\left(T_h\ln\frac{V_C}{V_B} +T_c\ln\frac{V_A}{V_D}\right)
    \end{equation*}
    Recall that since we are dealing with adiabats, we have $TV^{\gamma -1} = const$, where the constant is the same along a given adiabat. This implies that $T_hV_B^{\gamma - 1} = T_cV_A^{\gamma -1}$ and $T_hV_C^{\gamma - 1} = T_cV_D^{\gamma - 1}$. Dividing the first equation by the second and simplifying slightly gives $\frac{V_B}{V_C} = \frac{V_A}{V_D}$. Using this in the above equation we find \begin{equation*}
        \Delta W = -Nk_B\ln\frac{V_D}{V_A}(T_h - T_c) < 0
    \end{equation*}
    Now, by definition the efficiency of a Carnot engine for an ideal gas is \begin{equation*}
        \eta_{C,ideal\;gas} = \frac{W}{Q_h} = \frac{-\Delta W}{\Delta Q_1}
    \end{equation*}
    Now, since $U = U(T)$ for an ideal gas in a closed system, we have $\Delta U = 0$ for any isothermal process. Thus, during the isothermal expansion the 1st law gives us \begin{equation*}
        \Delta Q_1 = -\Delta W_{BC} = Nk_BT_h\ln\frac{V_D}{V_A} > 0
    \end{equation*}
    using the identity between volume ratios $\frac{V_B}{V_C} = \frac{V_A}{V_D}$. Using the expressions for $\Delta W$ and $\Delta Q_1$, we obtain the efficiency \begin{equation*}
        \eta_{C,ideal\;gas} = \frac{W}{Q_h} = \frac{-\Delta W}{\Delta Q_1} = 1 - \frac{T_c}{T_h}
    \end{equation*}
\end{eg}

\subsection{Carnot's Theorem}

We can now address the question of which heat engine is the most efficient: 

\begin{namthm}[Carnot's Theorem]
    No heat engine operating between two heat reservoirs (at temperatures $T_h$ and $T_c$) is more efficient than a Carnot engine operating between them.
\end{namthm}
\begin{proof}
    We start with any given heat engine and couple it with a Carnot engine, which is run in reverse as a refrigerator and powerd by the other engine:
    \begin{center}
        \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,288); %set diagram left start at 0, and has height of 288

%Rounded Rect [id:dp38561762820116385] 
\draw  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ] (185,20.91) .. controls (185,15.44) and (189.44,11) .. (194.91,11) -- (447.98,11) .. controls (453.45,11) and (457.89,15.44) .. (457.89,20.91) -- (457.89,50.64) .. controls (457.89,56.12) and (453.45,60.56) .. (447.98,60.56) -- (194.91,60.56) .. controls (189.44,60.56) and (185,56.12) .. (185,50.64) -- cycle ;
%Rounded Rect [id:dp11614062251907709] 
\draw  [color={rgb, 255:red, 0; green, 148; blue, 255 }  ,draw opacity=1 ] (185,206.24) .. controls (185,200.77) and (189.44,196.33) .. (194.91,196.33) -- (447.98,196.33) .. controls (453.45,196.33) and (457.89,200.77) .. (457.89,206.24) -- (457.89,235.98) .. controls (457.89,241.45) and (453.45,245.89) .. (447.98,245.89) -- (194.91,245.89) .. controls (189.44,245.89) and (185,241.45) .. (185,235.98) -- cycle ;
%Shape: Circle [id:dp5999093300147897] 
\draw   (228.22,130.5) .. controls (228.22,113.29) and (242.18,99.33) .. (259.39,99.33) .. controls (276.6,99.33) and (290.56,113.29) .. (290.56,130.5) .. controls (290.56,147.71) and (276.6,161.67) .. (259.39,161.67) .. controls (242.18,161.67) and (228.22,147.71) .. (228.22,130.5) -- cycle ;
%Shape: Circle [id:dp007932259288887522] 
\draw   (351.56,131.17) .. controls (351.56,113.95) and (365.51,100) .. (382.72,100) .. controls (399.94,100) and (413.89,113.95) .. (413.89,131.17) .. controls (413.89,148.38) and (399.94,162.33) .. (382.72,162.33) .. controls (365.51,162.33) and (351.56,148.38) .. (351.56,131.17) -- cycle ;
%Straight Lines [id:da7447172424421336] 
\draw    (259.4,60) -- (259.39,97.33) ;
\draw [shift={(259.39,99.33)}, rotate = 270.02] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da21237543507400014] 
\draw    (382.73,62.67) -- (382.72,100) ;
\draw [shift={(382.73,60.67)}, rotate = 90.02] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da523718903355215] 
\draw    (382.72,164.33) -- (382.71,196) ;
\draw [shift={(382.72,162.33)}, rotate = 90.02] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4858891627092614] 
\draw    (259.39,161.67) -- (259.38,193.33) ;
\draw [shift={(259.38,195.33)}, rotate = 270.02] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4063459383288237] 
\draw    (290.56,130.5) -- (349.56,131.14) ;
\draw [shift={(351.56,131.17)}, rotate = 180.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

% Text Node
\draw (435.56,10.73) node [anchor=north west][inner sep=0.75pt]    {$T_{h}$};
% Text Node
\draw (435.56,197.73) node [anchor=north west][inner sep=0.75pt]    {$T_{c}$};
% Text Node
\draw (244.89,121.73) node [anchor=north west][inner sep=0.75pt]    {$\neg C$};
% Text Node
\draw (376.89,121.73) node [anchor=north west][inner sep=0.75pt]    {$C$};
% Text Node
\draw (311,113.4) node [anchor=north west][inner sep=0.75pt]    {$W$};
% Text Node
\draw (261,72.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{h}$};
% Text Node
\draw (387,72.4) node [anchor=north west][inner sep=0.75pt]    {$Q'_{h}$};
% Text Node
\draw (261.39,165.07) node [anchor=north west][inner sep=0.75pt]    {$Q_{c}$};
% Text Node
\draw (387,171.4) node [anchor=north west][inner sep=0.75pt]    {$Q'_{c}$};
% Text Node
\draw (247,32) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,opacity=1 ] [align=left] {{\fontfamily{ptm}\selectfont Heat Bath (hot reservoir)}};
% Text Node
\draw (247,212) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 144; blue, 255 }  ,opacity=1 ] [align=left] {{\fontfamily{ptm}\selectfont Heat Bath (cold reservoir)}};


\end{tikzpicture}
    \end{center}
    Instead of considering both engines in isolation, we can equivalently consider them as a combined engine, taking in heat $Q_h - Q_h'$, and outputing heat $Q_c - Q_c'$. As this combined machine runs in a cycle, so $\Delta U = 0$ over a cycle, and only exchanges heat with the outside world without doing any external work, we have $Q_h + Q_c' = Q_c + Q_h'$ based on the first law of thermodynamics. Moreover, the combined machine falls under the 2nd law of thermodynamics. Using the formulation by Clausius, we have $Q_h - Q_h' \geq 0$, so $Q_h \geq Q_h'$, and hence $\frac{W}{Q_h} \leq \frac{W}{Q_h'}$. This implies that $\eta \leq \eta_C$ (where the last implication uses the fact that Carnot engines are exactly reversible).
\end{proof}

\begin{cor}
    All reversible engines have the same universal efficiency $\eta = \eta_C(T_h,T_c) = 1-\frac{T_c}{T_h}$.
\end{cor}
\begin{proof}
    Each reversible Carnot engine can be used to run any other reversible heat engine backwards.
\end{proof}



%%%%%%%%%%%%%%%%%%%% Section 1.3.3
\section{Thermodynamic Temperature Scale}


Recall the $0$th law ensures the existence of a temperature. For the ideal gas we can use the equation of state to define $T$, but this does not usually genrealize to other systems. We now use the universal efficiency $\eta_C$ of Carnot engines to establish a more general thermodynamic temperature scale. Consider two coupled Carnot engines with some empirical temperature scale $\Theta$: 
\begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,294); %set diagram left start at 0, and has height of 294

%Straight Lines [id:da05453093434369749] 
\draw    (60,12) -- (240,12) ;
%Straight Lines [id:da6200862679386634] 
\draw    (60,144) -- (240,144) ;
%Straight Lines [id:da706189415713308] 
\draw    (60,276) -- (240,276) ;
%Rounded Rect [id:dp4036699475049652] 
\draw  [fill={rgb, 255:red, 136; green, 255; blue, 0 }  ,fill opacity=1 ] (96,57.6) .. controls (96,52.3) and (100.3,48) .. (105.6,48) -- (194.4,48) .. controls (199.7,48) and (204,52.3) .. (204,57.6) -- (204,86.4) .. controls (204,91.7) and (199.7,96) .. (194.4,96) -- (105.6,96) .. controls (100.3,96) and (96,91.7) .. (96,86.4) -- cycle ;
%Rounded Rect [id:dp37939651638989935] 
\draw  [fill={rgb, 255:red, 255; green, 0; blue, 0 }  ,fill opacity=1 ] (96,201.6) .. controls (96,196.3) and (100.3,192) .. (105.6,192) -- (194.4,192) .. controls (199.7,192) and (204,196.3) .. (204,201.6) -- (204,230.4) .. controls (204,235.7) and (199.7,240) .. (194.4,240) -- (105.6,240) .. controls (100.3,240) and (96,235.7) .. (96,230.4) -- cycle ;
%Straight Lines [id:da7142280284189315] 
\draw    (150,12) -- (150,46) ;
\draw [shift={(150,48)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da0985311285686783] 
\draw    (150,96) -- (150,142) ;
\draw [shift={(150,144)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da5535079418179869] 
\draw    (150,144) -- (150,190) ;
\draw [shift={(150,192)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da3167310910998773] 
\draw    (150,240) -- (150,274) ;
\draw [shift={(150,276)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da576279060400507] 
\draw    (204,72) -- (250,72) ;
\draw [shift={(252,72)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da8007473356697774] 
\draw    (204,216) -- (250,216) ;
\draw [shift={(252,216)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da3767170061103269] 
\draw    (312,10) -- (492,10) ;
%Straight Lines [id:da8982178930087799] 
\draw    (312,274) -- (492,274) ;
%Rounded Rect [id:dp7505871568672602] 
\draw  [fill={rgb, 255:red, 0; green, 228; blue, 255 }  ,fill opacity=1 ] (348,129.6) .. controls (348,124.3) and (352.3,120) .. (357.6,120) -- (446.4,120) .. controls (451.7,120) and (456,124.3) .. (456,129.6) -- (456,158.4) .. controls (456,163.7) and (451.7,168) .. (446.4,168) -- (357.6,168) .. controls (352.3,168) and (348,163.7) .. (348,158.4) -- cycle ;
%Straight Lines [id:da6308256525563409] 
\draw    (402,10) -- (402,118) ;
\draw [shift={(402,120)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da3278501294030751] 
\draw    (402,168) -- (402,272) ;
\draw [shift={(402,274)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da07269132682112511] 
\draw    (456,144) -- (502,144) ;
\draw [shift={(504,144)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

% Text Node
\draw (98,50.4) node [anchor=north west][inner sep=0.75pt]    {$C_{1}$};
% Text Node
\draw (98,194.4) node [anchor=north west][inner sep=0.75pt]    {$C_{2}$};
% Text Node
\draw (37,4.4) node [anchor=north west][inner sep=0.75pt]    {$\Theta _{1}$};
% Text Node
\draw (37,134.4) node [anchor=north west][inner sep=0.75pt]    {$\Theta _{2}$};
% Text Node
\draw (37,266.4) node [anchor=north west][inner sep=0.75pt]    {$\Theta _{3}$};
% Text Node
\draw (152,15.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{1}$};
% Text Node
\draw (152,99.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{2}$};
% Text Node
\draw (152,147.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{2}$};
% Text Node
\draw (152,243.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{3}$};
% Text Node
\draw (208,52.4) node [anchor=north west][inner sep=0.75pt]    {$W_{12}$};
% Text Node
\draw (208,194.4) node [anchor=north west][inner sep=0.75pt]    {$W_{23}$};
% Text Node
\draw (264,127.4) node [anchor=north west][inner sep=0.75pt]    {$\hat{=}$};
% Text Node
\draw (349,122.4) node [anchor=north west][inner sep=0.75pt]    {$C_{12}$};
% Text Node
\draw (289,2.4) node [anchor=north west][inner sep=0.75pt]    {$\Theta _{1}$};
% Text Node
\draw (289,264.4) node [anchor=north west][inner sep=0.75pt]    {$\Theta _{3}$};
% Text Node
\draw (408,52.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{1}$};
% Text Node
\draw (408,206.4) node [anchor=north west][inner sep=0.75pt]    {$Q_{3}$};
% Text Node
\draw (460,124.4) node [anchor=north west][inner sep=0.75pt]    {$W_{13} =W_{12} +W_{23}$};


\end{tikzpicture}
\end{center}

with $\Theta_1 > \Theta_2 > \Theta_3$. For the three efficiencies we have \begin{align*}
    \eta_C(\Theta_1,\Theta_2) &= 1 - \frac{\Theta_2}{\Theta_1} \\
    \eta_C(\Theta_2,\Theta_3) &= 1 - \frac{\Theta_3}{\Theta_2} \\
    \eta_C(\Theta_1,\Theta_3) &= 1 - \frac{\Theta_3}{\Theta_1}
\end{align*}
These can be rewriting to obtain \begin{equation*}
    \Theta_2 = \Theta_1(1-\eta_C(\Theta_1,\Theta_2))
\end{equation*}
\begin{equation*}
    \Theta_3 = \Theta_2(1-\eta_C(\Theta_2,\Theta_3)) = \Theta_1(1-\eta_C(\Theta_1,\Theta_2))(1-\eta_C(\Theta_2,\Theta_3))
\end{equation*}
and \begin{equation*}
    \Theta_3 = \Theta_1(1-\eta_C(\Theta_1,\Theta_3))
\end{equation*}
Equating the last two equations and dividing out the empirical temperature we have that \begin{equation*}
    (1-\eta_C(\Theta_1,\Theta_2))(1-\eta_C(\Theta_2,\Theta_3)) = (1-\eta_C(\Theta_1,\Theta_3))
\end{equation*}
Defining the function $f \equiv 1 - \eta_C$, this can be written as $f(\Theta_1,\Theta_2)f(\Theta_2,\Theta_3) = f(\Theta_1,\Theta_3)$. To investigate the functional form of $f$ let us take the logarithm to find \begin{equation*}
    \ln f(\Theta_1,\Theta_2) + \ln f(\Theta_2,\Theta_3) = \ln f(\Theta_1,\Theta_3)
\end{equation*}
Taking the partial with respect to $\Theta_1$ we obtain \begin{equation*}
    \frac{\partial }{\partial\Theta_1}\ln f(\Theta_1,\Theta_2) = \frac{\partial }{\partial \Theta_1}\ln f(\Theta_1,\Theta_3)
\end{equation*}
For this equality to hold, the $\Theta_2$ dependence on the left and the $\Theta_3$ dependence on the right must both vanish. The only functional form of $f$ that allows this is $f(\Theta_1,\Theta_2) = \alpha(\Theta_1)\beta(\Theta_2)$. Further, we have that $$\alpha(\Theta_1)\beta(\Theta_2)\alpha(\Theta_2)\beta(\Theta_3) = \alpha(\Theta_1)\beta(\Theta_3)$$ which gives $\alpha(\Theta_2)=\beta^{-1}(\Theta_2)$. With this we find that \begin{equation*}
    \eta_C(\Theta_1,\Theta_2) = 1 - \frac{\beta(\Theta_2)}{\beta(\Theta_1)}
\end{equation*}
Then, our new thermodynamic temperature scale is $\beta$, which can be fixed by setting $T^*\equiv \beta(\Theta^*)$ for a single heat bath. For example, we can use the triple point of water, ice and steam and set $T^*$ to $273.15\;K$. Thus, reqriting the previous equation we have \begin{equation*}
    T = T^*(1-\eta_C(T^*,T))
\end{equation*}
Hence, using a heat bath at $T^*$ as a reference point and operating a Carnot engine between the heat bath and any system of interest, the temperature of that system is given by the above expression, which requires the measurement of the efficiency of the Carnot engine in that arrangement. 




%%%%%%%%%%%%%%%%%%%% Section 1.3.4
\section{Clausius' Teorem and Thermodynamic Entropy}

\begin{qst}
    How does the efficiency limit of $\eta_C$ affect the allowed heat exchanges for heat engines? More generally, what can we say about the heat increment supplied to a system during a cyclic process or transformation?
\end{qst}

For a Carnot engine operating between $T_1 > T_2$, we have \begin{equation*}
    1 - \frac{Q_c}{Q_h}\equiv \eta_C = 1-\frac{T_2}{T_1}, 
\end{equation*}
where $Q_c > 0$ is the heat leaving the Carnot engine and $Q_h$ is the heat entering the Carnot engine. From the perspective of the Carnot engine, its change in energy has a positive contribution $\Delta Q_1 = Q_h$ and a negative one $\Delta Q_2 = -Q_c$. Then we can rewrite our last equation as \begin{equation*}
    1+\frac{\Delta Q_2}{\Delta Q_1} = 1-\frac{T_2}{T_1}
\end{equation*}
or equivalently \begin{equation*}
    \frac{\Delta Q_2}{T_2} + \frac{\Delta Q_1}{T_1} = 0
\end{equation*}
which has the form of a \Emph{conservation law}: over one cycle, the sum over all heat exchanges, divided by the respective temperature, is zero for the Carnot engine.

\begin{namthm}[Clausius' Theorem]
    For any cyclic transformation (reversible or not), $\oint \frac{\delta Q}{T} \leq 0$, where $\delta Q$ is the heat increment supplied to the system at temperature $T$.
\end{namthm}
\begin{proof}
    Consider a system $S$ that follows a general cyclic, quasi-static, process involving $n$ heat exchanges $\delta Q_i$, positive or negative, with heat baths at temperatures $T_i$.
    \begin{center}
        \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Rectangle [id:dp9718478795330696] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (108.75,69.28) -- (416.25,69.28) -- (416.25,206.32) -- (108.75,206.32) -- cycle ;
\draw   (112.5,137.8) -- (108.75,146.37) -- (105,137.8) ;
\draw   (258.75,202.04) -- (266.25,206.32) -- (258.75,210.6) ;
\draw   (412.5,154.93) -- (416.25,146.37) -- (420,154.93) ;
\draw   (266.25,73.56) -- (258.75,69.28) -- (266.25,65) ;
%Straight Lines [id:da05926003170028049] 
\draw    (131.25,232.01) -- (153.75,232.01) ;
%Straight Lines [id:da6623216589483099] 
\draw    (142.5,208.32) -- (142.5,230.01) ;
\draw [shift={(142.5,232.01)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(142.5,206.32)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da16360484892103] 
\draw    (183.75,231.16) -- (206.25,231.16) ;
%Straight Lines [id:da7027749588512926] 
\draw    (195,207.46) -- (195,229.16) ;
\draw [shift={(195,231.16)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(195,205.46)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da14294308937739086] 
\draw    (363.75,232.01) -- (386.25,232.01) ;
%Straight Lines [id:da3528473664720504] 
\draw    (375,208.32) -- (375,230.01) ;
\draw [shift={(375,232.01)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(375,206.32)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;

% Text Node
\draw (117,212.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{1}$};
% Text Node
\draw (133.38,232.91) node [anchor=north west][inner sep=0.75pt]    {$T_{1}$};
% Text Node
\draw (171,212.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{2}$};
% Text Node
\draw (185.88,232.05) node [anchor=north west][inner sep=0.75pt]    {$T_{2}$};
% Text Node
\draw (194.13,208.21) node [anchor=north west][inner sep=0.75pt]    {$.\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ $};
% Text Node
\draw (351,212.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{n}$};
% Text Node
\draw (365.75,232.91) node [anchor=north west][inner sep=0.75pt]    {$T_{n}$};
% Text Node
\draw (111,72) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont System S}};


\end{tikzpicture}
    \end{center}
    Over one cycle we necessarily have $\Delta U_S = 0$. The system might also perform a total amount of work $\Delta W_S$ over one cycle. THus, the first law gives us $\Delta W_S = -\sum_{i=1}^n\delta Q_i$.

    Next we can couple each heat bath to a Carnot engine, which operates between the given heat bath and a general heat bath at temperature $T_0$ with $T_0 > T_i$ for all $1 \leq i \leq n$. 
    \begin{center}
        \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Rectangle [id:dp9718478795330696] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (93.75,64.28) -- (401.25,64.28) -- (401.25,201.32) -- (93.75,201.32) -- cycle ;
\draw   (97.5,132.8) -- (93.75,141.37) -- (90,132.8) ;
\draw   (243.75,197.04) -- (251.25,201.32) -- (243.75,205.6) ;
\draw   (397.5,149.93) -- (401.25,141.37) -- (405,149.93) ;
\draw   (251.25,68.56) -- (243.75,64.28) -- (251.25,60) ;
%Straight Lines [id:da05926003170028049] 
\draw    (107.5,225.69) -- (140,225.69) ;
%Straight Lines [id:da6623216589483099] 
\draw    (123.7,202) -- (123.7,223.69) ;
\draw [shift={(123.7,225.69)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(123.7,200)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da16360484892103] 
\draw    (173.8,225.69) -- (206.3,225.69) ;
%Straight Lines [id:da7027749588512926] 
\draw    (190,202) -- (190,223.69) ;
\draw [shift={(190,225.69)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(190,200)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da14294308937739086] 
\draw    (347.5,225.69) -- (380,225.69) ;
%Straight Lines [id:da3528473664720504] 
\draw    (363.7,202) -- (363.7,223.69) ;
\draw [shift={(363.7,225.69)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(363.7,200)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Shape: Circle [id:dp5736980178083375] 
\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (108.7,265) .. controls (108.7,256.72) and (115.42,250) .. (123.7,250) .. controls (131.98,250) and (138.7,256.72) .. (138.7,265) .. controls (138.7,273.28) and (131.98,280) .. (123.7,280) .. controls (115.42,280) and (108.7,273.28) .. (108.7,265) -- cycle ;
%Straight Lines [id:da8142969463925529] 
\draw    (123.7,227.69) -- (123.7,249.39) ;
\draw [shift={(123.7,251.39)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(123.7,225.69)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da3283484218567414] 
\draw    (123.7,282) -- (123.7,303.69) ;
\draw [shift={(123.7,305.69)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(123.7,280)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da24523758085021696] 
\draw    (84.4,265) -- (106.7,265) ;
\draw [shift={(108.7,265)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(82.4,265)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da24525757899358935] 
\draw    (90,305.7) -- (400,305.7) ;
%Shape: Circle [id:dp8546933548323299] 
\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (175,265.01) .. controls (175,256.72) and (181.72,250.01) .. (190,250.01) .. controls (198.28,250.01) and (205,256.72) .. (205,265.01) .. controls (205,273.29) and (198.28,280.01) .. (190,280.01) .. controls (181.72,280.01) and (175,273.29) .. (175,265.01) -- cycle ;
%Straight Lines [id:da02708894519473004] 
\draw    (190,227.7) -- (190,249.39) ;
\draw [shift={(190,251.39)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(190,225.7)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da3433191234977291] 
\draw    (190,282.01) -- (190,303.7) ;
\draw [shift={(190,305.7)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(190,280.01)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da16661554373415566] 
\draw    (150.7,265.01) -- (173,265.01) ;
\draw [shift={(175,265.01)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(148.7,265.01)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Shape: Circle [id:dp7498400397192551] 
\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (348.7,265.01) .. controls (348.7,256.72) and (355.42,250.01) .. (363.7,250.01) .. controls (371.98,250.01) and (378.7,256.72) .. (378.7,265.01) .. controls (378.7,273.29) and (371.98,280.01) .. (363.7,280.01) .. controls (355.42,280.01) and (348.7,273.29) .. (348.7,265.01) -- cycle ;
%Straight Lines [id:da41602007194959745] 
\draw    (363.7,227.7) -- (363.7,249.39) ;
\draw [shift={(363.7,251.39)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(363.7,225.7)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da9567107118770806] 
\draw    (363.7,282.01) -- (363.7,303.7) ;
\draw [shift={(363.7,305.7)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(363.7,280.01)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
%Straight Lines [id:da054083773725472906] 
\draw    (324.4,265.01) -- (346.7,265.01) ;
\draw [shift={(348.7,265.01)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;
\draw [shift={(322.4,265.01)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (7.65,-2.3) .. controls (4.86,-0.97) and (2.31,-0.21) .. (0,0) .. controls (2.31,0.21) and (4.86,0.98) .. (7.65,2.3)   ;

% Text Node
\draw (98.2,206.08) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{1}$};
% Text Node
\draw (91.2,221.08) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$T_{1}$};
% Text Node
\draw (166,206.94) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{2}$};
% Text Node
\draw (161,221.94) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$T_{2}$};
% Text Node
\draw (215,203.4) node [anchor=north west][inner sep=0.75pt]    {$\ .\ .\ .\ .\ .\ .\ .\ .\ .\ .\ $};
% Text Node
\draw (339.7,206.08) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{n}$};
% Text Node
\draw (334.7,221.08) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$T_{n}$};
% Text Node
\draw (96,67) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont System S}};
% Text Node
\draw (117.7,260.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$C_{1}$};
% Text Node
\draw (127,232.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{C_{1}}$};
% Text Node
\draw (82,247.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta W_{1}$};
% Text Node
\draw (71,301.4) node [anchor=north west][inner sep=0.75pt]    {$T_{0}$};
% Text Node
\draw (132,287.4) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{C_{1}}^{( 0)}$};
% Text Node
\draw (184,260.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$C_{2}$};
% Text Node
\draw (193.3,232.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{C_{2}}$};
% Text Node
\draw (148.3,247.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta W_{2}$};
% Text Node
\draw (198.3,287.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{C_{2}}^{( 0)}$};
% Text Node
\draw (357.7,260.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$C_{n}$};
% Text Node
\draw (367,232.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{C_{n}}$};
% Text Node
\draw (322,247.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta W_{n}$};
% Text Node
\draw (372,287.41) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize]  {$\delta Q_{C_{n}}^{( 0)}$};


\end{tikzpicture}
\end{center}
    Since we can operate each Carnot engine also as a refrigerator, we can require that $\delta Q_{C_i} = -\delta Q_i$ for all $i$. Thus, we have effectively removed the heat baths from the picture since the Carnot engines either directly provide or receive the heat from the system in each step. Further, the combined system is effectively coupled to a single heat bath at $T_0$, which will allow us to invoke the 2nd law. Given that we are dealing with Carnot engines, we can use the conservation law we established above. Specifically, we have \begin{equation*}
        \delta Q_{C_i}^{(0)} = -\frac{T_0}{T_i}\delta Q_{C_i} = \frac{T_0}{T_i}\delta Q_i
    \end{equation*}
    Thus, for the total heat exchanged with the heat bath at $T_0$ over one cycle we have \begin{equation*}
        \Delta Q^{(0)} = \sum_{i=1}^n\delta Q_{C_i}^{(0)} = T_0\sum_{i=1}^n\frac{\delta Q_i}{T_i}
    \end{equation*}
    Considering now the combined system, its internal energy does not change over one cycle since it is a cyclic process. Thus, the first law gives us \begin{equation*}
        -\Delta Q^{(0)} = \Delta W
    \end{equation*}
    over once cycle, where $\Delta W = \Delta W_S + \sum_{i=1}^n\delta W_i$ is the total external work done by the combined system over one cycle. Since the combined system is coupled to a single heat bath, Kelvin's formulation of the second law applies giving $\Delta W \geq 0$ (the system is not doing work on the surroundings). Combining the last three equations we can eliminate all reference to the Carnot engines and obtain \begin{equation*}
        -T_0\sum_{i=1}^n\frac{\delta Q_i}{T_i} \geq 0
    \end{equation*}
    Since $T_0 > 0$, this is equivalent to \begin{equation*}
        \sum_{i=1}^n\frac{\delta Q_i}{T_i}\leq 0
    \end{equation*}
    In the limit as $n\rightarrow \infty$, this takes on the integral form given in Clausius' theorem.
\end{proof}


\subsection{Reversible Processes}

We now investigate consequences of Clausius' theorem for reversible processes. For a reversible process any path $\gamma$ in state space can be traced in reverse, $-\gamma$. For a cyclic process, this means that $\oint_{\gamma}\frac{\delta Q}{T} \leq 0$ and $\oint_{-\gamma}\frac{\delta Q}{T} \leq 0$, implying that \begin{equation*}
    \oint_{\gamma}\frac{\delta Q}{T} = 0
\end{equation*}
Then, for reversible processes we find that $\frac{\delta Q}{T}$ is an exact differential, and it allows us to construct a new function of state \begin{equation*}
    S(B) - S(A) \equiv \int_A^B\frac{\delta Q}{T}
\end{equation*}
where $A$ and $B$ are specific thermodynamic states and the integral is path-independent. $S$ is called the \Emph{thermodynamic entropy}. In differential form this reads \begin{equation*}
    dS = \frac{\delta Q}{T}\;\text{ or }\;\delta Q = TdS
\end{equation*}
The latter expression shows that adiabatic curves can be constructed for a general system from the condition of constant $S$, and it also allows us to reformulate the \Emph{first law for reversible processes}: in the case of a closed system \begin{equation*}
    dU = \delta Q + \delta W = TdS + \sum_{i=1}^mJ_idq_i
\end{equation*}


\subsection{Irreversible Processes}

Consider two states $A$ and $B$, and suppsoe we take one path from $A$ to $B$, and one arbitrarily chosen reversible path from $B$ back to $A$. Then by Clausius' theorem \begin{equation*}
    \int_A^B\frac{\delta Q}{T} + \int_A^B\frac{\delta Q_{rev}}{T} \leq 0 \implies \int_A^B\frac{\delta Q}{T} \leq \int_A^B\frac{\delta Q_{rev}}{T} = \int_A^BdS = S(B) - S(A)
\end{equation*}

In differential form this reads \begin{equation*}
    \frac{\delta Q}{T} \leq dS
\end{equation*}
for any transformation. 
\begin{note}
    Entropy has to be calculated along a \Emph{reversible path}.
\end{note}

Hence, for irreversible processes $\frac{\delta Q}{T}$ is not exact so we can't use it to define the entropy. Further, for an isolated system there is no exchange of heat, so the above equation reduces to $\delta S \geq 0$ for all internal processes (entropy always increases in isolated systems). This implies that an isolated system attains a maximum value of entropy in equilibrium since spontaneous internal changes can only increase $S$. Hence, the direction of increasing entropy points out the \Emph{arrow of time} and the path to equilibrium.

\begin{law}[Second Law of Thermodynamics (Planck)]
    If $A$ and $B$ are two isolated systems with fixed internal energies $U_A$ and $U_B$ and fixed entropies $S_A(U_A)$ and $S_B(U_B)$ before coupling, then the entropy of the compound system after coupling, $S(U_A+U_B)$, must be at least as large as the sum of the initial entropies \begin{equation*}
        S(U_A+U_B) \geq S_A(U_A) + S_B(U_B)
    \end{equation*}
\end{law}


\subsection{Example of Thermodynamic Entropy}

We consider the isothermal expansion of an ideal gas. We first consider a reversible version:

\begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Rectangle [id:dp10216684904861673] 
\draw   (130,150) -- (370,150) -- (370,250) -- (130,250) -- cycle ;
%Straight Lines [id:da2429362111030111] 
\draw [line width=3]    (240,150) -- (240,250) ;
%Straight Lines [id:da02312568719884389] 
\draw    (130,150) -- (140,140) ;
%Straight Lines [id:da698655090030744] 
\draw    (140,150) -- (150,140) ;
%Straight Lines [id:da07106673797646934] 
\draw    (150,150) -- (160,140) ;
%Straight Lines [id:da1321872558765933] 
\draw    (160,150) -- (170,140) ;
%Straight Lines [id:da3169921022163851] 
\draw    (170,150) -- (180,140) ;
%Straight Lines [id:da846357475435797] 
\draw    (180,150) -- (190,140) ;
%Straight Lines [id:da11213509476040429] 
\draw    (190,150) -- (200,140) ;
%Straight Lines [id:da6252836454895341] 
\draw    (200,150) -- (210,140) ;
%Straight Lines [id:da039339508257494105] 
\draw    (210,150) -- (220,140) ;
%Straight Lines [id:da8989019748379705] 
\draw    (220,150) -- (230,140) ;
%Straight Lines [id:da5794438220803482] 
\draw    (230,150) -- (240,140) ;
%Straight Lines [id:da43435070544714827] 
\draw    (240,150) -- (250,140) ;
%Straight Lines [id:da5016893721353803] 
\draw    (250,150) -- (260,140) ;
%Straight Lines [id:da6567634677030327] 
\draw    (260,150) -- (270,140) ;
%Straight Lines [id:da5891637497632802] 
\draw    (270,150) -- (280,140) ;
%Straight Lines [id:da8441024933096624] 
\draw    (280,150) -- (290,140) ;
%Straight Lines [id:da683615803738971] 
\draw    (290,150) -- (300,140) ;
%Straight Lines [id:da2194252383088786] 
\draw    (300,150) -- (310,140) ;
%Straight Lines [id:da3969359335913998] 
\draw    (310,150) -- (320,140) ;
%Straight Lines [id:da6734788351912069] 
\draw    (320,150) -- (330,140) ;
%Straight Lines [id:da2134759175389327] 
\draw    (330,150) -- (340,140) ;
%Straight Lines [id:da6310619449607919] 
\draw    (340,150) -- (350,140) ;
%Straight Lines [id:da22066732465272287] 
\draw    (350,150) -- (360,140) ;
%Straight Lines [id:da4705664969654242] 
\draw    (360,150) -- (370,140) ;
%Straight Lines [id:da9298760509242914] 
\draw    (370,150) -- (380,140) ;
%Straight Lines [id:da6373583309643975] 
\draw    (130,260) -- (140,250) ;
%Straight Lines [id:da6588641980940437] 
\draw    (140,260) -- (150,250) ;
%Straight Lines [id:da4281837223676326] 
\draw    (150,260) -- (160,250) ;
%Straight Lines [id:da5366300699949145] 
\draw    (160,260) -- (170,250) ;
%Straight Lines [id:da25469000907977524] 
\draw    (170,260) -- (180,250) ;
%Straight Lines [id:da0500622710738865] 
\draw    (180,260) -- (190,250) ;
%Straight Lines [id:da8168801800285694] 
\draw    (190,260) -- (200,250) ;
%Straight Lines [id:da7734357231245377] 
\draw    (200,260) -- (210,250) ;
%Straight Lines [id:da17327490110501187] 
\draw    (210,260) -- (220,250) ;
%Straight Lines [id:da9584695235724512] 
\draw    (220,260) -- (230,250) ;
%Straight Lines [id:da6821913084159383] 
\draw    (230,260) -- (240,250) ;
%Straight Lines [id:da9279112473339979] 
\draw    (240,260) -- (250,250) ;
%Straight Lines [id:da9780798821244712] 
\draw    (250,260) -- (260,250) ;
%Straight Lines [id:da8775526244710663] 
\draw    (260,260) -- (270,250) ;
%Straight Lines [id:da07147360245322298] 
\draw    (270,260) -- (280,250) ;
%Straight Lines [id:da07475291275506835] 
\draw    (280,260) -- (290,250) ;
%Straight Lines [id:da8443282662392222] 
\draw    (290,260) -- (300,250) ;
%Straight Lines [id:da20573580250055712] 
\draw    (300,260) -- (310,250) ;
%Straight Lines [id:da9400832784154305] 
\draw    (310,260) -- (320,250) ;
%Straight Lines [id:da7355193459821481] 
\draw    (320,260) -- (330,250) ;
%Straight Lines [id:da6976567499699897] 
\draw    (330,260) -- (340,250) ;
%Straight Lines [id:da158176559338429] 
\draw    (340,260) -- (350,250) ;
%Straight Lines [id:da38170981188901876] 
\draw    (350,260) -- (360,250) ;
%Straight Lines [id:da15351419827022705] 
\draw    (360,260) -- (370,250) ;
%Straight Lines [id:da04758347212249814] 
\draw    (370,160) -- (380,150) ;
%Straight Lines [id:da11833668881914683] 
\draw    (370,170) -- (380,160) ;
%Straight Lines [id:da9010401302925775] 
\draw    (370,180) -- (380,170) ;
%Straight Lines [id:da15770792103421472] 
\draw    (370,190) -- (380,180) ;
%Straight Lines [id:da04102367425755071] 
\draw    (370,200) -- (380,190) ;
%Straight Lines [id:da908916033166947] 
\draw    (370,210) -- (380,200) ;
%Straight Lines [id:da06039734040677214] 
\draw    (370,220) -- (380,210) ;
%Straight Lines [id:da7224209900671532] 
\draw    (370,230) -- (380,220) ;
%Straight Lines [id:da24187023727119872] 
\draw    (370,240) -- (380,230) ;
%Straight Lines [id:da6841954807390329] 
\draw    (370,250) -- (380,240) ;
%Straight Lines [id:da05005205845206229] 
\draw [line width=3]    (130,150) -- (130,250) ;
%Straight Lines [id:da03533549159528393] 
\draw    (440,130) -- (440,270) ;
%Shape: Spring [id:dp2463160075928994] 
\draw   (380,200.49) .. controls (380.99,200.18) and (382.15,200) .. (383.49,200) .. controls (393.49,200) and (393.49,210) .. (388.49,210) .. controls (383.49,210) and (383.49,200) .. (393.49,200) .. controls (403.49,200) and (403.49,210) .. (398.49,210) .. controls (393.49,210) and (393.49,200) .. (403.49,200) .. controls (413.49,200) and (413.49,210) .. (408.49,210) .. controls (403.49,210) and (403.49,200) .. (413.49,200) .. controls (423.49,200) and (423.49,210) .. (418.49,210) .. controls (413.49,210) and (413.49,200) .. (423.49,200) .. controls (433.49,200) and (433.49,210) .. (428.49,210) .. controls (423.49,210) and (423.49,200) .. (433.49,200) .. controls (436.55,200) and (438.67,200.93) .. (440,202.23) ;
%Straight Lines [id:da4587017232208439] 
\draw    (240,200) -- (380,200) ;
%Shape: Right Angle [id:dp38498859572961575] 
\draw   (200,280) -- (180,280) -- (180,264) ;
\draw   (175,275) -- (180,265) -- (185,275) ;

% Text Node
\draw (139,161) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont idea}l\\{\fontfamily{ptm}\selectfont  gas}};
% Text Node
\draw (291,181) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont piston}};
% Text Node
\draw (390,181) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont spring}};
% Text Node
\draw (200,271) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont heat bath T}};
% Text Node
\draw (442,133) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont wall}};


\end{tikzpicture}
\end{center}

The work done by the gas will be stored in the spring, making the process reversible (assuming an idealized frictionless spring). For an isothermal process, $T = $ const. Since for an ideal gas we also have $U = U(T)$, this means that there is no change to the internal energy during this process. This fact together with the first law as our starting point, we have: $$\Delta Q = -\Delta W = \int_{V_1}^{V_2}pdV = Nk_BT\int_{V_1}^{V_2}\frac{1}{V}dV = Nk_BT\ln\frac{V_2}{V_1}$$
where we have used the equation of state for an ideal gas and the fact that $T$ and $N$ are constant. For the change in entropy of the gas, this gives us \begin{equation*}
    (\Delta S)_{gas} = \int_1^2\frac{\delta Q}{T} = \frac{1}{T}\int_1^2\delta Q = \frac{1}{T}\Delta Q = Nk_B\ln\frac{V_2}{V_1} > 0
\end{equation*}
Similarly, we can obtain the change in entropy for the heat bath: \begin{equation*}
    (\Delta S)_{hb} = \int_1^2\frac{\delta Q_{hb}}{T} = \frac{1}{T}\int_1^2\delta Q_{hb} = \frac{\Delta Q_{hb}}{T} = \frac{-\Delta Q}{T} = -(\Delta S)_{gas}
\end{equation*}
Consequently, there is an entropy increase for the gas and an entropy decrease for the heat bath such that the combined change of entropy adds up to zero, $(\Delta S)_{total} = (\Delta S)_{gas} + (\Delta S)_{hb} = 0$.

Now, let use consider an irreversible process:

\begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Rectangle [id:dp10216684904861673] 
\draw   (30,150) -- (270,150) -- (270,250) -- (30,250) -- cycle ;
%Straight Lines [id:da2429362111030111] 
\draw [line width=3]    (140,150) -- (140,250) ;
%Straight Lines [id:da02312568719884389] 
\draw    (30,150) -- (40,140) ;
%Straight Lines [id:da698655090030744] 
\draw    (40,150) -- (50,140) ;
%Straight Lines [id:da07106673797646934] 
\draw    (50,150) -- (60,140) ;
%Straight Lines [id:da1321872558765933] 
\draw    (60,150) -- (70,140) ;
%Straight Lines [id:da3169921022163851] 
\draw    (70,150) -- (80,140) ;
%Straight Lines [id:da846357475435797] 
\draw    (80,150) -- (90,140) ;
%Straight Lines [id:da11213509476040429] 
\draw    (90,150) -- (100,140) ;
%Straight Lines [id:da6252836454895341] 
\draw    (100,150) -- (110,140) ;
%Straight Lines [id:da039339508257494105] 
\draw    (110,150) -- (120,140) ;
%Straight Lines [id:da8989019748379705] 
\draw    (120,150) -- (130,140) ;
%Straight Lines [id:da5794438220803482] 
\draw    (130,150) -- (140,140) ;
%Straight Lines [id:da43435070544714827] 
\draw    (140,150) -- (150,140) ;
%Straight Lines [id:da5016893721353803] 
\draw    (150,150) -- (160,140) ;
%Straight Lines [id:da6567634677030327] 
\draw    (160,150) -- (170,140) ;
%Straight Lines [id:da5891637497632802] 
\draw    (170,150) -- (180,140) ;
%Straight Lines [id:da8441024933096624] 
\draw    (180,150) -- (190,140) ;
%Straight Lines [id:da683615803738971] 
\draw    (190,150) -- (200,140) ;
%Straight Lines [id:da2194252383088786] 
\draw    (200,150) -- (210,140) ;
%Straight Lines [id:da3969359335913998] 
\draw    (210,150) -- (220,140) ;
%Straight Lines [id:da6734788351912069] 
\draw    (220,150) -- (230,140) ;
%Straight Lines [id:da2134759175389327] 
\draw    (230,150) -- (240,140) ;
%Straight Lines [id:da6310619449607919] 
\draw    (240,150) -- (250,140) ;
%Straight Lines [id:da22066732465272287] 
\draw    (250,150) -- (260,140) ;
%Straight Lines [id:da4705664969654242] 
\draw    (260,150) -- (270,140) ;
%Straight Lines [id:da9298760509242914] 
\draw    (270,150) -- (280,140) ;
%Straight Lines [id:da6373583309643975] 
\draw    (30,260) -- (40,250) ;
%Straight Lines [id:da6588641980940437] 
\draw    (40,260) -- (50,250) ;
%Straight Lines [id:da4281837223676326] 
\draw    (50,260) -- (60,250) ;
%Straight Lines [id:da5366300699949145] 
\draw    (60,260) -- (70,250) ;
%Straight Lines [id:da25469000907977524] 
\draw    (70,260) -- (80,250) ;
%Straight Lines [id:da0500622710738865] 
\draw    (80,260) -- (90,250) ;
%Straight Lines [id:da8168801800285694] 
\draw    (90,260) -- (100,250) ;
%Straight Lines [id:da7734357231245377] 
\draw    (100,260) -- (110,250) ;
%Straight Lines [id:da17327490110501187] 
\draw    (110,260) -- (120,250) ;
%Straight Lines [id:da9584695235724512] 
\draw    (120,260) -- (130,250) ;
%Straight Lines [id:da6821913084159383] 
\draw    (130,260) -- (140,250) ;
%Straight Lines [id:da9279112473339979] 
\draw    (140,260) -- (150,250) ;
%Straight Lines [id:da9780798821244712] 
\draw    (150,260) -- (160,250) ;
%Straight Lines [id:da8775526244710663] 
\draw    (160,260) -- (170,250) ;
%Straight Lines [id:da07147360245322298] 
\draw    (170,260) -- (180,250) ;
%Straight Lines [id:da07475291275506835] 
\draw    (180,260) -- (190,250) ;
%Straight Lines [id:da8443282662392222] 
\draw    (190,260) -- (200,250) ;
%Straight Lines [id:da20573580250055712] 
\draw    (200,260) -- (210,250) ;
%Straight Lines [id:da9400832784154305] 
\draw    (210,260) -- (220,250) ;
%Straight Lines [id:da7355193459821481] 
\draw    (220,260) -- (230,250) ;
%Straight Lines [id:da6976567499699897] 
\draw    (230,260) -- (240,250) ;
%Straight Lines [id:da158176559338429] 
\draw    (240,260) -- (250,250) ;
%Straight Lines [id:da38170981188901876] 
\draw    (250,260) -- (260,250) ;
%Straight Lines [id:da15351419827022705] 
\draw    (260,260) -- (270,250) ;
%Straight Lines [id:da04758347212249814] 
\draw    (270,160) -- (280,150) ;
%Straight Lines [id:da11833668881914683] 
\draw    (270,170) -- (280,160) ;
%Straight Lines [id:da9010401302925775] 
\draw    (270,180) -- (280,170) ;
%Straight Lines [id:da15770792103421472] 
\draw    (270,190) -- (280,180) ;
%Straight Lines [id:da04102367425755071] 
\draw    (270,200) -- (280,190) ;
%Straight Lines [id:da908916033166947] 
\draw    (270,210) -- (280,200) ;
%Straight Lines [id:da06039734040677214] 
\draw    (270,220) -- (280,210) ;
%Straight Lines [id:da7224209900671532] 
\draw    (270,230) -- (280,220) ;
%Straight Lines [id:da24187023727119872] 
\draw    (270,240) -- (280,230) ;
%Straight Lines [id:da6841954807390329] 
\draw    (270,250) -- (280,240) ;
%Straight Lines [id:da05005205845206229] 
\draw [line width=0.75]    (30,150) -- (30,250) ;
%Shape: Right Angle [id:dp38498859572961575] 
\draw   (100,280) -- (80,280) -- (80,264) ;
\draw   (75,275) -- (80,265) -- (85,275) ;
%Straight Lines [id:da6240393448652244] 
\draw    (20,160) -- (30,150) ;
%Straight Lines [id:da06618897020996117] 
\draw    (20,170) -- (30,160) ;
%Straight Lines [id:da9578997465003252] 
\draw    (20,180) -- (30,170) ;
%Straight Lines [id:da14035317588324148] 
\draw    (20,190) -- (30,180) ;
%Straight Lines [id:da28751356867146516] 
\draw    (20,200) -- (30,190) ;
%Straight Lines [id:da20996336625159362] 
\draw    (20,210) -- (30,200) ;
%Straight Lines [id:da06900184505583318] 
\draw    (20,220) -- (30,210) ;
%Straight Lines [id:da19787646399737802] 
\draw    (20,230) -- (30,220) ;
%Straight Lines [id:da8937533832195679] 
\draw    (20,240) -- (30,230) ;
%Straight Lines [id:da6837655871316055] 
\draw    (20,250) -- (30,240) ;
%Straight Lines [id:da8730624412553964] 
\draw    (20,260) -- (30,250) ;
%Straight Lines [id:da4662680445037337] 
\draw [line width=0.75]    (300,198.5) -- (392,198.5)(300,201.5) -- (392,201.5) ;
\draw [shift={(400,200)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp7654546436479621] 
\draw   (420,151) -- (660,151) -- (660,251) -- (420,251) -- cycle ;
%Straight Lines [id:da4957556971692221] 
\draw    (420,151) -- (430,141) ;
%Straight Lines [id:da7911328358491527] 
\draw    (430,151) -- (440,141) ;
%Straight Lines [id:da14482838729842618] 
\draw    (440,151) -- (450,141) ;
%Straight Lines [id:da43143802969661915] 
\draw    (450,151) -- (460,141) ;
%Straight Lines [id:da010900293323580668] 
\draw    (460,151) -- (470,141) ;
%Straight Lines [id:da54047464155218] 
\draw    (470,151) -- (480,141) ;
%Straight Lines [id:da7950200154755129] 
\draw    (480,151) -- (490,141) ;
%Straight Lines [id:da21378379658618107] 
\draw    (490,151) -- (500,141) ;
%Straight Lines [id:da04595231590632709] 
\draw    (500,151) -- (510,141) ;
%Straight Lines [id:da015584736371790697] 
\draw    (510,151) -- (520,141) ;
%Straight Lines [id:da5615542568209158] 
\draw    (520,151) -- (530,141) ;
%Straight Lines [id:da03976983114492505] 
\draw    (530,151) -- (540,141) ;
%Straight Lines [id:da7666466541975896] 
\draw    (540,151) -- (550,141) ;
%Straight Lines [id:da8456893070030036] 
\draw    (550,151) -- (560,141) ;
%Straight Lines [id:da2802253195572617] 
\draw    (560,151) -- (570,141) ;
%Straight Lines [id:da5420324900208815] 
\draw    (570,151) -- (580,141) ;
%Straight Lines [id:da5919421428102054] 
\draw    (580,151) -- (590,141) ;
%Straight Lines [id:da9580467690674728] 
\draw    (590,151) -- (600,141) ;
%Straight Lines [id:da8987060211850415] 
\draw    (600,151) -- (610,141) ;
%Straight Lines [id:da3989177296396482] 
\draw    (610,151) -- (620,141) ;
%Straight Lines [id:da7739136545307406] 
\draw    (620,151) -- (630,141) ;
%Straight Lines [id:da43340871434295836] 
\draw    (630,151) -- (640,141) ;
%Straight Lines [id:da21233446836267156] 
\draw    (640,151) -- (650,141) ;
%Straight Lines [id:da08665469965397943] 
\draw    (650,151) -- (660,141) ;
%Straight Lines [id:da001828566199576498] 
\draw    (660,151) -- (670,141) ;
%Straight Lines [id:da46995823741590326] 
\draw    (420,261) -- (430,251) ;
%Straight Lines [id:da40898502643536516] 
\draw    (430,261) -- (440,251) ;
%Straight Lines [id:da33006003709084597] 
\draw    (440,261) -- (450,251) ;
%Straight Lines [id:da4403370974834946] 
\draw    (450,261) -- (460,251) ;
%Straight Lines [id:da09910753257315763] 
\draw    (460,261) -- (470,251) ;
%Straight Lines [id:da3263787303208714] 
\draw    (470,261) -- (480,251) ;
%Straight Lines [id:da3939572787072987] 
\draw    (480,261) -- (490,251) ;
%Straight Lines [id:da22224045530895964] 
\draw    (490,261) -- (500,251) ;
%Straight Lines [id:da3760816140428329] 
\draw    (500,261) -- (510,251) ;
%Straight Lines [id:da43610384642407674] 
\draw    (510,261) -- (520,251) ;
%Straight Lines [id:da27699152056186027] 
\draw    (520,261) -- (530,251) ;
%Straight Lines [id:da25535874444777984] 
\draw    (530,261) -- (540,251) ;
%Straight Lines [id:da02204680539402326] 
\draw    (540,261) -- (550,251) ;
%Straight Lines [id:da289717919573794] 
\draw    (550,261) -- (560,251) ;
%Straight Lines [id:da9578417217036497] 
\draw    (560,261) -- (570,251) ;
%Straight Lines [id:da2932021562319078] 
\draw    (570,261) -- (580,251) ;
%Straight Lines [id:da3687541337506155] 
\draw    (580,261) -- (590,251) ;
%Straight Lines [id:da1336556930885484] 
\draw    (590,261) -- (600,251) ;
%Straight Lines [id:da04094069738267603] 
\draw    (600,261) -- (610,251) ;
%Straight Lines [id:da11263861721388069] 
\draw    (610,261) -- (620,251) ;
%Straight Lines [id:da6622327264174903] 
\draw    (620,261) -- (630,251) ;
%Straight Lines [id:da37532828249128314] 
\draw    (630,261) -- (640,251) ;
%Straight Lines [id:da9961666123003028] 
\draw    (640,261) -- (650,251) ;
%Straight Lines [id:da9621937765102659] 
\draw    (650,261) -- (660,251) ;
%Straight Lines [id:da10584236043889939] 
\draw    (660,161) -- (670,151) ;
%Straight Lines [id:da9237394338423308] 
\draw    (660,171) -- (670,161) ;
%Straight Lines [id:da5742243995817473] 
\draw    (660,181) -- (670,171) ;
%Straight Lines [id:da7893262754279833] 
\draw    (660,191) -- (670,181) ;
%Straight Lines [id:da9220794090585858] 
\draw    (660,201) -- (670,191) ;
%Straight Lines [id:da44476084028411544] 
\draw    (660,211) -- (670,201) ;
%Straight Lines [id:da17710479372236643] 
\draw    (660,221) -- (670,211) ;
%Straight Lines [id:da7506078019116467] 
\draw    (660,231) -- (670,221) ;
%Straight Lines [id:da7192840973477705] 
\draw    (660,241) -- (670,231) ;
%Straight Lines [id:da23787537444433715] 
\draw    (660,251) -- (670,241) ;
%Straight Lines [id:da5172582867911493] 
\draw [line width=0.75]    (420,151) -- (420,251) ;
%Shape: Right Angle [id:dp0020224712984286963] 
\draw   (490,281) -- (470,281) -- (470,265) ;
\draw   (465,276) -- (470,266) -- (475,276) ;
%Straight Lines [id:da33326147345376156] 
\draw    (410,161) -- (420,151) ;
%Straight Lines [id:da8162242233241097] 
\draw    (410,171) -- (420,161) ;
%Straight Lines [id:da039706177803739306] 
\draw    (410,181) -- (420,171) ;
%Straight Lines [id:da561916986959849] 
\draw    (410,191) -- (420,181) ;
%Straight Lines [id:da1956405303914721] 
\draw    (410,201) -- (420,191) ;
%Straight Lines [id:da35335241617877156] 
\draw    (410,211) -- (420,201) ;
%Straight Lines [id:da26475813037398277] 
\draw    (410,221) -- (420,211) ;
%Straight Lines [id:da5708591395844156] 
\draw    (410,231) -- (420,221) ;
%Straight Lines [id:da7034988878222872] 
\draw    (410,241) -- (420,231) ;
%Straight Lines [id:da3705636825871046] 
\draw    (410,251) -- (420,241) ;
%Straight Lines [id:da16059908449951465] 
\draw    (410,261) -- (420,251) ;

% Text Node
\draw (39,161) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont idea}l\\{\fontfamily{ptm}\selectfont  gas}};
% Text Node
\draw (100,271) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont heat bath T}};
% Text Node
\draw (86,163.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (106,183.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (72,203.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (72,173.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (52,212.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (112,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (102,163.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (91,203.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (72,153.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (121,143.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (42,163.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (32,192.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (311,182) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont remove wall}};
% Text Node
\draw (302,203) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont free expansion}};
% Text Node
\draw (320,223.4) node [anchor=north west][inner sep=0.75pt]    {$\Delta W=0$};
% Text Node
\draw (511,202) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont idea}l {\fontfamily{ptm}\selectfont gas}};
% Text Node
\draw (490,272) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont heat bath T}};
% Text Node
\draw (476,164.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (496,184.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (462,204.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (462,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (442,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (502,214.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (492,164.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (481,204.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (452,152.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (511,144.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (431,172.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (422,193.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (615,154.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (635,174.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (601,194.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (601,164.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (581,203.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (641,204.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (631,154.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (620,194.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (591,142.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (570,162.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (561,183.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (531,164.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (522,173.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (551,153.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (542,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (562,213.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (522,223.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};
% Text Node
\draw (612,223.4) node [anchor=north west][inner sep=0.75pt]  [font=\Large]  {$\cdot $};


\end{tikzpicture}
\end{center}

This process is irreversible since reintroducing the wall will not lead back to the original state. As for the reversible case, $\Delta U = 0$. Yet, no work is done, $\Delta W = 0$, and the first law gives $\Delta Q = 0$. To calculate the change in entropy we need a reversible process taht has the same initial and final state as the irreversible one. The reversible version above can be used for that purpose. Thus \begin{equation*}
    (\Delta S)_{gas} = Nk_B\ln\frac{V_2}{V_1} > 0
\end{equation*}
However, for the heat bath we have to take into account that $\Delta Q = 0 = \Delta Q_{hb}$ --- the state of the heat bath did not change at all --- which leads to \begin{equation*}
    (\Delta S)_{hb} = \int_1^2\frac{\delta Q_{hb}}{T} = 0
\end{equation*}
Consequently, there is an entropy increase for the gas and no change in entropy for the heat bath such that the combined change of entropy of the system is positive, $(\Delta S)_{total} = (\Delta S)_{gas} + (\Delta S)_{hb} = Nk_B\ln\frac{V_2}{V_1} > 0$. Multiplying this expression by $T$, we obtain \begin{equation*}
    T(\Delta S)_{total} = Nk_BT\ln\frac{V_2}{V_1} > 0
\end{equation*}
where the left hand side is equivalent to the work done in the reversible case. THus, the irreversible process wastes usable energy.



%%%%%%%%%%%%%%%%%%%%%% Chapter 1.4
\chapter{Thermodynamical Potentials and Equilibrium}



%%%%%%%%%%%%%%%%%%%% Section 1.4.1
\section{Thermodynamic Potentials}

Recall that we have introduced thermodynamic entropy as a new state variable based on reversible processes. Now we answer the question 
\begin{qst}
    How can we uniquely determine all equilibrium properties of a given system?
\end{qst}
First, for reversible processes the 1st law reads \begin{equation*}
    dU = \delta Q + \delta W + \delta E_{chem} = TdS + \sum_{i=1}^mJ_idq_i + \sum_{j=1}^{\alpha}\mu_jdN_j
\end{equation*}
so we have that $U = U(S,\vec{q},\vec{N})$. Hence, the number of independent thermodynamic coordinates is $1+m+\alpha$, assuming that there are no additional constraints between these coordinates. From this expression for the first law we have that \begin{align*}
    \left(\frac{\partial U}{\partial S}\right)_{\vec{q},\vec{N}} &= T, \\
    \left(\frac{\partial U}{\partial q_i}\right)_{S,q_{j\neq i},\vec{N}} &= J_i \\
    \left(\frac{\partial U}{\partial N_j}\right)_{S,\vec{q},N_{i\neq j}} &= \mu_j
\end{align*}
Similarly, the function $U(S,\vec{q},\vec{N})$ ``generates" all other dependent thermodynamic quantities of interest through partial derivatives, including heat capacities, compressibilities, etcetera.

\begin{defn}
    A function that uniquely determines all equilibrium properties of a given system is called a \Emph{thermodynamic potential}
\end{defn}

\begin{rmk}
    The goal of thermodynamics is to do measurements such that one can construct a thermodynamic potential as a function of its arguments for a given system.
\end{rmk}

Statistical Mechanics aims to derive these thermodynamic potentials from the underlying microscopic dynamics.

\subsection{Maxwell Relations}

Requiring $U$ to be a two time continuously differentiable function, using the relations derived above and the fact that the mixed partials will be equal we have the following:

\begin{prop}
    Using the commutativity of mixed partials for two time continuously differentiable functions, we have the following \Emph{Maxwell relations}: \begin{equation}
        \frac{\partial^2 U}{\partial q_i\partial S} = \frac{\partial^2 U}{\partial S\partial q_i} \iff \frac{\partial T}{\partial q_i} = \frac{\partial J_i}{\partial S}
    \end{equation}
    and \begin{equation}
        \frac{\partial^2U}{\partial N_j\partial S} = \frac{\partial^2 U}{\partial S\partial N_j} \iff \frac{\partial T}{\partial N_j} = \frac{\partial \mu_j}{\partial S}
    \end{equation}
\end{prop}

\subsection{Thermodynamic Entropy}

Rewriting the first law for reversible processes we have \begin{equation*}
    dS = \frac{1}{T}\left(dU-\sum_{i=1}^mJ_idq_i - \sum_{j=1}^{\alpha}\mu_jdN_j\right)
\end{equation*}
Hence, we can express entropy as $S = S(U,\vec{q},\vec{N})$. Moreover, we can obtain all other thermodynamic quanitities of interest through partials: \begin{align*}
    \left(\frac{\partial S}{\partial U}\right)_{\vec{q},\vec{N}} &= \frac{1}{T}, \\
    \left(\frac{\partial S}{\partial q_i}\right)_{U,q_{j\neq i},\vec{N}} &= -\frac{J_i}{T}, \\
    \left(\frac{\partial S}{\partial N_j}\right)_{U,\vec{q},N_{i\neq j}} &= -\frac{\mu_j}{T}
\end{align*}
Thus, the thermodynamic entropy is a thermodynamic potential. If the arguments $U,\vec{q},\vec{N}$ are fixed there is no change in internal energy, no mechanical work is done, no chemical work is done, and there is no exchange of heat by the first law so we have an isolated system. Further, for an isolated system the thermodynamic entropy is maximal in equilibrium, so if entropy is expressed as a function of its \Emph{natural variable} $U,\vec{q}$ and $\vec{N}$ it is a thermodynamic potential with extremal properties.

\subsection{Legendre Transformation}

Consider an arbitrary smooth real valued function $f(x,y)$ defined on the plane, and consider its total differential $df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy$. We wish to replace the variable $x$ with its conjugate variable $u = \frac{\partial f}{\partial x}$. First, consider the function $g(x,y,u) = ux-f(x,y)$. It's exact differential is \begin{equation*}
    dg = d(ux) - df = xdu + udx - \frac{\partial f}{\partial x}dx - \frac{\partial f}{\partial y}dy
\end{equation*}
So that the $dx$ terms cancel out we choose $u(x,y) = \frac{\partial f}{\partial x}$, which gives \begin{equation*}
    dg = xdu - \frac{\partial f}{\partial y}dy \text{  and  } g = g(u,y) = ux(u,y) - f(x(u,y),y)
\end{equation*}
where we have made the assumption that $u(x,y) = \frac{\partial f}{\partial x}$ is suitably nice (is an increasing function, and hence invertible) such that we can write $x$ as a function of $u$ and $y$. Under these assumptions $g$ is the \Emph{Legendre transformation} of $f$ with respect to $x$. We will now use the Legendre transformation to derive different thermodynamic potentials starting from the expression for the total differential of the internal energy for reversible processes \begin{equation*}
    dU = TdS + \sum_{i=1}^mJ_idq_i + \sum_{j=1}^{\alpha}\mu_jdN_j
\end{equation*}
so $\partial_SU = T,\partial_{q_i}U = J_i$, and $\partial_{N_j}U = \mu_j$. Thus, $T,\vec{J}$ and $\vec{\mu}$ fulfill the requirements to be used as new thermodynamic coordinates in a Legendre transformation of the internal energy provided the invertibility conditions hold.


\subsection{Helmholtz Free Energy}

Given that $\partial_SU = T$ we can replace $S$ by $T$ through a Legendre transformation of the internal energy. The new function is defined as follows:

\begin{defn}
    The \Emph{Helmholtz free energy} is defined as \begin{equation*}
        F(T,\vec{q},\vec{N}) = U - TS
    \end{equation*}
    which is the negative of the standard Legendre transformation, and has total differential \begin{equation*}
        dF = dU - d(TS) = dU -SdT - TdS = \sum_{i=1}^mJ_idq_i + \sum_{j=1}^{\alpha}\mu_jdN_j - SdT
    \end{equation*}
    such that \begin{align*}
        J_i &= \left(\frac{\partial F}{\partial q_i}\right)_{q_{j\neq i},\vec{N},T} \\
        \mu_j &= \left(\frac{\partial F}{\partial N_j}\right)_{\vec{q},N_{i\neq j},T} \\
        S &= - \left(\frac{\partial F}{\partial T}\right)_{\vec{q},\vec{N}} 
    \end{align*}
\end{defn}

Note that $U = F+TS = F-T\frac{\partial F}{\partial T}$, so we can derive all thermodynamic properties of a given system from the Helmholtz free energy making it a thermodynamic potential. From the partials we have the following: 

\begin{prop}
    Using the relations above for the Helmholtz free energy, we have the Maxwell relation \begin{equation*}
        \frac{\partial^2F}{\partial q_i\partial T} = \frac{\partial^2F}{\partial T\partial q_i}\iff -\frac{\partial S}{\partial q_i} = \frac{\partial J_i}{\partial T}
    \end{equation*}
\end{prop}


\begin{eg}[Ideal Gas]
    For an ideal gas, the Maxwell relation takes the form $-\frac{\partial S}{\partial V} = -\frac{\partial p}{\partial T}$, so $\frac{\partial S}{\partial V} = \frac{\partial p}{\partial T}$. Using the equation of state for an ideal gas we have \begin{equation*}
        \frac{\partial S}{\partial V} = \frac{Nk_B}{V}
    \end{equation*}
    Integration of this partial differential equation gives \begin{equation*}
        S(T,V,N) = S_0+Nk_B\ln\frac{V}{V_0}+f(T,N)
    \end{equation*}
    where $S_0$ and $V_0$ are constants of integration and $f$ is a function is an arbitrary function to be specified by other partials of $S$.
\end{eg}

\subsection{Enthalpy}

Given that $\partial_{q_i}U = J_i$, we can replace $\vec{q}$ by $\vec{J}$ through a sequence of Legendre transformations of the internal energy. The new function is defined as follows:
\begin{defn}
    The \Emph{enthalpy} of a thermodynamic system is defined by \begin{equation*}
        \mathcal{H}(S,\vec{J},\vec{N}) = U - \sum_{i=1}^mJ_iq_i
    \end{equation*}
    (The negative of a sequence of Legendre transforms) with the exact differential \begin{equation*}
        d\mathcal{H} = dU - \sum_{i=1}^m(J_idq_i + q_idJ_i) = TdS - \sum_{i=1}^mq_idJ_i + \sum_{j=1}^{\alpha}\mu_jdN_j
    \end{equation*}
    with the partials \begin{align*}
        T &= \left(\frac{\partial \mathcal{H}}{\partial S}\right)_{\vec{J},\vec{N}}  \\
        q_i &= -\left(\frac{\partial \mathcal{H}}{\partial J_i}\right)_{S,J_{j\neq i},\vec{N}} \\
        \mu_j &= \left(\frac{\partial \mathcal{H}}{\partial N_j}\right)_{S,\vec{J},N_{i\neq j}} 
    \end{align*}
\end{defn}

Consequently, enthalpy is also a thermodynamic potential. Then we obtain the Maxwell relations:

\begin{prop}
    Using the mixed partials of the Enthalpy we have \begin{equation*}
        \frac{\partial^2\mathcal{H}}{\partial S\partial J_i} = \frac{\partial^2\mathcal{H}}{\partial J_i\partial S}\iff -\frac{\partial q_i}{\partial S} = \frac{\partial T}{\partial J_i}
    \end{equation*}
\end{prop}

\subsection{Gibbs Free Energy}

Performing a Legendre transformation on the Enthalpy by replacing $S$ by $T$, we obtain the following function:

\begin{defn}
    The \Emph{Gibbs free energy} is defined by \begin{equation*}
        G(T,\vec{J},\vec{N}) = U - TS - \sum_{i=1}^mJ_iq_i
    \end{equation*}
    with the exact differential \begin{equation*}
        dG = dU - TdS - SdT - \sum_{i=1}^m(J_idq_i+q_idJ_i) = -SdT - \sum_{i=1}^mq_idJ_i + \sum_{j=1}^{\alpha}\mu_jdN_j
    \end{equation*}
    and the partials \begin{align*}
        S &= -\left(\frac{\partial G}{\partial T}\right)_{\vec{J},\vec{N}} \\
        q_i &= -\left(\frac{\partial G}{\partial J_i}\right)_{T,J_{j\neq i},\vec{N}} \\
        \mu_j &=  \left(\frac{\partial G}{\partial N_j}\right)_{T,\vec{J},N_{i\neq j}} 
    \end{align*}
\end{defn}

Hence, the Gibbs free energy is also a thermodynamic potential. We also obtain the Maxwell relations:

\begin{prop}
    Using the mixed partials of the Gibbs free energy we have \begin{equation*}
        \frac{\partial^2G}{\partial T\partial J_i} = \frac{\partial^2G}{\partial J_i\partial T}\iff \frac{\partial q_i}{\partial T} = \frac{\partial S}{\partial J_i}
    \end{equation*}
\end{prop}


\subsection{Grand Potential}

Finally, we use $\partial_{N_j}U = \mu_j$ to replace $\vec{N}$ by $\vec{\mu}$ through a Legendre transformation, while also replacing $S$ by $T$. 
\begin{defn}
    The \Emph{grand potential} is defined by \begin{equation*}
        \mathcal{G}(T,\vec{q},\vec{\mu}) = U - TS - \sum_{j=1}^{\alpha}N_j\mu_j
    \end{equation*}
    with the exact differential \begin{equation*}
        d\mathcal{G} = -SdT + \sum_{i=1}^mJ_idq_i - \sum_{j=1}^{\alpha}N_jd\mu_j
    \end{equation*}
    \begin{align*}
        S &= -\left(\frac{\partial \mathcal{G}}{\partial T}\right)_{\vec{q},\vec{\mu}} \\
        J_i &= \left(\frac{\partial \mathcal{G}}{\partial q_i}\right)_{T,q_{j\neq i},\vec{\mu}} \\
        N_j &=  \left(\frac{\partial \mathcal{G}}{\partial \mu_j}\right)_{T,\vec{q},\mu_{i\neq j}} 
    \end{align*}
\end{defn}

Thus, the grand potential is also a thermodynamic potential. We also obtain the Maxwell relations:


\begin{prop}
    Using the mixed partials of the Gibbs free energy we have \begin{equation*}
        \frac{\partial^2\mathcal{G}}{\partial T\partial q_i} = \frac{\partial^2\mathcal{G}}{\partial q_i\partial T}\iff \frac{\partial J_i}{\partial T} = -\frac{\partial S}{\partial q_i}
    \end{equation*}
\end{prop}

\subsection{Example: Surface Tension of a Liquid}

Consider the situation where the surface area of a liquid is increased from $A$ to $A+dA$ at fixed volume $V$. The associated work due to surface tension $\sigma$ is given by $\delta W_A = \sigma dA$. Experimentally, it has been established that $\sigma = \sigma(T) = \alpha(1-T/T_c)$ with $\alpha > 0$ for $T < T_c$.

Based on the 1st law we have \begin{equation*}
    dU = \delta Q + \delta W = \delta Q + \delta W_V + \delta W_A = TdS - pdV + \sigma dA
\end{equation*}
for reversible processes. Thus, $U = U(S,V,A)$.

\begin{qst}
    How does the temperature change if $A$ changes?
\end{qst}

To answer this we find $\frac{\partial T}{\partial A}$, which corresponds to an adiabatic ($\delta Q = 0 \iff dS = 0$) and isochoric ($dV = 0$) reversible process. Looking at the expression for $U$ we can use a Maxwell relation \begin{equation*}
    \frac{\partial T}{\partial A} = \frac{\partial \sigma}{\partial S} = \frac{\partial \sigma}{\partial T}\frac{\partial T}{\partial S} = \frac{d\sigma}{dT}\left(\frac{\partial S}{\partial T}\right)^{-1}
\end{equation*}
where we have used the chain rule together with the fact that $\sigma = \sigma(T)$. From the experimental form of $\sigma$ above we have $\frac{d\sigma}{dT} = -\frac{\alpha}{T_c}$. From the expression for the internal energy $dU = TdS - pdV + \sigma dA$ we have \begin{align*}
    dS &= \frac{1}{T}(dU+pdV-\sigma dA) = \frac{1}{T}\left[\frac{\partial U}{\partial T}dT + \frac{\partial U}{\partial V}dV + \frac{\partial U}{\partial A}dA + pdV - \sigma dA\right] \\
    &= \frac{1}{T}\left[\frac{\partial U}{\partial T}dT + \left(\frac{\partial U}{\partial V}+p\right)dV + \left(\frac{\partial U}{\partial A}-\sigma\right)dA\right] 
\end{align*}
Hence, we can read off \begin{equation*}
    \frac{\partial S}{\partial T} = \frac{1}{T}\frac{\partial U}{\partial T} = \frac{1}{T}C_{V,A}
\end{equation*}
Combining all of the above we get \begin{equation*}
    \frac{\partial T}{\partial A} = \frac{d\sigma}{dT} \frac{T}{C_{V,A}} = \frac{-\alpha T}{C_{V,A}T_c} < 0
\end{equation*}
Thus, the temperature decreases with increasing surface area. Provided $C_{V,A} = const$, we find that $T = T_0\exp\left\{-\gamma(A-A_0)\right\}$ with $\gamma \equiv \frac{\alpha}{C_{V,A}T_c} > 0$.

\begin{qst}
    How does the entropy change if $A$ changes?
\end{qst}

To ensure we don't have overlapping changes with other types of work, we need $V = const$ and since $\sigma = \sigma(T)$ we need $T = const$ to avoid having some changes in $S$ arising from changes in $\sigma$. So we need to find $\left(\frac{\partial S}{\partial A}\right)_{T,V}$. Since we are considering the thermodynamic coordinates $A,T,V$ we need to consider the Helmholtz free energy. Using $dU = TdS - pdV +\sigma dA$ we find \begin{equation*}
    dF = d(U-TS) = -SdT-pdV+\sigma dA
\end{equation*}
Then we can read off the Maxwell relation \begin{equation*}
    \frac{\partial S}{\partial A} = -\frac{\partial \sigma}{\partial T} = \frac{\alpha}{T_c} > 0
\end{equation*}
using the experimental expression for $\sigma$. Thus, for constant temperature and volume the thermodynamic entropy increases linearly with $A$.

\begin{qst}
    How does the internal energy change if $A$ changes for an isothermal and isochoric reversible process?
\end{qst}

We need to find $\left(\frac{\partial U}{\partial A}\right)_{T,V}$. From the expression for the internal energy we have that \begin{equation*}
    \left(\frac{\partial U}{\partial A}\right)_{T,V} = T\left(\frac{\partial S}{\partial A}\right)_{T,V} + \sigma = \alpha\frac{T}{T_c} + \alpha(1-T/T_c) = \alpha > 0 
\end{equation*}
where we have used the result of the previous question and the given expression for $\sigma$. Thus, again we have a linear increase.

\begin{qst}
    How much heat is necessary to change the surface area from $A_1$ to $A_2$ for an isothermal and isochoric reversible process?
\end{qst}
For a reversible process we have that $\delta Q = TdS$. For an isothermal process we only need to investigate the changes in entropy. Specifically, we need to find $S(T,A,V)$. We start with the Helmholtz free energy: \begin{equation*}
    dF = -SdT -pdV + \sigma dA
\end{equation*}
Specifically, we can see that \begin{equation*}
    \frac{\partial F}{\partial A} = \sigma = \sigma(T)
\end{equation*}
Integrating this expression gives \begin{equation*}
    F(T,V,A) = \sigma(T)A+F_V(T,V)
\end{equation*}
such that the Helmholtz free energy is the sum of a surface term $F_A(T,V) \equiv \sigma(T)A$, and a volume term $F_V(T,V)$. Using this expression we can obtain $S$: \begin{equation*}
    S = -\frac{\partial F}{\partial T} = -\frac{\partial F_A}{\partial T} - \frac{\partial F_V}{\partial T} = -A\frac{d\sigma}{dT} - \frac{\partial F_V}{\partial T}
\end{equation*}
Note only the first term depends on $A$, and the second term only depends on $V$ and $T$ which we are holding cosntant. Then, we have \begin{align*}
    \Delta Q &= T(S(T,V,A_2) - S(T,V,A_1)) = -T\left[\frac{\partial F_A}{\partial T}(T,A_2) - \frac{\partial F_A}{\partial T}(T,A_1)\right] \\
    &= -T\left(A_2\frac{d\sigma}{dT} - A_1\frac{d\sigma}{dT}\right) = \alpha\frac{T}{T_c}\left(A_2 - A_1\right)
\end{align*}
using the given expression for $\sigma$.


%%%%%%%%%%%%%%%%%%%% Section 1.4.2
\section{Approach to Equilibrium and Equilibrium Conditions}


We aim to show that each thermodynamic potential takes on an extermal value in equilibrium in situations where their natural variables are held constant. This is a consequence of the 2nd law.

\subsection{Isolated System}

In this case we have $dU = 0$, $dq_i = 0$, and $dN_j = 0$, which implies that $\delta Q = 0$ based on the first law. It follows that $dS \geq \frac{\delta Q}{T} = 0$ and $dS = 0$ in equilibrium. Thus, entropy is maximal in equilibrium, given by $S = S(U,\vec{q},\vec{N})$.

Now, let us consider a gas with a single particle type in a container that is subdivided into two compartments:
\begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,288); %set diagram left start at 0, and has height of 288

%Shape: Rectangle [id:dp8945252446537386] 
\draw   (140,80) -- (400,80) -- (400,200) -- (140,200) -- cycle ;
%Straight Lines [id:da19676236442724315] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (270,80) -- (270,200) ;
%Straight Lines [id:da5279386372303521] 
\draw    (270,50) -- (270,78) ;
\draw [shift={(270,80)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

% Text Node
\draw (171,131.4) node [anchor=north west][inner sep=0.75pt]    {$U_{1} ,V_{1} ,N_{1}$};
% Text Node
\draw (301,132.4) node [anchor=north west][inner sep=0.75pt]    {$U_{2} ,V_{2} ,N_{2}$};
% Text Node
\draw (226,32) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\fontfamily{ptm}\selectfont permeable wall}};


\end{tikzpicture}
\end{center}
Since we are dealing with an isolated system we have $U \equiv U_1+U_2 = const, V \equiv V_1+V_2 = const, N \equiv N_1+N_2 = const, S \equiv S_1(U_1,V_1,N_1)+S_2(U_2,V_2,N_2)$ such that different changes need to compensate, i.e., $dU_1 = -dU_2, dV_1 = -dV_2, dN_1 = -dN_2$. In equilibrium we also have \begin{align*}
    0 &= dS = dS_1+dS_2 \\
    &= \left[\frac{\partial S_1}{\partial U_1}-\frac{\partial S_2}{\partial U_2}\right]dU_1+\left[\frac{\partial S_1}{\partial V_1} - \frac{\partial S_2}{\partial V_2}\right]dV_1 + \left[\frac{\partial S_1}{\partial N_1} - \frac{\partial S_2}{\partial N_2}\right]dN_1 \\
    &= \left(\frac{1}{T_1} - \frac{1}{T_2}\right)dU_1 + \left(\frac{p_1}{T_1} - \frac{p_2}{T_2}\right)dV_1 - \left(\frac{\mu_1}{T_1} - \frac{\mu_2}{T_2}\right)dN_1
\end{align*}
For this expression to be true, all prefactors of the infinitesimal changes need to vanish since $U_1$, $V_1$, and $N_1$ are independent variables. Thus, we obtain that \begin{equation*}
    T_1 = T_2,\;\;\;p_1 = p_2,\;\;\;\mu_1 = \mu_2,
\end{equation*}
meaning that temperature, pressure, and chemical potential are identical in both compartments in equilibrium. Inductively continuing this process of splitting the compartments into smaller parts, and repeating the argument, we find that in an isolated system in equilibrium, temperature, pressure, and chemical potential are the same everywhere.

\subsection{Isolated System Coupled to a Heat Bath}

In this case, we have $dT = 0, dq_i = 0,$ and $dN_j = 0$, which corresponds to the natural variables of the Helmholtz free energy. Note that the heat bath is assumed to be sufficiently large such that its temperature fully controls the temperature of the system of interest. Starting from the second law and using the 1st law for this case we have \begin{equation*}
    TdS \geq \delta Q = dU - \sum_{i=1}^mJ_idq_i - \sum_{j=1}^{\alpha}\mu_jdN_j = dU,
\end{equation*}
SInce we also have $dT = 0$, we can rewrite this inequality as \begin{equation*}
    d(TS) = TdS \geq dU \iff d(U-TS) \leq 0 \iff dF \leq 0
\end{equation*}
Hence, for all irreversible processes under the condition that $T = const, \vec{q} = const, \vec{N} = const$, the Helmholtz free energy decreases. In equilibrium $dF = 0$ and $F$ is minimal.

Using a similar line of argument as the last case, we can show that pressure and chemical potential are everywhere the same in equilibrium in this case.

\subsection{Isolated System Coupled to a Heat Bath and Exchanging Work with Constant External Forces}

In this case we have $dT = 0, dJ_i = 0,$ and $dN_j = 0$, which corresponds to the natural variables of the Gibbs free energy. Starting from the second law and then using the 1st we have \begin{equation*}
    d(TS) = TdS \geq \delta Q = dU - \sum_{i=1}^mJ_idq_i - \sum_{j=1}^{\alpha}\mu_jdN_j = dU - d\left(\sum_{i=1}^mJ_iq_i\right)
\end{equation*}
Rewriting this expression we obtain \begin{equation*}
    dG = d\left(U - \sum_{i=1}^mJ_iq_i - TS\right) \leq 0
\end{equation*}
Hence, for all irreversible processes under the condition that $T = const, \vec{J} = const, \vec{N} = const$, the Gibbs free energy decreases. In equilibrium, $dG = 0$ and $G$ is minimal.

Again we can show that the chemical potential is everywhere the same in equilibrium in this case.

\subsection{Closed System Exchanging Work with Constant External Forces and Heat at Constant Entropy}

In this case we have $dS = 0, dJ_i = 0$ and $dN_j = 0$, which corresponds to the natural variables of the enthalpy. Starting from the 2nd law and then using the 1st law we have \begin{equation*}
    0 = TdS \leq \delta Q = dU - \sum_{i=1}^mJ_idq_i - \sum_{j=1}^{\alpha}\mu_jdN_j = d\left(U-\sum_{i=1}^mJ_iq_i\right) = d\mathcal{H}
\end{equation*}
Hence, for all irreversible processes under the condition that $S = const, \vec{J} = const, \vec{N} = const$, the enthalpy decreases. In equilibrium $d\mathcal{H} = 0$ and $\mathcal{H}$ is minimal.







%%%%%%%%%%%%%%%%%%%% Section 1.4.3
\section{The Third Law of Thermodynamics}

Recall our definition of thermodynamic entropy is only specified up to an additive constant. Empirical observations indicate that this constant is indeed constrained.

\begin{law}[Third Law of Thermodynamics (Nernst)]
    The entropy of all systems at absolute zero temperature is a universal constant that can be taken to be $0$, independent of the specific choice and value of the other state variables, for example $\lim\limits_{T\rightarrow 0}S(T,\vec{q},\vec{N}) = 0$ and $\lim\limits_{T\rightarrow 0}S(T,\vec{J},\vec{N}) = 0$.
\end{law}

There are a number of consequences to this formulation of the third law:

\begin{cor}
    All partial derivatives of the entropy with respect to any thermodynamic coordinate other than $T$ vanish at zero temperature.
\end{cor}
\begin{proof}
    Since $S(T=0,\vec{X}) = 0$, $\lim\limits_{T\rightarrow 0}\frac{\partial S}{\partial \vec{X}}\Bigg\rvert_T = 0$ for all thermodynamic coordinates $\vec{X}$.
\end{proof}

\begin{cor}
    All heat capacities vanish at zero temperature.
\end{cor}
\begin{proof}
    $C_{\vec{X}} = \left(\frac{\delta Q}{dT}\right)_{\vec{X}} = \left(T\frac{\partial S}{\partial T}\right)_{\vec{X}}$ implies that $$S(T,\vec{X}) - S(0,\vec{X}) = \int_0^TdT'\frac{C_{\vec{X}}(T')}{T'} + f(\vec{X})$$ Due to the third law, $S(0,\vec{X}) = 0$ and due to the first consequence above $f(\vec{X}) = 0$. To ensure that $S(T,\vec{X})$ is finite, the integral needs to be finite, which requires that $\lim\limits_{T\rightarrow 0}C_{\vec{X}}(T) = 0$.
\end{proof}

\begin{cor}
    All thermal expansitivities vanish at zero temperature.
\end{cor}
\begin{proof}
    $\beta_J \equiv \frac{1}{q}\frac{\partial q}{\partial T}\Bigg\rvert_{J} = \frac{1}{q}\frac{\partial S}{\partial J}\Bigg\rvert_T$, where we have used a Maxwell relation to obtain the last equality. Given the first consequence above, this implies that $\lim\limits_{T\rightarrow 0}\beta_J = 0$.
\end{proof}

\begin{cor}
    It is impossible to cool any system to absolute zero temperature.
\end{cor}

We explore an example since the general proof is beyond the scope of this course. Consider the situation when we cool a gas by alternating between an adiabatic reduction in pressure ($dS = 0$) and an isothermal compression ($dT = 0$). As the sketch shows, it would require infinitely many steps to cool the system to aboslue zero.

\begin{center}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398

%Shape: Axis 2D [id:dp5039424026544654] 
\draw  (50,268.5) -- (320,268.5)(77,75) -- (77,290) (313,263.5) -- (320,268.5) -- (313,273.5) (72,82) -- (77,75) -- (82,82)  ;
%Curve Lines [id:da6073591797218043] 
\draw    (77,268.5) .. controls (152.57,260.58) and (280.74,142.89) .. (299.47,91.54) ;
\draw [shift={(300,90)}, rotate = 468.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da5054543072909845] 
\draw    (77,268.5) .. controls (155.54,267.51) and (351.37,205.75) .. (388.91,171.04) ;
\draw [shift={(390,170)}, rotate = 495.14] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da8065266451670814] 
\draw    (400,140) -- (269,140) ;
\draw [shift={(267,140)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da09778382680957276] 
\draw    (267,140) -- (267,224) ;
\draw [shift={(267,226)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da9873396589141186] 
\draw    (267,226) -- (172,226.98) ;
\draw [shift={(170,227)}, rotate = 359.40999999999997] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da09559421901170295] 
\draw    (170,227) -- (170,252) ;
\draw [shift={(170,254)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da3352272051208962] 
\draw    (170,254) -- (126,254) ;
\draw [shift={(124,254)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da6899520073196863] 
\draw    (124,254) -- (124,261) ;
\draw [shift={(124,263)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da8555888912055112] 
\draw    (179,150) .. controls (179,194.98) and (221.3,183.63) .. (265,184.04) ;
\draw [shift={(267,184.07)}, rotate = 181.03] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

% Text Node
\draw (327,262.4) node [anchor=north west][inner sep=0.75pt]    {$T$};
% Text Node
\draw (61,62.4) node [anchor=north west][inner sep=0.75pt]    {$S$};
% Text Node
\draw (298,72.4) node [anchor=north west][inner sep=0.75pt]    {$P_{ >}$};
% Text Node
\draw (391,160.4) node [anchor=north west][inner sep=0.75pt]    {$P_{< }$};
% Text Node
\draw (103.22,266.39) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize,rotate=-306.87]  {$\ddots $};
% Text Node
\draw (123,123.4) node [anchor=north west][inner sep=0.75pt]    {$\underbrace{\delta Q=T\Delta S< 0}$};


\end{tikzpicture}    
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Part 2
\part{Statistical Mechanics}


%%%%%%%%%%%%%%%%%%%%%% Chapter 2.1
\chapter{Microcanonical Ensembles}

%%%%%%%%%%%%%%%%%%%% Section 2.1.1
\section{Rules for Large Numbers}

\begin{note}
    Statistical mechanics provides probabilistic approach to equilibrium macroscopic properties of systems with a large number of degrees of freedom based on the underlying microscopic dynamics. While it leaves out the question of how equilibrium is reached, it aims to provide the equilibrium phase space density $\rho_{eq,M}(\mu)$ for a given macrostate $M$.
\end{note}


Since we are dealing with large numbers of degrees of freedom, $N \approx 10^{23}$, we can often consider the \Emph{thermodynamic limit} $N\rightarrow \infty$. This lets us typically distinguish between three main cases: \begin{enumerate}
    \item \Emph{Intensive} quantities are independent of $N$, the size of a system (e.g. $T$, $p$),
    \item \Emph{Extensive} quantities are proportional to $N$, the size of a system (e.g. $E_{tot}$, $S$)
    \item \Emph{Exponential} dependence: $O(\exp(N\phi))$ (e.g. volume in phase space)
\end{enumerate}

\begin{defn}
    The \Emph{thermodynamic limit} of a system in statistical mechanics is the limit for a large number $N$ of particles where the volume is taken to grow in proportion with the number of particles. That is \begin{equation*}
        N\rightarrow \infty, V\rightarrow \infty,\;\text{ and }\;\frac{N}{V} = const
    \end{equation*}
    This says we can scale up the system without changing the intrinsic properties of the system.
\end{defn}

\begin{rmk}
    In the thermodynamic limit small perturbations and fluctuations in your ensemble average vanish, and all thermodynamic ensembles give the same results.
\end{rmk}

When considering the thermodynamic limit we also want that the intrinsic properties of the system do not change to a great degree, and that the particles are relatively evenly spread through the volume rather than being bunched into smaller structures.

\subsection{Saddle Point Integration}
Consider the integral \begin{equation*}
    J = \int_{-\infty}^{\infty}dx\exp\{N\phi(x)\}
\end{equation*}
For this integral to be well-defined the function $\phi(x)$ has to be continuous and go to $-\infty$ for large positive and negative arguments. Thus, there is at least one global maximum $\phi(x_{max})$. Assuming that the global maximum is unique, let us do a Taylor series expansion of $\phi$ around $x_{max}$ to obtain \begin{align*}
    J &= \int_{-\infty}^{\infty}\exp\left\{N\left[\phi(x_{max}) - \frac{1}{2}|\phi''(x)|(x-x_{max})^2+O((x-x_{max})^3)\right]\right\} \\
    &\approx \exp\{N\phi(x_{max})\}\int_{-\infty}^{\infty}\exp\left\{-\frac{N}{2}|\phi''(x)|(x-x_{max})^2\right\} \\
    &= \exp\{N\phi(x_{max})\}\sqrt{\frac{2\pi}{N|\phi''(x_{max})|}}
\end{align*}
where we have used the properties of a maximum, $\phi'(x_{max}) = 0$ and the normalization to do the Gaussian integral. We can simplify further by \begin{equation*}
    \lim\limits_{N\rightarrow \infty}\frac{\ln J}{N} = \lim\limits_{N\rightarrow \infty}\left[\phi(x_{max}) - \frac{1}{2N}\ln\frac{N|\phi''(x_{max})|}{2\pi}\right] = \phi(x_{max})
\end{equation*}
which is an intensive quantity. Thus, in this case the value of the integral only depends on the maximum value of the integrand.


%%%%%%%%%%%%%%%%%%%% Section 2.1.2
\section{Microcanonical Ensemble}

Recall, statistical mechanics aims to provide the equilibrium phase space density $\rho_{eq,M}(\mu)$ for a given macrostate $M$. Now, for the case of an isolated system the internal energy $U$, the generalized displacements $\vec{q}$, and the number of all particles of different types $\vec{N}$ are fixed such that no mechanical work and no chemical work are done and no heat is exchanged either. Thus our macrostate is $M = (U,\vec{q},\vec{N})$, and the corresponding statistical ensemble is called the \Emph{microcanonical ensemble}. The evolution of the corresponding microstates $\mu(t)$ is given by the Hamiltonian $H(\mu)$. Since the internal energy is conserved, all allowed microstates are constrained to the surface $H(\mu)  = U$ in phase space.

\begin{axi}[Central Postulate of Statistical Mechanics] 
    For an isolated system, all allowed states are equally likely: \begin{equation*}
        \rho_{eq,M}(\mu) \equiv p_{(U,\vec{q},\vec{N})}(\mu) = \frac{1}{\Omega(U,\vec{q},\vec{N})}\delta(H(\mu) - U)
    \end{equation*}
    where $\Omega$ is the normalization constant corresponding to the (abstract) area of the constant energy surface or the number of allowed microstates in the discrete case.
\end{axi}


\begin{thm}
    For the microcanonical ensemble in the thermodynamic limit, the \Emph{Shannon-Boltzmann} entropy defined as \begin{equation*}
        S(U,\vec{q},\vec{N}) \equiv -k_B\langle \ln p_{(U,\vec{q},\vec{N})}\rangle = k_B\ln\Omega(U,\vec{q},\vec{N})
    \end{equation*}
    is equivalent to the thermodynamic entropy and allows us to derive the laws of thermodynamics (excluding the 3rd one).
\end{thm}
\begin{proof}
    We consider the proof in parts:
    \begin{itemize}
        \item Extensivity of the Shannon-Boltzmann entropy: For a collection of independent subsystems, we have $\Omega_{total} = \prod_i\Omega_i$. Thus $S(U,\vec{q},\vec{N}) = k_B\sum_i\ln\Omega_i$ is additive, consistent with an extensive quantity.
        \item $0$th Law: Let us consider two substystems that are initially isolated. Then, we couple them thermally such that they exchange heat but no work. The combined system is isolated such that $U \equiv U_1+U_2 = const$ and eventually reaches equilibrium, corresponding to the microcanonical ensemble. Denote the state of the combined system as $\mu$, which we can write as the product state $\mu \equiv \mu_1\otimes \mu_2$. Under the assumption that the interactions between the subsystems are weak, the total energy is simply the energy contained within each subsystem, $H(\mu_1\otimes \mu_2) = H_1(\mu_1) + H_2(\mu_2)$. Focusing on the internal energy $U$ since it is the only variable that can change internally between the two subsystems due to the thermal coupling, we have \begin{equation*}
                p_U(\mu) = \frac{1}{\Omega(U)}\delta(H(\mu) - U) = \frac{1}{\Omega(U)}\delta(H_1(\mu_1)+H_2(\mu_2) - U)
        \end{equation*}
            Under the same assumption, the states in subsystem $1$ and $2$ are effectively independent of one another such that $\Omega(U)$ can be calculated as follows: \begin{equation*}
                \Omega(U) = \int dU_1\Omega_1(U_1)\Omega_2(U-U_1) = \int dU_1\exp\left\{\frac{S_1(U_1) + S_2(U-U_1)}{k_B}\right\}
            \end{equation*}
            Since $S_1$ and $S_2$ are extensive, we can use the saddle point integration to obtain the approximation \begin{equation*}
                S(U) = k_B\ln\Omega(U) \approx S_1(U_1^*) + S_2(U-U_1^*)
            \end{equation*}
            where $U_1^*$ is the internal energy that maximizes the entropy, so \begin{equation*}
                \frac{\partial}{\partial U_1}(S_1(U_1) + S_2(U-U_1))\Big\rvert_{U_1 = U_1^*} = 0
            \end{equation*}
            Using $U_2 = U-U_1$ and the chain rule we have \begin{equation*}
                \left(\frac{\partial S_1}{\partial U_1}\right)_{\vec{q},\vec{N}} = -\left(\frac{\partial S_2}{\partial U_2}\right)_{\vec{q},\vec{N}}\left(\frac{\partial U_2}{\partial U_1}\right)_{\vec{q},\vec{N}} = \left(\frac{\partial S_2}{\partial U_2}\right)_{\vec{q},\vec{N}}
            \end{equation*}
            Since the left hand side only depends on subsystem $1$ and the right hand side only depends on subsystem 2, we have arrived at an empirical temperature that is shared by both subsystems. In particular, the expressions are consistent with the expressions for the thermodynamic entropy: $\left(\frac{\partial S}{\partial U}\right)_{\vec{q},\vec{N}} = \frac{1}{T}$
        \item $2$nd Law: By construction of the saddle point integration approximation, we have for all $U_1$ and $U_2 = U-U_1$ including the initial conditions before coupling, \begin{equation*}
                S(U) \approx S_1(U_1^*) + S_2(U_2^*) \geq S_1(U_1) + S_2(U_2)
        \end{equation*}
            Rewriting this expression we have for the entropy difference \begin{align*}
                \delta S \equiv S_1(U_1^*) + S_2(U_2^*) - S_1(U_1) - S_2(U_2) &\geq 0
                S_1(U_1^*) - S_1(U_1) + S_2(U_2^*) - S_2(U_2) &\geq 0
                \left[\left(\frac{\partial S_1}{\partial U_1}\right)_{\vec{q},\vec{N}} - \left(\frac{\partial S_2}{\partial U_2}\right)_{\vec{q},\vec{N}}\right] \delta U_1 &\geq 0
                \left[\frac{1}{T_1} - \frac{1}{T_2}\right] \delta U_1 & \geq 0
            \end{align*}
            The first equation implies the entropy has increased and the last implies that heat or energy flows from the hotter to the colder subsystem. Note however that this is only a strict law in the thermodynamic limit when the saddle point integration becomes exact. For finite systems, fluctuations are possible but they are exponentially suppressed, $O(\exp(-\alpha N))$.
        \item $1$st Law: Consider reversible changes leave the entropy $S(U,\vec{q},\vec{N})$ unchanged: \begin{align*}
                0 = \delta S &\equiv S(U+\vec{J}\cdot d\vec{q}+\vec{\mu}\cdot d\vec{N},\vec{q}+d\vec{q},\vec{N}+d\vec{N}) - S(U,\vec{q},\vec{N}) \\
                &= \left[\left(\frac{\partial S}{\partial U_1}\right)_{\vec{q},\vec{N}}\vec{J} + \left(\frac{\partial S}{\partial \vec{q}}\right)_{U,\vec{N}}\right]\cdot \delta \vec{q} + \left[\left(\frac{\partial S}{\partial U}\right)_{\vec{q},\vec{N}}\vec{\mu} + \left(\frac{\partial S}{\partial \vec{N}}\right)_{U,\vec{q}}\right]\cdot \delta \vec{N}
        \end{align*}
            Since the changes $\delta \vec{N}$ and $\delta \vec{q}$ are independent of each other, both terms in angular brackets have to vanish separately. Thus we have \begin{align*}
                \left(\frac{\partial S}{\partial q_i}\right)_{U,q_{j\neq i},\vec{N}} &= -\left(\frac{\partial S}{\partial U}\right)_{\vec{q},\vec{N}}J_i = -\frac{J_i}{T} \\
                \left(\frac{\partial S}{\partial N_i}\right)_{U,N_{j\neq i},\vec{q}} &= -\left(\frac{\partial S}{\partial U}\right)_{\vec{q},\vec{N}}\mu_i = -\frac{\mu_i}{T}
            \end{align*}
            Thus we can write the total differential of the Shannon-Boltzmann entropy as \begin{equation*}
                dS(U,\vec{q},\vec{N}) = \frac{dU}{T} - \frac{\vec{J}\cdot d\vec{q}}{T} - \frac{\vec{\mu}\cdot d\vec{N}}{T}
            \end{equation*}
            so the Shannon-Boltzmann entropy is equivalent to the thermodynamic entropy, and we revocer the 1st law by rewriting the above equation: \begin{equation*}
                dU = TdS + \vec{J}\cdot d\vec{q}+\vec{\mu}\cdot d\vec{N}
            \end{equation*}
    \end{itemize}
\end{proof}

Note the above theorem holds only in the thermodynamic limit, so the Shannon-Boltzmann entropy is problematic in the context of the microcanonical ensemble outside the thermodynamic limit. An alternative in such a case is the Gibbs entropy, $S(U,\vec{q},\vec{N}) = k_B\ln\Omega(E< U,\vec{q},\vec{N})$

\subsection{Determining Thermodynamic Properties}

We have the following recipe for deriving the thermodynamic properties of a given system in a microcanonical setting in the thermodynamic limit: \begin{itemize}
    \item Specify the Hamiltonian $H$
    \item Compute $\Omega$
    \item Get $S = k_B\ln\Omega$, which is the thermodynamic potential suitable for an isolated system
    \item Use $S$ to determine $T$ and all other thermodynamic quantities through partial derivatives using the first law.
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%% Chapter 2.2
\chapter{Ensemble Theory}


%%%%%%%%%%%%%%%%%%%% Section 2.2.1
\section{Canonical Ensemble}


We now consider the situation which is thermally coupled to a heat bath or reservoir such that it only exchanges heat but no work. Hence the generalized siplacements $\vec{q}$ are fixed and the number of particles of different types $\vec{N}$ are also fixed. Although the internal energy $U$ is no longer fixed, at equilibrium both the heat bath and our system of interest share the same temperature $T$. We usually assume that the heat bath is much larger than the system of interest such that our system of interest basically takes on the temperature $T$ of the heat bath and the heat bath can be used to control the temperature of the system.

In this case our macrostate is $M \equiv (T,\vec{q},\vec{N})$. The corresponding ensemble is called the \Emph{canonical ensemble}. To derive the equilibrium phase space density we start by considering the combined system ($S$ and the heat bath $R$) which is isolated such that we can use the microcanonical ensemble. If we assume that the coupling between the heat bath and the system is weak such that the total internal energy of the combined system is $U_{tot} = H_{S\otimes R}(\mu) \approx H_S(\mu_S) + H_R(\mu_R)$ such that \begin{equation*}
    \rho_{eq,M}(\mu) \equiv p_{(T,\vec{q},\vec{N})}(\mu) \equiv p(\mu_S\otimes\mu_R) = \frac{1}{\Omega_{S\otimes R}(U_{tot})}\delta(H_S(\mu_S)+H_R(\mu_R)-U_{tot})
\end{equation*}
This gives the joint pdf of the system $S$ and the heat bath $R$. To determine the unconditional pdf for $S$ we integrate: \begin{align*}
    p(\mu_S) &\equiv \int d\Gamma_Rp(\mu_S\otimes\mu_R) \\
    &= \int d\Gamma_R \frac{1}{\Omega_{S\otimes R}(U_{tot})}\delta(H_S(\mu_S)+H_R(\mu_R)-U_{tot}) \\
    &= \frac{\Omega_R(U_{tot} - H_S(\mu_S))}{\Omega_{S\otimes R}(U_{tot})} \\ 
    &= \frac{\exp\left\{\frac{1}{k_B}S_R(U_{tot}-H_S(\mu_S))\right\}}{\Omega_{S\otimes R}(U_{tot})}  \\
    &\propto\exp\left\{\frac{1}{k_B}S_R(U_{tot}-H_S(\mu_S))\right\}
\end{align*}
using the Shannon-Boltzmann entropy ($S = k_B\ln\Omega$). Since we assume the heat bath $R$ is much larger than our system $S$, $U_{tot} \approx U_R >> U_S = H_S(\mu_S)$, and we can do a Taylor expansion of $S_R$ about $U_{tot}$: \begin{align*}
    S_R(U_{tot}-H_S(\mu_S)) &\approx S_R(U_{tot}) - \left(\frac{\partial S_R}{\partial U_R}\right)_{\vec{q},\vec{N}}(U_tot) H_S(\mu_S) \\
    &\approx S_R(U_{tot}) - \left(\frac{\partial S_R}{\partial U_R}\right)_{\vec{q},\vec{N}}(U_R)H_S(\mu_S) \\
    &= S_R(U_{tot}) - \frac{1}{T}H_S(\mu_S)
\end{align*}
Plugging this expression in the equation above we arrive at \begin{equation*}
    p(\mu_S) = \frac{e^{-\beta H_S(\mu_S)}}{Z(T,\vec{q},\vec{N})}
\end{equation*}
where we define $\beta \equiv \frac{1}{k_BT}$, and $Z(T,\vec{q},\vec{N}) \equiv \int d\Gamma_Se^{-\beta H_S(\mu_S)}$ is the normalization constant, called the \Emph{partition function}. Since the derived expression for the equilibrium phase space density of the canonical ensemble is a function of $H_S$, it satisfies Liouville's Theorem. In particular, all microstates with the same energy are equally likely in the canonical ensemble.

We can also derive an expression analogous to the equilibrium phase space density using the maximum entropy principle with the constraint that $\langle U_S\rangle$ is fixed. Let $E_{mech} = \langle U_S\rangle$. Then we have the two constraints $g_1(p) \equiv 1 - \int d\Gamma_Sp(\mu_S) = 0$ and $g_2(p) \equiv E_{mech} - \int d\Gamma_SH(\mu_S)p(\mu_S) = 0$ such that two Lagrange multipliers are necessary. 


We now focus on other quantities of interest in the canonical ensemble starting with the internal energy, which can be considered as a random variable since it is not fixed. We can calculate the pdf of $U_S$ by \begin{align*}
    p(U_S) &= \int d\Gamma_Sp(\mu_S)\delta(H_S(\mu_S) - U_S) \\
    &= e^{-\beta U_S}\frac{\Omega_S(U_S)}{Z} \\
    &= \frac{1}{Z}\exp\left\{-\frac{U_S}{k_BT}\right\}\exp\left\{\frac{S(U_S)}{k_B}\right\} \\
    &= \frac{1}{Z}e^{-\beta F(U_S)} 
\end{align*}
using the Helmholts free energy $F = U_S - TS(U_S)$. Thus, the most likely value of $U_S$ is the one that minimizes $F$, which is the thermodynamic potential coinciding with the natural variables, i.e. the macrostate of the canonical ensemble.

The mean internal energy can be given by \begin{align*}
    \langle U_S\rangle &\equiv \int d\Gamma_SH(\mu_S)\frac{e^{-\beta H_S(\mu_S)}}{Z} \\
    &= -\frac{1}{Z}\frac{\partial}{\partial \beta}\int d\Gamma_Se^{-\beta H_S(\mu_S)} \\
    &= - \frac{1}{Z}\frac{\partial Z}{\partial \beta} \\
    &= -\frac{\partial \ln Z}{\partial \beta}
\end{align*}
using the fact that the boundaries of the integral do not depend on $\beta$. Note that $\ln Z$ gives \begin{align*}
    \ln Z &\equiv \ln\int d\Gamma_Se^{-\beta H_S(\mu_S)} \\
    &= \ln\int dU_S\int d\Gamma_Se^{-\beta H_S(\mu_S)}\delta(H_S(\mu_S) - U_S) \\
    &= \ln\int dU_Se^{-\beta F(U_S)} \\
    &\approx -\beta F(U^*_S)
\end{align*}
where we have first used the expression of $p(U_S)$ derived above and then the saddle point itnegration such taht $U^*_S$ is the value of the internal energy that minimizes $F(U_S)$. Hence we have arrived at \begin{equation*}
    F(T,\vec{q},\vec{N}) \approx -k_BT\ln Z(T,\vec{q},\vec{N})
\end{equation*}
which is true only if $U_S^* \approx \langle U\rangle$. This holds as $p(U)$ obeys the central limit theorem for $N\rightarrow \infty$ such that $U_S^* = \langle U\rangle$ in the limit.

The expression for the Shannon-Boltzmann entropy in the canonical ensemble is \begin{equation*}
    S \equiv -k_B\langle \ln p\rangle = -k_B\langle(-\beta H_S - \ln Z)\rangle = k_B(\langle \beta H_S\rangle + \langle \ln Z\rangle) = \frac{\langle U\rangle}{T} - k_B\beta F = \frac{\langle U\rangle - F}{T}
\end{equation*}
which is the expression for the thermodynamic entropy. Note we use $\langle U\rangle$ instead of $U$ since the internal energy can fluctuate due to the coupling with the heat bath.


%%%%%%%%%%%%%%%%%%%% Section 2.2.2
\section{Overview of Ensembles}

For all types of ensembles, statistical mechanics provides an expression for the corresponding thermodynamic potential, which is the one whose natural variables are held constant in the given thermodynamic situation as specified by the macrostate $M$. In all cases the normalization constant of the corresponding equilibrium phase space density determines the thermodynamic potential. Thus, to determine the thermodynamic properties of a given system in equilibrium one needs to compute the normalization constant. 

\begin{rmk}[Generalized Method]
    \leavevmode
    \begin{itemize}
        \item Specify the microstate of the given system
        \item Specify the Hamiltonian $H$
        \item Specify the macrostate for the given situation to determine the ensemble
        \item Compute the relevant normalization constant for the given ensemble
        \item Use the normalization constant to obtain the relevant thermodynamic potential for the given ensemble
        \item Use the thermodynamic potential to determine all thermodynamic quantities through partial derivatives using the 1st law.
    \end{itemize}
\end{rmk}

Important examples of ensembles include:

\begin{table}[H]
    \centering
    \caption{Ensembles}
    \begin{tabular}{c|ccc}
        Ensemble & Macrostate $M$ & $p_M(\mu)$ & Thermodynamic Potential \\ \hline
        Microcanonical & $(U,\vec{q},\vec{N})$ & $\frac{\delta(H(\mu) - U)}{\Omega}$ & $S(U,\vec{q},\vec{N}) = k_B\ln \Omega$ \\
        Canonical & $(T,\vec{q},\vec{N})$ & $\frac{e^{-\beta H(\mu)}}{Z}$ & $F(T,\vec{q},\vec{N}) = -k_BT\ln Z$ \\
        Gibbs Canonical & $(T,\vec{J},\vec{N})$ & $\frac{e^{-\beta(H(\mu) - \vec{J}\cdot \vec{q})}}{Z_G}$ & $G(T,\vec{J},\vec{N}) = -k_BT\ln Z_G$ \\
        Grand Canonical & $(T,\vec{q},\vec{\mu})$ & $\frac{e^{-\beta(H(\mu)-\vec{\mu}\cdot \vec{N})}}{Q}$ & $\mathcal{G}(T,\vec{q},\vec{\mu}) = -k_BT\ln Q$ \\
    \end{tabular}
\end{table}

We close with some important remarks:

\begin{rmk}
    \leavevmode
    \begin{itemize}
        \item For any system with a finite number of particles or constituents, the ensembles are not equivalent
        \item In the thermodynamic limit, however, they do become equivalent as follows from the central limit theorem.
        \item Thus, to solve a given problem in the thermodynamic limit, one is free to choose any ensemble. In this case, the canonical ensemble is often the mathematically most convenient choice.
    \end{itemize}
\end{rmk}


%%%%%%%%%%%%%%%%%%%% Section 2.2.3
\section{Examples of Ensembles}


\subsection{Pauli Paramagnet or the Two-Level System}

This system corresponds to a collection of atomic magnetic spins with two possible orientations (for example $N$ atoms in a crystal with a single valence electron or $N$ impurity atoms trapped in a solid matrix with two states each). The microstate of such a system can be described by the set of occupation numbers $\mu = \{n_i\}$ where each occupation number is either $0$ or $1$ corresponding to the two possible states.

How can we obtain a Hamiltonian in this case? Our starting point is that the Hamiltonian is equal to the internal energy. For the system here each orientation corresponds to a different energy level - for example $e_{down} = 0$ and $e_{up} = \epsilon$. Thus we can write the Hamiltonian as \begin{equation*}
    H = \sum_{i=1}^Nn_i\epsilon_i
\end{equation*}
neglecting any interactions between the different spins. This can be justified as a reasonable first order approximation by the fact that we consider a paramagnet here, which has the property that there is no macroscopic net magnetization in the absence of an external magnetic field. So we can treat the different spins as independent.

The above Hamiltonian is the simplest model of a two-level system, often called a \Emph{Pauli paramagnet}.

We need to determine the macrostate and the ensemble, while we are considering the thermodynamic limit so we are free to choose any ensemble. We use the canonical ensemble in the following such that the macrostate is $M \equiv (T,N)$. The phase space density is given by \begin{equation*}
    p(\{n_i\}) = \frac{1}{Z}e^{-\beta\varepsilon \sum_{i=1}^Nn_i}
\end{equation*}
For the partition function we then have \begin{equation*}
    Z(T,N) = \sum_{n_1=0}^1\cdots \sum_{n_N=0}^1e^{-\beta \epsilon\sum_{i=1}^Nn_i} = \left[\sum_{n_1=0}^1e^{-\beta \epsilon n_1}\right]\cdots \left[\sum_{n_N=0}^1e^{-\beta \epsilon n_N}\right] = (1+e^{-\beta \epsilon})^N
\end{equation*}

Now, we can find the probability that a single spin is in a specific orientation using the unconditional probability \begin{equation*}
    p_1(n_1) \equiv \sum_{n_2=0}^1\cdots \sum_{n_N=0}^1p(\{n_i\}) = \sum_{n_2=0}^1\cdots \sum_{n_N=0}^1\frac{1}{Z}e^{-\beta \epsilon\sum_{i=1}^Nn_i} = \frac{e^{-\beta \epsilon n_1}}{1+e^{-\beta \epsilon}}
\end{equation*}
Analogous expressions hold for all other spins. Comparison with the expression for $p(\{n_i\})$ shows that we can write $p(\{n_i\}) = \prod_{i=1}^Np_i(n_i)$. This indicates that the different spins are independent as expected.

We now investigate the relevant thermodynamic potential, which is the Helmholtz free energy for the canonical ensemble: \begin{equation*}
    F(T,N) = -k_BT\ln Z = -k_BT\ln (1+e^{-\beta \epsilon})^N = -Nk_BT\ln(1+e^{-\beta \epsilon})
\end{equation*}
Recall that the exact differential of the Helmholtz free energy is $dF = dU-d(ST) = \vec{J}\cdot d\vec{q} + \vec{\mu}\cdot d\vec{N}-SdT$. We first determine the entropy by \begin{equation*}
    S(T,N) = -\left(\frac{\partial F}{\partial T}\right)_N = Nk_B\ln(1+e^{-\epsilon/k_BT}) + nK_bT\frac{e^{-\epsilon/k_BT}\frac{\epsilon}{k_BT^2}}{1+e^{-\epsilon/k_BT}} = -\frac{F(T,N)}{T} + \frac{N\epsilon}{T}\frac{1}{1+e^{\epsilon/k_BT}}
\end{equation*}
For the internal energy, we can use the expression for the entropy to get \begin{equation*}
    \langle U \rangle = F + TS = \frac{N\epsilon}{1+e^{\epsilon/k_BT}}
\end{equation*}

Recall we could have also calculated it from $\langle U \rangle = -\frac{\partial \ln Z}{\partial \beta}$. Now, in the low temperature limit, $T\rightarrow 0$, we see that $\langle U\rangle \rightarrow 0$, which means all spins are in the lower energy state such that the system is in its ground state. Conversely, in the high temperature limit, $T\rightarrow \infty$, $\langle U \rangle \rightarrow \frac{N\epsilon}{2}$, which means that on average half of the spins are in the higher energy state.

For the heat capacities we only have $C_N$ for the Pauli paramagnet since the macrostate doesn't involve any generalized coordinates. So we find \begin{equation*}
    C_N = \left(\frac{\partial U}{\partial T}\right)_N = \left(\frac{\partial \langle U\rangle}{\partial T}\right)_N = \frac{N\epsilon^2}{k_BT^2}\frac{e^{\epsilon/k_BT}}{(1+e^{\epsilon/k_BT})^2}
\end{equation*}
In the limit $T\rightarrow 0$, we have $C_N\rightarrow 0$, consistent with the 3rd law. In the limit $T\rightarrow \infty$, we have $C_N\rightarrow 0$ again.


\subsection{Ideal Gas in the Canonical Ensemble}

The microstate for a gas is given by $\mu = \{\vec{q},\vec{p}\}$. Our main assumption for an ideal gas is that the interactions between the particles can be neglected, so we do not take into account any interaction potential. We also assume the particles can be treated as point particles such that they have no rotational or vibrational energy. Thus, the Hamiltonian has only two contributions, the translational kinetic energy of all particles and a confining potential that captures the interactions of the particles with the wall of the container: \begin{equation*}
    H = \sum_{i=1}^N\left[\frac{\vec{p}_i\cdot\vec{p}_i}{2m} + \tilde{U}(\vec{q}_i)\right]
\end{equation*}
where $m$ is the mass of the particles and the confining potential $\tilde{U}$ is assumed to be zero inside the container and infinite outside.

In the canonical ensemble, the macrostate is then $M \equiv (T,V,N)$ and the equilibrium phase space density takes on the following form: \begin{equation*}
    p(\{\vec{q}_i,\vec{p}_i\}) = \frac{1}{Z}\exp\left\{-\beta\sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}\right\}\delta(\{\vec{q}_i\} \in container)
\end{equation*}
For the partition function we then find \begin{align*}
    Z(T,V,N) &= \int d\Gamma\exp\left\{-\beta\sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}\right\}\delta(\{\vec{q}_i\} \in container) \\
    &= \int \prod_{i=1}^Nd\vec{q}_id\vec{p}_i\exp\left\{-\beta\sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}\right\}\delta(\{\vec{q}_i\} \in container) \\
    &= \int_Vd\vec{q}_1\cdots \int_Vd\vec{q}_N\int_{-\infty}^{\infty}d\vec{p}_1\cdots \int_{-\infty}^{\infty}d\vec{p}_N\exp\left\{-\beta\sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}\right\} \\
    &=\prod_{i=1}^N\left(V\int_{-\infty}^{\infty}d\vec{p}_ie^{-\beta\vec{p}_i\cdot\vec{p}_i/2m}\right) \\
    &= \prod_{i=1}^N\left(V\sqrt{2\pi mk_BT}^3\right) = \left(\frac{V}{\lambda(T)^3}\right)^N
\end{align*}
where $\lambda(T) = \frac{1}{\sqrt{2\pi mk_BT}}$, which can be considered a characteristic ``length."

Now for the Helmholtz free energy, we obtain: \begin{equation*}
    F(T,V,N) = -k_BT\ln Z = -Nk_BT\ln\frac{V}{\lambda^3}
\end{equation*}
For the internal energy we find \begin{equation*}
    \langle U\rangle = -\frac{\partial\ln Z}{\partial \beta} = N\frac{3}{2}\beta = \frac{3}{2}Nk_BT
\end{equation*}
Thus, as in the case experimentally, we find that the internal energy does not depend on $V$. Note in the thermodynamic limit the fluctuations around $\langle U \rangle$ vanish due to the central limit theorem, such that there is no difference between $U$ and $\langle U \rangle$.

We can compute the pressure from the Helmholtz free energy by \begin{equation*}
    P(T,V,N) = -\left(\frac{\partial F}{\partial V}\right)_{T,N} = Nk_BT\frac{1}{V}
\end{equation*}
and we arrive at the equation of state for an ideal gas.

To derive the kinetic energy in terms of temperature, let us consider the unconditional pdf that particle $1$ has a specific momentum $\vec{p}_1$. By definition we have \begin{align*}
    p(\vec{p}_1) &= \int_Vd\vec{q}_1\cdots\int_Vd\vec{q}_N\int_{-\infty}^{\infty}d\vec{p}_2\cdots...\int_{-\infty}^{\infty}d\vec{p}_Np(\{\vec{q}_i,\vec{p}_i\}) \\
    &= \frac{V}{Z}e^{-\beta\vec{p}_1\cdot\vec{p}_1/2m}\prod_{i=2}^N(V[2\pi mk_BT]^{3/2}) \\
    &= \frac{e^{-\beta\vec{p}_1\cdot\vec{p}_1/2m}}{(2\pi mk_BT)^{3/2}}
\end{align*}
which is a three dimensional Gaussian distribution in $\vec{p}$, with $\langle p_x\rangle = \langle p_y\rangle = \langle p_z\rangle = 0$ and $\langle p_x^2\rangle = \langle p_y^2\rangle = \langle p_z^2\rangle = mk_BT$. Then the average kinetic energy is \begin{equation*}
    \langle E_{kin}\rangle = \frac{1}{2m}\sum_{i=1}^N(\langle p_x^2\rangle + \langle p_y^2\rangle + \langle p_z^2\rangle) = \frac{1}{2m}\sum_{i=1}^N(3mk_BT) = \frac{3}{2}Nk_BT = U
\end{equation*}


\subsection{Maxwell-Boltzmann distribution for speed}

The pdf that a given particle has a specific momentum $mv$ is \begin{align*}
    p(mv) &= \int_{-\infty}^{\infty}dp_x\int_{-\infty}^{\infty}dp_y\int_{-\infty}^{\infty}dp_zp(\vec{p})\delta(|\vec{p}|-mv) \\
    &= \int_0^{\infty}dp_r\int_0^{\pi}dp_{\theta}\int_0^{2\pi}dp_{\phi}p_r^2\sin p_{\theta}\frac{1}{(2\pi mk_BT)^{3/2}}e^{-\beta p_r^2/2m}\delta(p_r - mv) \\
    &= \frac{1}{(2\pi mk_BT)^{3/2}}(mv)^2e^{-\beta(mv)^2/2m}\int_0^{\pi}dp_{\theta}\int_0^{2\pi}dp_{\phi}\sin p_{\theta} \\
    &= \frac{4\pi m^2v^2}{(2\pi mk_BT)^{3/2}}e^{-\beta mv^2/2}
\end{align*}    

Now, with a change of variables we have \begin{equation*}
    p_V(v) = p(mv)\left|\frac{d(mv)}{dv}\right| = \left(\frac{m}{2\pi k_BT}\right)^{3/2}4\pi v^2e^{-\beta mv^2/2}
\end{equation*}
to obtain the pdf that a particle has a specific speed $v$. Note that this is no longer a Gaussian, but instead is asymmetric.


\subsection{Ideal Gas in the Microcanonical Ensemble}

We repeat the previous analysis in the microcanonical ensemble. In the microcanonical ensemble the macrostate is $M \equiv (U,V,N)$ and the equilibrium phase space density takes the form \begin{equation*}
    p(\{\vec{q}_i,\vec{p}_i\}) = \frac{1}{\Omega}\delta\left(U - \sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}\right)\delta(\{\vec{q}_i\}\in container)
\end{equation*}
For the normalization constant we have \begin{align*}
    \Omega(U,V,N) &= \int\prod_{i=1}^Nd\vec{q}_id\vec{p}_i\delta\left(U - \sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}\right)\delta(\{\vec{q}_i\}\in container) \\
    &= V^N\int_{-\infty}^{\infty}\prod_{i=1}^Nd\vec{p}_i\delta\left(U - \sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}\right)
\end{align*}
The delta function enforces $2mU \equiv \sum_{i=1}^N\vec{p}_i\cdot\vec{p}_i$. This describes the surface of a $3N$-dimensional sphere of radius $R = \sqrt{2m U}$. This surface has area given by $S_{3N}R^{3N-1}$, where the \Emph{generalized solid angle} is given by $S_d = \frac{2\pi^{d/2}}{(d/2-1)!}$. Note that if the argument of the factorial is not an integer, the analytical extension of the factorial to real numbers has to be used, which is the Gamma function: \begin{equation*}
    N! = \int_0^{\infty}dx x^Ne^{-x} \equiv \Gamma(N+1)
\end{equation*}
Together we have that \begin{equation*}
    \Omega(U,V,N) = V^N\frac{2\pi^{3N/2}}{(3N/2-1)!}(2mU)^{(3N-1)/2}
\end{equation*}
Taking advantage of $N >>1$ we can compute the entropy: \begin{align*}
    S(U,V,N) &= k_B\ln\Omega(U,V,N) \\
    &= k_B\left[N\ln V + \ln 2 + 3N/2\ln \pi - \ln([3N/2-1]!) + (3N-1)/2\ln(2mU)\right] \\
    &\approx k_BN[\ln V+3/2\ln\pi - 3/2\ln(3N/2-1) + 3/2\ln 3 + 3/2\ln(2mU)] \\
    &\approx k_BN\ln\left[V\left(\frac{4\pi emU}{3N}\right)^{3/2}\right]
\end{align*}
From this we can calculate other thermodynamic properties such as the temperature: \begin{equation*}
    \frac{1}{T} = \left(\frac{\partial S}{\partial U}\right)_{N,V} = k_BN3/2\frac{1}{U}
\end{equation*}
Rewriting this expression we once again attain $U = \frac{3}{2}Nk_BT$, from which we can obtain the heat capacity: \begin{equation*}
    C_V = \left(\frac{\partial U}{\partial T}\right)_V = \frac{3}{2}Nk_B
\end{equation*}
For the pressure we have \begin{equation*}
    \frac{p}{T} = \left(\frac{\partial S}{\partial V}\right)_{U,N} = k_BN\frac{1}{V}
\end{equation*}
so $pV = Nk_BT$.




%%%%%%%%%%%%%%%%%%%%%% Chapter 2.3
\chapter{Gibbs Canonical Ensemble}


%%%%%%%%%%%%%%%%%%%% Section 2.3.1
\section{Mixing Entropy and Gibbs Paradox}

We now wish to address the question of what happens if we mix two ideal gases of different types. First, we want to compute the temperature of the joint system after mixing, $T_f$, given that the initial temperatures of the two subsystems $T_1$ and $T_2$ were identical. Since the joint system is isolated, we are in teh microcanonical ensemble. Also for an ideal gas we have already found that $U = \frac{3}{2}Nk_BT$ such that $T = \frac{2U}{3Nk_B}$. Hence we have \begin{equation*}
    T_1 = T_2 \iff \frac{U_1}{N_1} = \frac{U_2}{N_2}\iff U_2 = U_1\frac{N_2}{N_1}
\end{equation*}
After mixing, the final state has an internal energy given by \begin{equation*}
    U= U_1+U_2 = \frac{3}{2}N_1k_BT_f + \frac{3}{2}N_2k_BT_f
\end{equation*}
Rewriting the above equation we have \begin{equation*}
    \frac{3}{2}k_BT_f = \frac{U_1+U_2}{N_1+N_2} = \frac{U_1+U_1\frac{N_2}{N_1}}{N_1+N_2} = \frac{U_1}{N_1} = \frac{3}{2}k_BT_1
\end{equation*}
Hence the final temperature is the same as the initial temperature as expected. Now we consider entropy. Since mixing is typically an irreversible process, we expect an increase in entropy. For the initial entropy we have \begin{equation*}
    S_{initial} = S_1+S_2 = k_BN_1\ln\left[V_1\left(\frac{4\pi 2m_1U_1}{3N_1}\right)^{3/2}\right] + k_BN_2\ln\left[V_2\left(\frac{4\pi 2m_2U_2}{3N_2}\right)^{3/2}\right]
\end{equation*}
where we have used the expression for the entropy of an ideal gas derived previously. Using the notation \begin{equation*}
    \sigma_i \equiv \ln\left(\frac{4\pi em_iU_i}{3N_i}\right)^{3/2} = \frac{3}{2}\ln(2\pi em_iT_i)
\end{equation*}
we can write $S_{initial} = k_BN_1(\ln V_1+\sigma_1) + k_BN_2(\ln V_2 + \sigma_2)$. For the final entropy we have \begin{align*}
    S_{final} &= k_B\ln \Omega(U_1,U_2,V,N_1,N_2) \\
    &= k_B\ln[\Omega(U_1,V,N_1)\Omega(U_2,V,N_2)] \\
    &= k_B\ln\Omega(U_1,V,N_1) + k_B\ln\Omega(U_2,V,N_2) 
\end{align*}
assuming the two ideal gases do not interact such that the internal energies $U_1$ and $U_2$ do not change over time. Thus, mixing only leads to a change in the accessible volume to $V = V_1+V_2$ for both gases such that \begin{equation*}
    S_{final} = S_1(U_1,V,N_1) + S_2(U_2,V,N_2) = k_BN_1(\ln V + \sigma_1) + k_BN_2(\ln V +\sigma_2) 
\end{equation*}
For the difference in entropy, called the \Emph{mixing entropy} it follows that \begin{align*}
    \Delta S_{mix} &= S_{final} - S_{initial} \\
    &= k_BN_1(\ln V + \sigma_1) + k_BN_2(\ln V +\sigma_2) - k_BN_1(\ln V_1 + \sigma_1) - k_BN_2(\ln V_2 +\sigma_2) \\
    &= k_BN_1\ln\frac{V}{V_1} + k_BN_2\ln\frac{V}{V_2} \\
    &= k_BN\left[\frac{N_1}{N}\ln\frac{V}{V_1} + \frac{N_2}{N}\ln\frac{V}{V_2}\right]
\end{align*}
defining $N \equiv N_1+N_2$. Since $V > \max\{V_1,V_2\}$, the above expression implies that $\Delta S_{mix} > 0$ as expected.

\begin{cust}[Gibbs Paradox]
    For identical gases with identical initial densities, $n = \frac{N_1}{V_1} = \frac{N_2}{V_2}$, the calculation above also gives $\Delta S_{mix} > 0$. Yet, in this case removing the partition does not change the state of the system and it is reversible such that we should have $\Delta S_{mix}$.
\end{cust}

\subsection{Resolution to Gibbs Paradox}

Gibbs paradox implies an issue with our derived formula for the entropy of an ideal gas. The ultimate answer for how to fix this is given by Quantum mechanics. In the context of Classical Mechanics we can resolve this issue by an additional postulate, which takes into account whether particles are distinguishable or not. For the ideal gas, we are dealing with indistinguishable particles. This implies that there are $N!$ permutations or relabellings of the particles leading to indistinguishable microstates. To correct for this overcounting, we can divide our measure for the total number of states $\Omega$ obtained previously by $N!$, such that \begin{equation*}
    \Omega(U,V,N) = \frac{1}{N!}V^N\frac{2\pi^{3N/2}}{(3N/2-1)!}(2mU)^{(3N-1)/2}
\end{equation*}
This gives a slight modified expression for the entropy, after applying Stirling's formula in the thermodynamic limit: \begin{align*}
    S(U,V,N) &= k_B\ln\Omega(U,V,N) \\
    &= k_B\left[-\ln N! + N\ln V + \ln 2 + 3N/2\ln \pi - \ln([3N/2-1]!) + (3N-1)/2\ln (2mU)\right] \\
    &\approx k_BN\ln\left[\frac{e}{N}V\left(\frac{4\pi emU}{3N}\right)^{3/2}\right] \\
    &= Nk_B\left(\ln\frac{eV}{N} + \sigma\right)
\end{align*}
So $V$ has been replaced by $eV/N$. Going back to our calculations for the mixed entropy, let us consider the case of non=identical ideal gases: \begin{align*}
    \Delta S_{mix} &= S_{final} - S_{initial} \\
    &= N_1k_B\left(\ln\frac{eV}{N_1} + \sigma_1\right) + N_2k_B\left(\ln\frac{eV}{N_2} + \sigma_2\right) - N_1k_B\left(\ln\frac{eV_1}{N_1} + \sigma_1\right) - N_2k_B\left(\ln\frac{eV_2}{N_2} + \sigma_2\right) \\
    &= Nk_B\left[\frac{N_1}{N}\ln\frac{V}{V_1}+\frac{N_2}{N}\ln\frac{V}{V_2}\right] > 0
\end{align*}
as before. Returning to the case of ideal gases of the same type with identical initial densities, the final entropy now takes into account $(N_1+N_2)!$ instead of just $N_1!$ and $N_2!$, so specifically \begin{align*}
    S_{final} &= k_B\ln \Omega(U_1,U_2,V,N_1,N_2) \\
    &= k_B\ln\left[\frac{1}{(N_1+N_2)!}V^{N_1}\frac{2\pi^{3N_1/2}}{(3N_1/2-1)!}(2m_1U_1)^{(3N_1-1)/2}V^{N_2}\frac{2\pi^{3N_2/2}}{(3N_2/2-1)!}(2m_2U_2)^{(3N_2-1)/2}\right] \\
    &\approx -k_B(N\ln N - N\ln e) + k_BN_1(\ln V + \sigma_1) + k_BN_2(\ln V+\sigma_2) 
\end{align*}
With this we can calculate the mixing entropy: \begin{align*}
    \Delta S_{mix} &= S_{final} - S_{initial} \\
    &= k_BN_1(\ln V + \sigma_1) + k_BN_2(\ln V+\sigma_2) - k_BN\ln\frac{N}{e} - k_BN_1\left(\ln\frac{eV_1}{N_1}+\sigma_1\right)-k_BN_2\left(\ln\frac{eV_2}{N_2}+\sigma_2\right) \\
    &= k_BN_1\left(\ln V - \ln N + \ln e - \ln\frac{eV_1}{N_1}\right) + k_BN_2\left(\ln V - \ln N + \ln e - \frac{eV_2}{N_2}\right) \\
    &= k_BN_1\left(\ln\frac{V}{N}-\ln\frac{V_1}{N_1}\right) + k_BN_2\left(\ln\frac{V}{N} - \ln\frac{V_2}{N_2}\right) \\
    &= 0
\end{align*}
since by construction the initial and final densities are the same.

\subsection{Postulate for Indistinguishable Particles}

\begin{cust}[Postulate for Indistinguishable Particles]
    The phase space volume for \Emph{indistinguishable particles} takes on the form \begin{equation*}
        d\Gamma_N = \frac{1}{h^{3N}N!}\prod_{i=1}^Nd^3\vec{q}_id^3\vec{p}_i
    \end{equation*}
    where $h$ is Planck's constant, which is necessary to make the phase space volume dimensionless, which can be motivated by Heisenber's uncertainty principle, $\Delta p\Delta q \geq h$, leading to an effective coarse graining in phase space.
\end{cust}



%%%%%%%%%%%%%%%%%%%% Section 2.3.2
\section{Gibbs Canonical Ensemble}


The \Emph{Gibbs Canonical ensemble} describes the case of a closed system that is thermally coupled to a heat bath or reservoir at temperature $T$ while also allowed to exchange work with the environment at constant generalized force $\vec{J}$. The numbers of particles of different types $\vec{N}$ are fixed (closed system $\implies$ no chemical work). Note that the internal energy $U$ is not fixed due to the exchange of heat and mechanical work. In summary, our macrostate is $M \equiv (T,\vec{J},\vec{N})$ for the Gibbs canonical ensemble. This macrostate corresponds to the natural variables of the Gibbs free energy, $G = U - TS - \vec{q}\cdot\vec{J}$, making it the appropriate thermodynamic potential. The total energy has the two contributions, the internal energy representing the microscopic contributions of the system itself, and the energy due to the external forces representing macroscopic contributions: \begin{equation*}
    E_{tot} = H - \vec{J}\cdot \vec{q}
\end{equation*}
We can treat the combined system consisting of the system of interest, the heat bath, and the reservoir of mechanical work as an isolated system such that it falls into the microcanonical ensemble. This gives: \begin{equation*}
    p(\mu_S,\vec{q}) = \frac{e^{-\beta(H_S(\mu_S)-\vec{J}\cdot\vec{q})}}{Z_G(T,\vec{J},\vec{N})}
\end{equation*}
where $\beta = \frac{1}{k_BT}$, and we have defined \begin{equation*}
    Z_G(T,\vec{J},\vec{N}) \equiv \int d\vec{q}d\Gamma_Se^{-\beta(H_S(\mu_S)-\vec{J}\cdot\vec{q})}
\end{equation*}
called the \Emph{Gibbs partition function}.

\begin{defn}
    The average generalized displacements are given by \begin{equation*}
        \langle q_i\rangle = \frac{1}{Z_G(T,\vec{J},\vec{N})} \int d\vec{q}d\Gamma_S q_ie^{-\beta(H_S(\mu_S) - \vec{J}\cdot\vec{q})} 
    \end{equation*}
    which we can express equivalently as \begin{equation*}
        \langle q_i\rangle = k_BT\frac{\partial \ln Z_G}{\partial J_i}
    \end{equation*}
\end{defn}
Based on our thermodynamic relations for the Gibbs free energy we have $q_i = -\left(\frac{\partial G}{\partial J_i}\right)_{T,\vec{N},J_{j\neq i}}$, so the Gibbs free energy can be expressed in terms of the Gibbs partition function by \begin{equation*}
    G(T,\vec{J},\vec{N}) = -k_BT\ln Z_G
\end{equation*}

\subsection{Heat Capacities}

In the canonical ensemble we could only consider $C_{\vec{q}}$ since there was no exchange of mechanical work. In contrast, in the Gibbs canonical ensemble we can now also calculate $C_{\vec{J}}$. Indeed, we have \begin{equation*}
    C_{\vec{J}} \equiv \left(\frac{\delta Q}{dT}\right)_{\vec{J}} = \left(\frac{\partial U}{\partial T}\right)_{\vec{q}} + \sum_{i=1}^m\left[\left(\frac{\partial U}{\partial q_i}\right)_{T,q_{j\neq i}} - J_i\right]\left(\frac{dq_i}{dT}\right)_{\vec{J}} = \left(\frac{\partial}{\partial T}\right)_{\vec{J},\vec{N}}\left[U(T,\vec{q},\vec{N}) - \vec{q}\cdot \vec{J}\right] = \left(\frac{\partial \mathcal{H}}{\partial T}\right)_{\vec{J},\vec{N}}
\end{equation*}
where $\mathcal{H}$ is the enthalpy. From the Gibbs partition function we find \begin{equation*}
    -\frac{\partial \ln Z_G}{\partial \beta} = \langle H-\vec{q}\cdot\vec{J}\rangle = \langle H\rangle - \langle \vec{q}\rangle \cdot\vec{J} = \mathcal{H}
\end{equation*}
where the last equality holds in the thermodynamic limit. Hence we have all the pieces needed to compute $C_{\vec{J}}$.

Finally, we can calculate the internal energy in the Gibbs canonical ensemble by \begin{equation*}
    \langle U\rangle = \langle H\rangle = \mathcal{H} + \langle \vec{q}\rangle \cdot \vec{J}
\end{equation*}


\subsection{Ideal Gas in the Isobaric Ensemble}

As in the canonical ensemble, $\mu \equiv\{\vec{q},\vec{p}\}$ and the Hamiltonian also remains unchanged, $H = \sum_{i=1}^N\left[\frac{\vec{p}_i\cdot\vec{p}_i}{2m} + \tilde{U}(\vec{q}_i)\right]$, where $\tilde{U}$ is the confining potential. The macrostate is determined by the temperature, pressure, and number of particles $M \equiv (T,p,N)$, since the pressure is held constant but not the volume. For the equilibrium phase space density we have in this case \begin{equation*}
    p(\{\vec{q},\vec{p}\},V) = \frac{1}{Z_G(T,p,N)}e^{-\beta\sum_{i=1}^N\frac{\vec{p_i}\cdot\vec{p}_i}{2m} - \beta pV}\delta(\{\vec{q}_i\}\in V)
\end{equation*}
For the Gibbs partition function we calculate \begin{align*}
    Z_G(T,p,N) &= \int_0^{\infty}dV\int d\Gamma e^{-\beta\sum_{i=1}^N\frac{\vec{p_i}\cdot\vec{p}_i}{2m} - \beta pV}\delta(\{\vec{q}_i\}\in V)\\
    &= \int_0^{\infty}dVe^{-\beta p V}\int\frac{1}{N!}\left(\prod_{i=1}^N\frac{d\vec{q}_id\vec{p}_i}{h^3}\right)e^{-\beta\sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}}\delta(\{\vec{q}_i\}\in V) \\
    &= \int_0^{\infty}dV\frac{e^{-\beta pV}}{N!h^{3N}}\int_Vd\vec{q}_1\cdots \int_Vd\vec{q}_N\int_{-\infty}^{\infty}d\vec{p}_1\cdots \int_{-\infty}^{\infty}d\vec{p}_Ne^{-\beta\sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m}}\delta(\{\vec{q}_i\}\in V) \\
    &=\frac{1}{N!h^{3N}}\int_0^{\infty}dVe^{-\beta pV}\prod_{i=1}^N\left(\int_Vd\vec{q}_i\delta(\{\vec{q}_i\}\in V)\int_{-\infty}^{\infty}d\vec{p}_ie^{-\beta\frac{\vec{p}_i\cdot\vec{p}_i}{2m}}\right) \\
    &= \frac{1}{N!h^{3N}}\int_0^{\infty}dVe^{-\beta pV}\prod_{i=1}^N\left(V\cdot \left(\frac{2\pi m}{\beta}\right)^{3/2}\right) \\
    &= \frac{1}{N!\lambda(T)^{3N}}\int_0^{\infty}dV V^Ne^{-\beta pV}
\end{align*}
where $\lambda(T) = \frac{h}{\sqrt{2\pi mk_BT}}$, which is now a proper characteristic length due to the use of the correct dimensionless phase space volume. The remaining integral, with $\int_0^{\infty}dV V^Ne^{-\beta pV} = \frac{1}{(\beta p)^{N+1}}\Gamma(N+1) = \frac{N!}{(\beta p)^{N+1}}$, so \begin{equation*}
    Z_G(T,p,N) = \frac{1}{N!\lambda(T)^{3N}}\cdot \frac{N!}{(\beta p)^{N+1}} = \frac{1}{\lambda(T)^{3N}(\beta p)^{N+1}}
\end{equation*}
Thus we have the Gibbs free energy \begin{equation*}
    G(T,p,N) = -k_BT\ln Z_G = k_BTN\ln(\lambda(T)^3(\beta p)^{(N+1)/N}) \approx k_BTN\ln (\lambda(T)^3\beta p)
\end{equation*}
where the approximation becomes exact in the thermodynamic limit. Since $dG = -SdT + Vdp + \mu dN$, we obtain \begin{equation*}
    \langle V\rangle = \left(\frac{\partial G}{\partial p}\right)_{T,N} = \frac{Nk_BT}{p}
\end{equation*}
recovering the equation of state of an ideal gas. We also obtain the enthalpy \begin{equation*}
    \mathcal{H} = -\frac{\partial \ln Z_G}{\partial \beta} = \frac{5}{2}Nk_BT
\end{equation*}
and the heat capacity \begin{equation*}
    C_P = \left(\frac{\partial\mathcal{H}}{\partial T}\right)_{p,N} = \frac{5}{2}Nk_B
\end{equation*}
For the internal energy we find \begin{equation*}
    \langle U\rangle = \mathcal{H} - p\langle V\rangle = \frac{3}{2}Nk_BT
\end{equation*}
coinciding with the result obtained in the canonical ensemble. Smilarly, we also get \begin{equation*}
    C_V = \left(\frac{\partial \langle U\rangle}{\partial T}\right)_{V,N} = \frac{3}{2}Nk_B
\end{equation*}
Thus we have $C_p - C_V = Nk_B > 0 $ as expected.

\subsection{Paramagnet in an External Magnetic Field}

We consider a paramagnet in the form of the spins of $N$ molecules making up the magnet. However, we allow the spins to have any orientation such that the microstate is fully determined by a set of $N$ three dimesnional vectors $\mu = \{\vec{\sigma}_i\}$. We require that $|\vec{\sigma}_i| = 1$, so each spin is in one and only one orientation. Since we are interested in the effect of a constant external magnetic field $\vec{B}$, our macrostate is $M \equiv (T,\vec{B},N)$. The interaction between spins and an external magnetic field take on the form $\vec{B}\cdot \vec{M}$, where $\vec{M}$ is the net magnetic moment or net magnetization of our paramagnet. We can write it here as $\vec{M} = \mu_0\sum_{i=1}^N\vec{\sigma}_i$ where $\mu_0$ is the microscopic magnetic moment. Hence the Gibbs partition function for this discrete system can be written as \begin{equation*}
    Z_G(T,\vec{B},N) = \sum_{\{\sigma\}}e^{-\beta H + \beta \vec{B}\cdot\vec{M}}
\end{equation*}
where the sum runs over all possible microstates and $H$ is the Hamiltonian capturing the interactions between the spins. In the following we consider spin $1/2$ particles, $\sigma_i = \pm 1$, when the external magnetic field $\vec{B}$ is parallel to $\vec{M}$. We also neglect any interactions between the spins such that $H = 0$. Thus we have for the equilibrium phase space density \begin{equation*}
    p(\{\sigma_i\}) = \frac{1}{Z_G}e^{\beta B\mu_0\sum_{i=1}^N\sigma_i}
\end{equation*}
Calculating the Gibbs partition function \begin{align*}
    Z_G(T,B,N) &= \sum_{\{\sigma\}}e^{\beta B\mu_0\sum_{i=1}^N\sigma_i} \\
    &= \sum_{\sigma_1 \in \{-1,1\}}\cdots \sum_{\sigma_N\in\{-1,1\}}e^{\beta B\mu_0\sum_{i=1}^N\sigma_i} \\
    &=\sum_{\sigma_1 \in \{-1,1\}}e^{\beta B\mu_0\sigma_1}\cdots \sum_{\sigma_N\in\{-1,1\}}e^{\beta B\mu_0\sigma_N} \\
    &= (e^{\beta B\mu_0} + e^{-\beta B\mu_0})^N = [2\cosh(\beta \mu_0B)]^N
\end{align*}
From this the Gibbs free energy can be found: \begin{equation*}
    G(T,B,N) = -k_BT\ln Z_G = -k_BTN\ln[2\cosh(\beta\mu_0 B)]
\end{equation*}
Using the properties of the Gibbs free energy $dG = -SdT - MdB+\mu dN$, we obtain \begin{equation*}
    \langle M\rangle = -\left(\frac{\partial G}{\partial B}\right)_{T,N} = Nk_BT\frac{2\sinh(\beta\mu_0 B}{2\cosh(\beta\mu_0B)}\beta\mu_0 = N\mu_0\tanh(\beta\mu_0B)
\end{equation*}
This is the \Emph{equation of state} for this system. From this equation of state we investigate the magnetic susceptibility, $\chi$, which measures how strong the magnetic response of the paramagnet to an external magnetic field is \begin{equation*}
    \chi(T) \equiv\left(\frac{\partial M}{\partial B}\right)_{T,N}\Big\rvert_{B=0} = \frac{N\mu_0}{[\cosh(\beta\mu_0B)]^2}\beta\mu_0\Big\vert_{B=0} = \frac{N\mu_0^2}{k_BT}
\end{equation*}
This expression is known as the \Emph{Curie law}, which was first discovered experimentally.

Next we investigate the heat capacity $C_B$. To do so we need the enthalpy first: \begin{equation*}
    \mathcal{H} = -\frac{\partial \ln Z_G}{\partial \beta} = -N\mu_0B\tanh(\beta\mu_0B)
\end{equation*}
The last expression coincides with $-B\langle M\rangle$ as expected. Using the expression for the enthalpy we find \begin{equation*}
    C_B = \left(\frac{\partial \mathcal{H}}{\partial T}\right)_{B,N} = \frac{Nk_B\mu_0^2B^2\beta^2}{[\cosh(\beta\mu_0B)]^2}
\end{equation*}
Observe that now the relevant quantity is the product $|B\beta|$ such that increasing the absolute value of $B$ has the same effect as decreasing $T$, and decreasing the absolute value of $B$ has the same effect as increasing $T$.


%%%%%%%%%%%%%%%%%%%% Section 2.3.3
\section{Grand Canonical Ensemble}

The \Emph{Grand canonical ensemble} is for a system that is allowed to exchange heat and particles with the environment under the constraint that the system is maintained at constant temperature $T$ and constant chemical potential $\mu$. The macrostate is given by $M \equiv (T,\vec{\mu},\vec{q})$. 

Let $R$ be our heat and particle reservoir interacting with our system of interest, $S$, in which no work is exchanged. Let $H_R(\mu_R)$ denote the internal energy of $R$ and $H_S(\mu_S)$ denote the internal energy of $S$. Derivations follows as with the canonical ensemble (i.e. using the maximum entropy principle with $\langle N\rangle$ fixed): \begin{equation*}
    \rho_{eq}(\mu_s) = \int d\Gamma_R\rho_{eq}(\mu_S\otimes\mu_R) = \frac{e^{-\beta(H(\mu_S)-\vec{\mu}\cdot\vec{N}(\mu_S))}}{Q(T,\vec{\mu},\vec{q}}
\end{equation*}
where \begin{equation*}
    Q(T,\mu,\vec{q} = \int d\Gamma_S e^{-\beta(H(\mu_S) - \mu N(\mu_S))} = \sum_{N=0}^{\infty}e^{\beta\mu N}\int d\Gamma_{S\backslash N}e^{-\beta H(\mu_S)} = \sum_{N=0}^{\infty}e^{\beta\mu N}Z(T,N,\vec{q})
\end{equation*}
is the grand partition function, and $Z(T,N,\vec{q})$ is the canonical partition function. Then the probability density function for the number of particles is given by \begin{equation*}
    p(N) = \frac{e^{\beta\mu N}Z(T,N,\vec{q})}{Q(T,N,\vec{q})}
\end{equation*}
and we have \begin{equation*}
    \langle N\rangle = \frac{1}{Q}\frac{1}{\beta}\left(\frac{\partial Q}{\partial \mu}\right)_{T,\vec{q}} = \frac{1}{\beta}\left(\frac{\partial \ln Q}{\partial \mu}\right)_{T,\vec{q}}
\end{equation*}
As $p(N)$ is a probability density function, it obeys the central limit theorem in the limit $\langle N\rangle \rightarrow \infty$, and so all three ensembles are equivalent in the thermodynamic limit. Thus \begin{align*}
    Q(T,\mu,\vec{q}) &= \sum_{N=0}^{\infty}e^{\beta\mu N}Z(T,N,\vec{q}) \\
    &\approx e^{\beta \mu N^*}Z(T,N^*,\vec{q}) \tag{saddle point integration} \\
    &\approx e^{\beta \mu\langle N\rangle}Z(T,\langle N\rangle,\vec{q}) \tag{CLT} \\
    &= e^{\beta \mu\langle N\rangle}e^{-\beta F} \tag{substituting $F = -k_BT\ln Z$} \\
    &= e^{-\beta(-\mu\langle N\rangle + \langle U\rangle - TS)} \tag{substituting $F = \langle U\rangle - TS$} \\
    &= e^{-\beta\mathcal{G}}
\end{align*}
with $\mathcal{G}(T,\mu,\vec{q}) := \langle U\rangle - TS - \mu\langle N\rangle$ is the grand potential. Solving for $\mathcal{G}$ we obtain the expression \begin{equation*}
    \boxed{\mathcal{G}(T,\mu,\vec{q} = -k_BT\ln Q(T,\mu,\vec{q})}
\end{equation*}

\subsection{Ideal Gas in the Grand Canonical Ensemble}

Consider an ideal gas which is allowed to exchange heat and particles, of the same kind, with a reservoir. Then the microstate is $\mu_S \equiv \{\vec{p}_i,\vec{q}_i\}$, and $M \equiv (T,\mu,V)$. Then the grand partition function is \begin{align*}
    Q(T,\mu,V) &= \sum_{N=0}^{\infty}e^{\beta \mu N}\frac{1}{N!}\int\left(\prod_{i=1}^N\frac{d^3\vec{q}_id^3\vec{p}_i}{h^3}\right)e^{-\beta \sum_{i=1}^N\vec{p}_i\cdot\vec{p}_i/2m}\delta(\{\vec{q}_i\} \in \text{box}) \\
    &= \sum_{N=0}^{\infty}e^{\beta \mu N}\frac{1}{N!}\left(\frac{V}{\lambda(T)^3}\right)^N \tag{$\lambda(T) = \frac{h}{\sqrt{2\pi mk_BT}}$} \\
    &= \exp\left\{e^{\beta \mu}\frac{V}{\lambda(T)^3}\right\}
\end{align*}
Then the grand potential for the gas is \begin{equation*}
    \mathcal{G}(T,\mu,V) = -k_BT\ln Q = -k_BTe^{\beta \mu}\frac{V}{\lambda(T)^3}
\end{equation*}
From thermodynamics we have that $d\mathcal{G} = -SdT - Nd\mu - PdV$. Then \begin{equation*}
    P = -\left(\frac{\partial\mathcal{G}}{\partial V}\right)_{\mu,T} = k_BTe^{\beta \mu}\frac{1}{\lambda(T)^3} 
\end{equation*}
and \begin{equation*}
    \langle N\rangle = -\left(\frac{\partial \mathcal{G}}{\partial \mu}\right)_{T,V} = e^{\beta \mu}\frac{V}{\lambda(T)^3}
\end{equation*}
and so we obtain the equation of state \begin{equation*}
    P = k_BT\frac{\langle N\rangle}{V}
\end{equation*}
Solving for $\mu$ we get \begin{equation*}
    \mu = k_BT\ln\left(\frac{P\lambda(T)^3}{k_BT}\right) = k_BT\ln\left(\frac{\langle N\rangle \lambda(T)^3}{V}\right)
\end{equation*}
which is the chemical potential necessary to add particles. Recall that the differential of chemical work is given by $\delta W_{chem} = \vec{\mu}\cdot d\vec{N}$.

\subsection{Surfactant Adsorption}

\begin{defn}
    \Emph{Surfactant} are compounds that reduce the surface tension between two liquids, e.g. molecules with a hydrophil head and hydrophobe tail.
\end{defn}

Surfactant molecules can reduce their energy by contacting with air (increased DLL)

\begin{qst}
    What is the ratio of surfactants in a solution versus at the surface?
\end{qst}

To answer this we construct a model. In our model we assume surfactants in the solution can be regarded as an ideal 3D gas, that surfactants on the surface can be treated as an ideal 2D gas, that there is a difference in potential energy of $\varepsilon_0$ between the surface and the solution, and that water only serves as a heat bath.

Thus, in isolation: $$H_d = \underbrace{N_d\varepsilon_d}_{\text{uniform attraction potential}} + \sum_{i=1}^{N_d}\frac{\vec{p}_i\cdot\vec{p}_i}{2m}$$ for $d = 2$ and $d = 3$, with $\varepsilon_0 = \varepsilon_3 - \varepsilon_2 > 0$. Then the partition function is  \begin{align*}
    Z_d &= \frac{1}{N_d!h^{dN_d}}\int\cdots \int \prod_{i=1}^{N_d}d^d\vec{q}_id^d\vec{p}_ie^{-\beta H_d}\cdot\delta(\{\vec{q}_i\}\in V_d) \\
    &= \frac{1}{N_d!}\left(\frac{V_d}{\lambda(T)^d}\right)^{N_d}e^{-\beta N_d\varepsilon_d},\;\;\lambda(T) = \frac{h}{\sqrt{2\pi mk_BT}}
\end{align*}
Then $F_d = -k_BT\ln Z_d$, so as $dF = -SdT + \vec{J}\cdot d\vec{q}+\vec{\mu}\cdot d\vec{N}$, $$\mu_d = \left(\frac{\partial F_d}{\partial N_d}\right)_{V,T} = -k_BT\left(\frac{\partial\ln Z_d}{\partial N_d}\right)_{V,T} \approx \varepsilon_d - k_BT\ln\left(\frac{V_d}{N_d\lambda(T)^d}\right)$$
using Stirling's approximation in the limit $N\rightarrow \infty$.

Now we consider the coupled system at temperature $T$ and chemical potential $\mu$: \begin{align*}
    Q_d(\mu,T,V_d) &= \sum_{N_d=0}^{\infty}Z_d(N_d,V_d,T) e^{\beta \mu N_d} \\
    &= \sum_{N_d=0}^{\infty}\frac{1}{N_d!}\left(\frac{V_d}{\lambda(T)^d}\right)^{N_d}e^{-\beta N_d\varepsilon_d}e^{\beta\mu N_d} \\
    &= \exp\left[\frac{V_d}{\lambda(T)^d}e^{\beta(\mu-\varepsilon_d)}\right]
\end{align*}
Thus, $$\langle N_d\rangle = \frac{1}{\beta}\left(\frac{\partial \ln Q_d}{\partial \mu}\right)_{T,V_d} = \frac{V_d}{\lambda(T)^d}e^{\beta(\mu-\varepsilon_d)}$$
We are interested in the coexistence of teh surfactants in the 3D solution and its 2D surface: \begin{equation*}
    \frac{\langle N_2\rangle}{\langle N_3\rangle} = \frac{V_2\lambda(T)}{V_3}e^{\beta(\varepsilon_3-\varepsilon_2)} =\frac{V_2}{V_3}\lambda(T)e^{\beta\varepsilon_0}
\end{equation*}
Then we have $$n_2 = \frac{\langle N_2\rangle}{V_2} = \frac{\langle N_3\rangle}{V_3}\lambda(T)e^{\beta\varepsilon_0} = n_3\lambda(T)e^{\beta\varepsilon_0}$$ where $n_d$ is the density of surfactants in the $d$ dimensional portion of the solution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Part 3
\part{Probability Theory}


%%%%%%%%%%%%%%%%%%%%%% Chapter 3.1
\chapter{Characteristics of Probability Theory}

%%%%%%%%%%%%%%%%%%%% Section 3.1.1
\section{Measure Theory}

We first formalize the notion of a probability space:

\begin{defn}
    A \Emph{probability space} $(\Omega, \mathcal{F}, P)$ is a \Emph{measure space} with a measure $P$ that satisfies the probability axioms. Here, $\Omega$ is the non-empty set of all possible outcomes, $\mathcal{F}$ is the $\sigma$-algebra of subsets of $\Omega$, its elements are called events, and $P$ is a function from $\mathcal{F}$ to $\R$ that assigns to each event a probability between $0$ and $1$.

    The probability axioms are (Kolmogorov):
    \begin{enumerate}
        \item $P(E) \geq 0$ for all $E \in \mathcal{F}$ (positivity)
        \item $P(\Omega) = 1$ (normalization)
        \item $P(E_1\cup E_2) = P(E_1) + P(E_2) - P(E_1\cap E_2)$ for all $E_1,E_2 \in \mathcal{F}$ (additivity), so $P(E_1\cup E_2) = P(E_1)+P(E_2)$ if $E_1\cap E_2 = \emptyset$.
    \end{enumerate}
\end{defn}

To deconstruct this formal definition we pursue certain measure theoretic terms:

\begin{defn}
    A class of sets $\mathcal{A} \subseteq 2^{\Omega}$, for $\Omega \neq \emptyset$ a nonempty set, is called \begin{itemize}
        \item $\bigcap$-closed (closed under intersections) or a \Emph{$\pi$-system} if $A\cap B \in \mathcal{A}$ whenever $A,B \in \mathcal{A}$
        \item $\sigma-\bigcap$-closed (closed under countable intersections) if $\bigcap_{n=1}^{\infty}A_n\in \mathcal{A}$ for any countable collection $\{A_n\}_{n=1}^{\infty}\subseteq \mathcal{A}$
        \item $\bigcup$-closed (closed under unions) if $A\cup B \in \mathcal{A}$ whenever $A,B \in \mathcal{A}$.
        \item $\sigma-\bigcup$-closed (closed under countable unions) if $\bigcup_{n=1}^{\infty}A_n\in \mathcal{A}$ for any countable collection $\{A_n\}_{n=1}^{\infty}\subseteq \mathcal{A}$
        \item $\backslash$-closed (closed under differences) if $A\backslash B\in \mathcal{A}$ whenever $A,B \in \mathcal{A}$
        \item closed under complements if $A^c:= \Omega\backslash A \in \mathcal{A}$ for any set $A \in \mathcal{A}$.
    \end{itemize}
\end{defn}

\begin{defn}[$\sigma$-algebra]
    A class of sets $\mathcal{A} \subseteq 2^{\Omega}$ is called a \Emph{$\sigma$-algebra} if it fulfills the following three conditions: \begin{itemize}
        \item[(i)] $\Omega \in \mathcal{A}$
        \item[(ii)] $\mathcal{A}$ is closed under complements
        \item[(iii)] $\mathcal{A}$ is closed under countable unions
    \end{itemize}
\end{defn}

$\sigma$-algebras are the natural classes of sets to be considered as events in probability theory.

\begin{defn}
    A class of sets $\mathcal{A} \subseteq 2^{\Omega}$ is called an \Emph{algebra} if the following three conditions are fulfilled: \begin{itemize}
        \item[(i)] $\Omega \in \mathcal{A}$ 
        \item[(ii)] $\mathcal{A}$ is $\backslash$-closed
        \item[(iii)] $\mathcal{A}$ is $\bigcup$-closed
    \end{itemize}
\end{defn}

\begin{defn}
    A class of sets $\mathcal{A} \subseteq 2^{\Omega}$ is called a \Emph{ring} if the following three conditions hold:  \begin{itemize}
        \item[(i)] $\emptyset \in \mathcal{A}$ 
        \item[(ii)] $\mathcal{A}$ is $\backslash$-closed
        \item[(iii)] $\mathcal{A}$ is $\bigcup$-closed
    \end{itemize}
    If it is also $\sigma-\bigcup$-closed, then $\mathcal{A}$ is called a \Emph{$\sigma$-ring}.
\end{defn}

\begin{defn}
    A class of sets $A\subseteq 2^{\Omega}$ is called a \Emph{semiring} if  \begin{itemize}
        \item[(i)] $\emptyset \in \mathcal{A}$ 
        \item[(ii)] for any two sets $A,B\in\mathcal{A}$ the difference $B\backslash A$ is a finite union of mutually disjoing sets in $\mathcal{A}$
        \item[(iii)] $\mathcal{A}$ is $\bigcap$-closed
    \end{itemize}
\end{defn}

\begin{defn}
    A class of sets $\mathcal{A} \subseteq 2^{\Omega}$ is called a \Emph{$\lambda$-system} (or Dynkin's $\lambda$-system) if  \begin{itemize}
        \item[(i)] $\Omega \in \mathcal{A}$ 
        \item[(ii)] for any two sets $A,B\in\mathcal{A}$ with $A\subseteq B$, the difference $B\backslash A$ is in $\mathcal{A}$
        \item[(iii)] $\bigsqcup_{n=1}^{\infty}A_n \in \mathcal{A}$ for any countable collection of pairwise disjoint sets $\{A_n\}_{n=1}^{\infty} \subseteq \mathcal{A}$
    \end{itemize}
\end{defn}

\begin{thm}
    \leavevmode
    \begin{itemize}
        \item[(i)] Every $\sigma$-algebra is a $\lambda$-system, an algebra, and a $\sigma$-ring
        \item[(ii)] Every $\sigma$-ring is a ring, and every ring is a semiring
        \item[(iii)] Every algebra is a ring. An algebra on a finite set $\Omega$ is a $\sigma$-algebra.
    \end{itemize}
\end{thm}

\begin{defn}
    Let $A_1,A_2,...$ be subsets of $\Omega$. The sets \begin{equation*}
        \liminf_{n\rightarrow \infty}A_n := \bigcup_{n=1}^{\infty}\bigcap_{m=n}^{\infty}A_m\;\text{ and }\;\limsup_{n\rightarrow \infty}A_n := \bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}A_m
    \end{equation*}
    are called \Emph{limes inferior} and \Emph{limes superior}, respectively, of the sequence $(A_n)_{n\in\N}$.
\end{defn}

Equivalently these sets can be formulated as \begin{align*}
    \liminf_{n\rightarrow\infty}A_n &= \{\omega \in \Omega: |\{n \in \N:\omega \notin A_n\}| < \infty\}
    \limsup_{n\rightarrow \infty}A_n &= \{\omega \in \Omega: |\{n\in\N: \omega \in A_n\}| = \infty\}
\end{align*}
Thus, limes inferior is the event where eventually all of $A_n$ occur, while limes superior is the event where infinitely many of the $A_n$ occur.

\begin{defn}
    The \Emph{indicator function} on the set $A$ is defined by \begin{equation*}
        \mathbb{1}_A(x) := \left\{\begin{array}{lc} 1, & \text{if } x \in A, \\ 0, & \text{if } x \notin A, \end{array}\right.
    \end{equation*}
\end{defn}

\begin{thm}
    Let $I$ be an arbitrary index set, and assume $\mathcal{A}_i$ is a $\sigma$-algebra for every $i \in I$. Hence the intersection \begin{equation*}
        \mathcal{A}_I := \bigcap_{i\in I}\mathcal{A}_i
    \end{equation*}
    is a $\sigma$-algebra. The analogous statement holds for rings, $\sigma$-rings, algebras, and $\lambda$-systems.
\end{thm}

\begin{thm}
    Let $\mathcal{E} \subseteq 2^{\Omega}$. Then there exists a smallest $\sigma$-algebra $\sigma(\mathcal{E})$ with $\mathcal{E} \subseteq \sigma(\mathcal{E})$: \begin{equation*}
        \sigma(\mathcal{E}) := \bigcap_{\mathcal{E}\subseteq \mathcal{A}\subseteq 2^{\Omega}\text{ is a $\sigma$-algebra}}\mathcal{A}
    \end{equation*}
    $\sigma(\mathcal{E})$ is called the $\sigma$-algebra \Emph{generated by} $\mathcal{E}$. $\mathcal{E}$ is called a \Emph{generator} of $\sigma(\mathcal{E})$. Similarly, we define $\delta(\mathcal{E})$ as the $\lambda$-system generated by $\mathcal{E}$.
\end{thm}

\begin{thm}
    Let $\mathcal{D}\subseteq 2^{\Omega}$ be a $\lambda$-system. Then \begin{equation*}
        \mathcal{D}\text{ is a $\pi$-system } \iff \mathcal{D} \text{ is a $\sigma$-algebra}
    \end{equation*}
\end{thm}

\begin{namthm}[Dynkin's $\pi-\lambda$ theorem]
    If $\mathcal{E} \subseteq 2^{\Omega}$ is a $\pi$-system, then $\sigma(\mathcal{E}) = \delta(\mathcal{E})$.
\end{namthm}

We are often interested in $\sigma$-algebras generated by topologies. 

\begin{defn}
    Let $\Omega \neq \emptyset$ be an arbitrary set. A class of sets $\tau \subseteq \Omega$ is called a \Emph{topology} on $\Omega$ if it has the following three properties:  \begin{itemize}
        \item[(i)] $\emptyset, \Omega \in \tau$ 
        \item[(ii)] $A\cap B \in \tau$ for any $A,B\in \tau$
        \item[(iii)] $\left(\bigcup_{A\in\mathcal{F}}A\right)\in\tau$ for any $\mathcal{F}\subseteq \tau$
    \end{itemize}
    The pair $(\Omega,\tau)$ is called a \Emph{topological space}. The sets $A \in \tau$ are called \Emph{open}, and the sets $A \subseteq \Omega$ with $A^c \in \tau$ are called \Emph{closed}.
\end{defn}

\begin{defn}[Borel $\sigma$-algebra]
    Let $(\Omega,\tau)$ be a topological space. The $\sigma$-algebra \begin{equation*}
        \mathcal{B}(\Omega) := \mathcal{B}(\Omega,\tau) := \sigma(\tau)
    \end{equation*}
    that is generated by the open sets is called the \Emph{Borel $\sigma$-algebra} on $\Omega$. The elements $A \in \mathcal{B}(\Omega,\tau)$ are called \Emph{Borel sets} or \Emph{Borel measurable sets}.
\end{defn}

\begin{defn}
    Let $\mathcal{A} \subseteq 2^{\Omega}$ be an arbitrary class of subsets of $\Omega$ and let $A \in 2^{\Omega}\backslash\{\emptyset\}$. The class \begin{equation*}
        \mathcal{A}\vert_A := \{A\cap B:B \in \mathcal{A}\}\subseteq 2^A
    \end{equation*}
    is called the \Emph{trace} of $\mathcal{A}$ on $A$ or the \Emph{restriction} of $\mathcal{A}$ to $A$.
\end{defn}


\subsection{Set Functions}

\begin{defn}
    Let $\mathcal{A} \subseteq 2^{\Omega}$ and let $\mu:\mathcal{A}\rightarrow [0,\infty]$ be a set function. We say that $\mu$ is \begin{itemize}
        \item[(i)] \Emph{monotone} if $\mu(A) \leq \mu(B)$ for any two sets $A,B\in\mathcal{A}$ with $A\subseteq B$,
        \item[(ii)] \Emph{additive} if $\mu\left(\bigsqcup_{i=1}^nA_i\right) = \sum_{i=1}^n\mu(A_i)$ for any choice of finitely many mutually disjoint sets $A_1,...,A_n \in \mathcal{A}$ with $\bigsqcup_{i=1}^nA_i \in \mathcal{A}$
        \item[(iii)] \Emph{$\sigma$-additive} if $\mu\left(\bigsqcup_{i=1}^{\infty}A_i\right) = \sum_{i=1}^{\infty}\mu(A_i)$ for any choice of countably many mutually disjoint sets $A_1,A_2,... \in \mathcal{A}$ with $\bigsqcup_{i=1}^{\infty}A_i \in \mathcal{A}$
        \item[(iv)] \Emph{subadditive} if for any choice of finitely many sets $A,A_1,...,A_n \in \mathcal{A}$ with $A \subseteq \bigcup_{i=1}^mA_i$, we have $\mu(A) \leq \sum_{i=1}^n\mu(A_i)$, 
        \item[(v)] \Emph{$\sigma$-subadditive} if for any choice of countably many sets $A,A_1,A_2,... \in \mathcal{A}$ with $A \subseteq \bigcup_{i=1}^{\infty}A_i$, we have $\mu(A) \leq \sum_{i=1}^{\infty}\mu(A_i)$,
    \end{itemize}
\end{defn}


\begin{defn}
    Let $\mathcal{A}$ be a semiring and let $\mu:\mathcal{A}\rightarrow [0,\infty]$ be a set function with $\mu(\emptyset) = 0$. $\mu$ is called a \begin{itemize}
        \item \Emph{content} if $\mu$ is additive,
        \item \Emph{premeasure} if $\mu$ is $\sigma$-additive
        \item \Emph{measure} if $\mu$ is a premeasure and $\mathcal{A}$ is a $\sigma$-algebra, 
        \item \Emph{probability measure} if $\mu$ is a measure and $\mu(\Omega) = 1$
    \end{itemize}
\end{defn}

\begin{defn}
    Let $\mathcal{A}$ be a semiring. A content $\mu$ on $\mathcal{A}$ is called \begin{itemize}
        \item[(i)] \Emph{finite} if $\mu(A) < \infty$ for every $A \in \mathcal{A}$
        \item[(ii)] \Emph{$\sigma$-finite} if there exists a sequence of sets $\Omega_1,\Omega_2,... \in \mathcal{A}$ such that $\Omega = \bigcup_{n=1}^{\infty}\Omega_n$ and such that $\mu(\Omega_n) < \infty$ for all $n \in \N$.
    \end{itemize}
\end{defn}

\begin{defn}
    For any $\omega \in \Omega$, $\delta_{\omega}$ defined by $\delta_{\omega}(A) = \mathbb{1}_A(\omega)$ is a probability measure on any $\sigma$-algebra $\mathcal{A} \subseteq 2^{\Omega}$, and is called the \Emph{Dirac measure} for the point $\omega$.
\end{defn}

\begin{defn}
    Let $\Omega$ be a finite nonempty set. The set function defined by \begin{equation*}
        \mu(A) := \frac{|A|}{|\Omega|}
    \end{equation*}
    for all $A \subseteq \Omega$ defines a probability measure on $\mathcal{A} = 2^{\Omega}$. $\mu$ is called the \Emph{uniform distribution} on $\Omega$. The resulting triple $(\Omega,\mathcal{A},\mathcal{U}_{\Omega})$ is called a \Emph{Laplace space} where $\mathcal{U}_{\Omega} := \mu$.
\end{defn}

We remark that a countable linear combination of measures (resp. premeasures, contents) is once again a measure (resp. premeasure, content). 

\begin{defn}
    Let $\Omega$ be an at most countable nonempty set and let $\mathcal{A} = 2^{\Omega}$. Further, let $(p_{\omega})_{\omega \in \Omega}$ be nonnegative numbers. Then $A\mapsto \mu(A) := \sum_{\omega \in A}p_{\omega}$ defines a $\sigma$-finite measure on $2^{\Omega}$. We call $p = (p_{\omega})_{\omega \in \Omega}$ the \Emph{weight function} of $\mu$. The number $p_{\omega}$ is called the weight of $\mu$ at point $\omega$. If the sum over $\Omega$, $\sum_{\omega \in \Omega}p_{\omega}$, equals one, then $\mu$ is a probability measure, and we call the weight function, $p = (p_{\omega})_{\omega \in \Omega}$, a \Emph{probability vector}.
\end{defn}

\begin{lem}[Properties of Contents]
    Let $\mathcal{A}$ be a semiring and let $\mu$ be a content on $\mathcal{A}$. Then the following statements hold: \begin{itemize}
        \item[(i)] If $\mathcal{A}$ is a ring, then $\mu(A\cup B) + \mu(A\cap B) = \mu(A) + \mu(B)$ for any two sets $A,B \in \mathcal{A}$
        \item[(ii)] $\mu$ is monotone. If $\mathcal{A}$ is a ring, then $\mu(B) = \mu(A) + \mu(B\backslash A)$ for any two sets $A,B \in \mathcal{A}$ with $A\subseteq B$.
        \item[(iii)] $\mu$ is subadditive. If $\mu$ is $\sigma$-additive, then $\mu$ is also $\sigma$-subadditive.
        \item[(iv)] If $\mathcal{A}$ is a ring, then $\sum_{n=1}^{\infty}\mu(A_n) \leq \mu\left(\bigsqcup_{n=1}^{\infty}A_n\right)$ for any choice of countably many mutually disjoint sets $A_1,A_2,... \in \mathcal{A}$ with $\bigsqcup_{n=1}^{\infty}A_n \in \mathcal{A}$
    \end{itemize}
\end{lem}


\begin{thm}
    Let $\mathcal{A}$ be a ring and let $\mu$ be a content on $\mathcal{A}$. Let $n \in \N$ and $A_1,...,A_n \in \mathcal{A}$. Then the following inclusion and exclusion formulas hold: \begin{align*}
        \mu(A_1\cup....\cup A_n) &= \sum_{k=1}^n(-1)^{k-1}\sum_{\{i_1,..,i_k\}\subseteq \{1,...,n\}}\mu(A_{i_1}\cap...\cap A_{i_k}), \\
        \mu(A_1\cap ...\cap A_n) &= \sum_{k=1}^n(-1)^{k-1}\sum_{\{i_1,...,i_k\}\subseteq \{1,...,n\}}\mu(A_{i_1}\cup...\cup A_{i_k})
    \end{align*}
    where the second summation is over all subsets of $\{1,...,n\}$ with $k$ elements.
\end{thm}


\begin{defn}
    Let $A,A_1,A_2,...$ be sets. We write \begin{itemize}
        \item $A_n\uparrow A$ and say that $(A_n)_{n\in \N}$ increases to $A$ if $A_1\subseteq A_2\subseteq ...$ and $\bigcup_{n=1}^{\infty}A_n = A$
        \item $A_n\downarrow A$ and say $(A_n)_{n\in \N}$ decreases to $A$ if $A_1\supseteq A_2\supseteq ...$ and $\bigcap_{n=1}^{\infty}A_n = A$
    \end{itemize}
\end{defn}

\begin{defn}[Continuity of Contents]
    Let $\mu$ be a content on the ring $\mathcal{A}$. \begin{itemize}
        \item[(i)] $\mu$ is called \Emph{lower semicontinuous} if $\mu(A_n)\xrightarrow[n\rightarrow \infty]\mu(A)$ for any $A \in \mathcal{A}$ and any sequence $(A_n)_{n\in \N}$ in $\mathcal{A}$ with $A_n\uparrow A$
        \item[(ii)] $\mu$ is called \Emph{upper semicontinuous} if $\mu(A_n)\xrightarrow[n\rightarrow \infty]\mu(A)$ for any $A \in \mathcal{A}$ and any sequence $(A_n)_{n \in \N}$ in $\mathcal{A}$ with $\mu(A_n) < \infty$ for some (and then eventually all) $n \in \N$ and $A_n \downarrow A$.
        \item[(iii)] $\mu$ is called \Emph{$\emptyset$-continuous} if (ii) holds for $A = \emptyset$.
    \end{itemize}
\end{defn}

\begin{thm}[Continuity and Premeasure]
    Let $\mu$ be a content on the ring $\mathcal{A}$. Consider the following properties: \begin{itemize}
        \item $\mu$ is $\sigma$-additive
        \item $\mu$ is $\sigma$-subadditive
        \item $\mu$ is lower semicontinuous 
        \item $\mu$ is $\emptyset$-continuous 
        \item $\mu$ is upper semicontinuous
    \end{itemize}
    Then the following implications hold: \begin{equation*}
        (i) \iff (ii) \iff (iii) \implies (iv) \iff (v)
    \end{equation*}
    If $\mu$ is finite, then we also have $(iv) \implies (iii)$.
\end{thm}

\begin{defn}
    \leavevmode
    \begin{itemize}
        \item A pair $(\Omega,\mathcal{A})$ consisting of a nonempty set $\Omega$ and a $\sigma$-algebra $\mathcal{A}\subseteq 2^{\Omega}$ is called a \Emph{measurable space}. The sets $A \in \mathcal{A}$ are called \Emph{measurable sets}. If $\Omega$ is at most countably infinite and if $A = 2^{\Omega}$, then the measurable space $(\Omega,2^{\Omega})$ is called \Emph{discrete}.
        \item A triple $(\Omega,\mathcal{A},\mu)$ is called a \Emph{measure space} if $(\Omega,\mathcal{A})$ is a measurable space and if $\mu$ is a measure on $\mathcal{A}$.
        \item If in addition $\mu(\Omega) = 1$, then $(\Omega,\mathcal{A},\mu)$ is called a \Emph{probability space}. In this case, the sets $A \in \mathcal{A}$ are called \Emph{events}.
        \item The set of all finite measures on $(\Omega,\mathcal{A})$ is denoted $\mathcal{M}_f(\Omega) := \mathcal{M}_f(\Omega,\mathcal{A})$. The subset of probability measures is denoted by $\mathcal{M}_1(\Omega) := \mathcal{M}_1(\Omega, \mathcal{A})$. Finally, the set of $\sigma$-finite measures on $(\Omega,\mathcal{A})$ is denoted by $M_{\sigma}(\Omega,\mathcal{A})$.
    \end{itemize}
\end{defn}


\subsection{The Measure Extension Theorem}

We wish to now construct measures $\mu$ on $\sigma$-algebras. We define $\mu$ on a smaller class of sets, a semiring, and then extend to the whole $\sigma$-algebra.

\begin{eg}
    Let $n \in \N$ and let $\mathcal{A} = \{(a,b]:a,b \in \R^n, a < b\}$, where $a < b$ implies $a_i < b_i$ for all $1 \leq i \leq n$, and $(a,b] = (a_1,b_1]\times ... \times (a_n,b_n]$. Then $\mathcal{A}$ is a semiring of half-open rectangles $(a,b] \in \R^n$. The $n$-dimensional volume of such a rectangle is \begin{equation*}
        \mu((a,b]) = \prod_{i=1}^n(b_i - a_i)
    \end{equation*}
    Extending this set function to a measure on the Borel $\sigma$-algebra $\mathcal{B}(\R^n) = \sigma(\mathcal{A})$ gives the Lebesgue measure.
\end{eg}

\begin{namthm}[Caratheodory]
    Let $\mathcal{A} \subseteq 2^{\Omega}$ be a ring and let $\mu$ be a $\sigma$-finite premeasure on $\mathcal{A}$. There exists a unique measure $\tilde{\mu}$ on $\sigma(\mathcal{A})$ such that $\tilde{\mu}(A) = \mu(A)$ for all $A \in \mathcal{A}$. Furthermore, $\tilde{\mu}$ is $\sigma$-finite.
\end{namthm}

\begin{lem}
    Let $(\Omega,\mathcal{A},\mu)$ be a $\sigma$-finite measure space and let $\mathcal{E}\subseteq \mathcal{A}$ be a $\pi$-system that generates $\mathcal{A}$. Assume that there exist sets $E_1,E_2,... \in \mathcal{E}$ such that $E_n\uparrow \Omega$ and $\mu(E_n) < \infty$ for all $n \in \N$. Then $\mu$ is uniquely determined by the values $\mu(E)$, $E \in \mathcal{E}$.
\end{lem}


\begin{defn}
    A set function $\mu^*:2^{\Omega}\rightarrow [0,\infty]$ is called an \Emph{outer measure} if \begin{enumerate}[label=\roman*]
        \item $\mu^*(\emptyset) = 0$ 
        \item $\mu^*$ is monotone,
        \item $\mu^*$ is $\sigma$-subadditive
    \end{enumerate}
\end{defn}

\begin{lem}
    Let $\mathcal{A}\subseteq 2^{\Omega}$ be an arbitrary class of sets with $\emptyset \in \mathcal{A}$ and let $\mu$ be a monotone set function on $\mathcal{A}$ with $\mu(\emptyset) = 0$. For $A \subseteq \Omega$, define the set of countable coverings of $A$ with sets $F \in \mathcal{A}$: \begin{equation*}
        \mathcal{U}(A) = \left\{\mathcal{F} \subseteq \mathcal{A}:\mathcal{F} \text{ is at most countable and } A \subseteq \bigcup_{F\in\mathcal{F}}F\right\}
    \end{equation*}
    Define \begin{equation*}
        \mu^*(A) := \inf\left\{\sum_{F\in\mathcal{F}}\mu(F):\mathcal{F}\in\mathcal{U}(A)\right\}
    \end{equation*}
    where $\inf\emptyset := \infty$. Then $\mu^*$ is an outer measure. If in addition $\mu$ is $\sigma$-subadditive, then $\mu^*(A) = \mu(A)$ for all $A \in \mathcal{A}$.
\end{lem}



\subsection{Measurable Maps}


\subsection{Physical Interpretation}

Conventially we have two main interpretations of probabilities:

\begin{itemize}
    \item \Emph{Objectivist}: Probabilities can be measured experimentally from the relative frequency of occurrences of an event in repeated tests (ensemple): $P(E) = \lim\limits_{N\rightarrow \infty}\frac{N_E}{N}$, where $N_E$ is the number of times event $E$ occurred in $N$ tests. In the contest of statistical inference, this is also known as the \Emph{frequentist} school of thought. It believes that the probability distributions are intrinsic to the system (example: probabilities in quantum mechanics)
    \item \Emph{Subjectivist}: Probabilities are beliefs or theoretical estimates based on uncertainties related to a lack of precise knowledge of events. Thus, they cannot be measured. (Example: Single roll of a dice. There 6 possible outcomes, and without any prior info we cannot pick one outcome over the others such that we need to assume that all events consisting of single outcomes are equally likely, $P(\{1\}) = 1/6$. In the context of statistical inference this is called the Bayesian school of thought).
\end{itemize}

%%%%%%%%%%%%%%%%%%%% Section 3.1.2
\section{Random variables}

We wish to model one or more random experiments as a probability space $(\Omega,\mathcal{A},\mathbf{P})$. The sets $A \in \mathcal{A}$ are called \Emph{events}. Random observations, or variables, are measurable maps. The probabilities of the possible random observations are described in terms of the distribution of the corresponding random variable, which is the image measure of $\mathbf{P}$ inder $X$.

\begin{defn}[Random Variables]
    Let $(\Omega',\mathcal{A}')$ be a measurable space and let $X:\Omega\rightarrow \Omega'$ be measurable. \begin{itemize}
        \item[(i)] $X$ is called a \Emph{random variable} with values in $(\Omega',\mathcal{A}')$. If $(\Omega',\mathcal{A}') = (\R,\mathcal{B}(\R))$, then $X$ is called a real random variable or simply a random variable.
        \item[(ii)] For $A' \in \mathcal{A}'$, we denote $\{X \in A'\} := X^{-1}(A')$ and $\mathbf{P}[X\in A'] := \mathbf{P}[X^{-1}(A')]$. In particular, we let $\{X\geq 0\} := X^{-1}([0,\infty))$ and define $\{X\leq b\}$ similarly, and so on.
    \end{itemize}
\end{defn}

\begin{defn}[Distributions]
    Let $X$ be a random variable. \begin{itemize}
        \item[(i)] The probability measure $\mathbf{P}_X := \mathbf{P}\circ X^{-1}$ is called the \Emph{distribution} of $X$.
        \item[(ii)] For a real random variable $X$, the map $F_X:x\mapsto \mathbf{P}[X\leq x]$ is called the \Emph{distribution function of $X$} (or, more accurately, of $\mathbf{P}_X$). We write $X\sim \mu$ if $\mu = \mathbf{P}_X$ and say that $X$ has distribution $\mu$.
        \item[(iii)] A family $(X_i)_{i \in I}$ of random variables is called \Emph{identically distributed} if $\mathbf{P}_{X_i} = \mathbf{P}_{X_j}$ for all $i,j \in I$. We write $X =^{\mathcal{D}} Y$ if $\mathbf{P}_X = \mathbf{P}_Y$ ($\mathcal{D}$ for distribution)
    \end{itemize}
\end{defn}

\begin{thm}
    For any distribution function $F$, there exists a real random variable $X$ with $F_X = F$.
\end{thm}

We now present prominent distributions of real random variables, before going into the specifics for thermodynamics:

\begin{eg}
    \leavevmode
    \begin{enumerate}[label=\roman*]
        \item Let $p \in [0,1]$ and let $\mathbf{P}[X=1] = p,\mathbf{P}[X=0] = 1-p$. Then $\mathbf{P}_X =: \text{Ber}_p$ is called the \Emph{Bernoulli distribution} with parameter $p$; formally, \begin{equation*}
                \text{Ber}_p = (1-p)\delta_0+p\delta_1
        \end{equation*}
            Its distribution function is \begin{equation*}
                F_X(x) = \left\{\begin{array}{cc} 0, & \text{if } x < 0, \\ 1-p, & \text{if } x \in [0,1) \\ 1, & \text{if } x \geq \end{array}\right.
            \end{equation*}
        \item Let $p \in [0,1]$ and $n \in \N$, and let $X:\Omega\rightarrow \{0,...,n\}$ be such that \begin{equation*}
                \mathbf{P}[X=k] = \begin{pmatrix} n \\ k \end{pmatrix} p^k(1-p)^{n-k}
        \end{equation*}
            Then $\mathbf{P}_X =: b_{n,p}$ is called the \Emph{binomial distribution} with parameters $n$ and $p$; formally \begin{equation*}
                b_{n,p} = \sum_{k=0}^n\begin{pmatrix} n\\ k\end{pmatrix}p^k(1-p)^{n-k}\delta_k
            \end{equation*}
        \item Let $p \in (0,1]$ and $X:\Omega\rightarrow \N_0$ with \begin{equation*}
                \mathbf{P}[X=n] = p(1-p)^n\;\text{ for any } n \in \N_0
        \end{equation*}
            Then $\gamma_p := b^-_{1,p} := \mathbf{P}_X$ is called the \Emph{geometric distribution} with parameter $p$; formally \begin{equation*}
                \gamma_p = \sum_{n=0}^{\infty}p(1-p)^n\delta_n
            \end{equation*}
            Its distribution function is $F(x) = 1-(1-p)^{\lfloor x+1\rfloor\lor0}$ for $x \in \R$.
        \item Let $r > 0$ and let $p \in (0,1]$. We denote \begin{equation*}
                b^-_{r,p} := \sum_{k=0}^{\infty}\begin{pmatrix} -r \\ k\end{pmatrix}(-1)^kp^r(1-p)^k\delta_k
        \end{equation*}
            the \Emph{negative binomial distribution} with parameters $r$ and $p$, and where $\begin{pmatrix} x\\ k\end{pmatrix} = \frac{x(x-1)...(x-k+1)}{k!}$ is the generalized binomial coefficient.
            \item Let $\lambda \in [0,\infty)$ and let $X:\Omega\rightarrow \N_0$ be such that \begin{equation*}
                    \mathbf{P}[X = n] = e^{-\lambda}\frac{\lambda^n}{n!}\;\text{ for any } n \in \N_0
            \end{equation*}
            Then $\mathbf{P}_X =: \text{Poi}_{\lambda}$ is the \Emph{Poisson distribution} with parameter $\lambda$.
        \item Consider an urn with $B \in \N$ black balls and $W \in \N$ white balls. Draw $n \in \N$ balls from the urn without replacement. The probability of drawing exactly $b \in \{0,...,n\}$ black balls is given by the \Emph{hypergeometric distribution} with parameters $B,W,n \in \N$: \begin{equation*}
                \text{Hyp}_{B,W;n}(\{b\}) = \frac{\begin{pmatrix} B \\ b\end{pmatrix}\begin{pmatrix} W\\ n-b\end{pmatrix}}{\begin{pmatrix} B+W \\ n\end{pmatrix}},\;b\in\{0,...,n\}
        \end{equation*}
            Generally, the probability of drawing out of $n$ balls exactly $b_i$ of which of colour $i$ for each $i = 1,...,k$, with the restriction $b_1+...+b_k = n$ and $b_i \leq B_i$ for all $i$, is given by the \Emph{generalized hypergeometric distribution} \begin{equation*}
                \text{Hyp}_{B_1,...,B_k;n}(\{(b_1,...,b_k)\}) = \frac{\begin{pmatrix} B_1 \\ b_1\end{pmatrix}...\begin{pmatrix} B_k\\ b_k\end{pmatrix}}{\begin{pmatrix} B_1+...+B_k \\ n\end{pmatrix}},
            \end{equation*}
        \item Let $\mu \in \R$, $\sigma^2 > 0$ and let $X$ be a real random variable with \begin{equation*}
                \mathbf{P}[X\leq x] = \frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^xe^{-\frac{(t-\mu)^2}{2\sigma^2}}dt\;\text{ for } x \in \R
        \end{equation*}
            Then $\mathbf{P}_X =: \mathcal{N}_{\mu,\sigma^2}$ is called the \Emph{Gaussian normal distribution} with parameters $\mu$ and $\sigma^2$. In particular, $\mathcal{N}_{0,1}$ is called the standard normal distribution.
        \item Let $\theta > 0$ and let $X $ be a nonnegative random variable such that \begin{equation*}
                \mathbf{P}[X\leq x] = \mathbf{P}[X \in [0,x]] = \int_0^x\theta e^{-\theta t}dt\;\text{ for } x \geq 0
        \end{equation*}
            Then $\mathbf{P}_X$ is called the \Emph{exponential distribution} with parameter $\theta$.
    \end{enumerate}
\end{eg}

\begin{defn}
    If the distribution function $F:\R^n\rightarrow [0,1]$ is of the form \begin{equation*}
        F(x) = \int_{-\infty}^{x_1}dt_1...\int_{-\infty}^{x_n}dt_nf(t_1,...,t_n) \;\text{ for } x = (x_1,...,x_n) \in \R^n
    \end{equation*}
    for some integrable function $f:\R^n\rightarrow [0,\infty)$, then $f$ is called the \Emph{density} of the distribution.
\end{defn}

\begin{eg}
    \leavevmode
    \begin{enumerate}[label=\roman*]
        \item Let $\theta, r > 0$ and let $\Gamma_{\theta,r}$ be the distribution on $[0,\theta)$ with density \begin{equation*}
                x\mapsto \frac{\theta^r}{\Gamma(r)}x^{r-1}e^{-\theta x}
        \end{equation*}
            where $\Gamma(r) = \int_0^{t}\theta^rx^{r-1}e^{-\theta x}dx$ is the gamma function. Then $\Gamma_{\theta,r}$ is called the \Emph{Gamma distribution} with scale parameter $\theta$ and shape parameter $r$.
        \item Let $r,s > 0$ and let $\beta_{r,s}$ be the distribution on $[0,1]$ with density \begin{equation*}
                x\mapsto \frac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)}x^{r-1}(1-x)^{s-1}
        \end{equation*}
            Then $\beta_{r,s}$ is called the \Emph{Beta distribution} with parameters $r$ and $s$.
    \end{enumerate}
\end{eg}


%%%%%%%%%%%%%%%%%%%% Section 3.1.3
\section{States and Combined Events}

\begin{defn}
    A \Emph{microstate} is a specific outcome or elementary event.
\end{defn}

\begin{defn}
    A \Emph{macrostate} is a collection or union of microstates with some common property.
\end{defn}

Consider the example of a gas. In this case, a microstate would correspond to a specific initial condition of all particles and a macrostate could correspond to all those microstates with the same $U,N,$ and $V$. 

For the Bernoulli process, we have \begin{align*}
    P_{micro,N} &= P_H^{n_H}P_T^{n_T} = P_H^{n_H}(1-P_H)^{N-n_H} = P_{micro,N}(n_H) \\
    P_{macro, N} &= \begin{pmatrix} N \\ n_H \end{pmatrix}P_H^{n_H}P_T^{n_T} = \begin{pmatrix} N \\ n_H \end{pmatrix}P_H^{n_H}(1-P_H)^{N-n_H} = P_{macro,N}(n_H)
\end{align*}
where $n_H$ is the number of times heads occurs, $n_T$ is the number of times tails occurs, and we have used $N = n_H + n_T$ and $P_H + P_T = 1$. Observe that \begin{equation*}
    \sum_{n_H=0}^NP_{macro,N}(n_H) = \sum_{n_H=0}^N\begin{pmatrix} N \\ n_H \end{pmatrix}P_H^{n_H}(1-P_H)^{N-n_H} = (P_H + (1-P_H))^N = 1
\end{equation*}
so $P_{macro,N}$ is normalized, as required. For a collection of independent identically distributed Bernoulli processes, we have the \Emph{binomial distribution} \begin{equation*}
    P_N(n) = \begin{pmatrix} N \\ n\end{pmatrix} P^n(1-P)^{N-n}
\end{equation*}

Now, we look at the construction of probabilities of events from other events: \begin{enumerate}[label=\roman*]
    \item $P(E_1\cup E_2) = P(E_2 \cup E_1) = P(E_1)+P(E_2)$ provided $E_1\cap E_2 = \emptyset$
    \item If $E_1$ and $E_2$ are independent events, then $P(E_1\cap E_2) = P(E_2\cap E_1) = P(E_1)P(E_2)$
    \item $P(E_1\cup E_2) = P(E_1) + P(E_2) - P(E_1\cap E_2)$
    \item The \Emph{conditional probability} $P(E_2\vert E_1)$ corresponds to the probability that $E_2$ occurs given that $E_1$ occurred. By definition, this means that $P(E_2\vert E_1) = \frac{P(E_2\cap E_1)}{P(E_1)}$.
    \item We say that $E_1$ and $E_2$ are independent events if and only if $P(E_2\vert E_1) = P(E_2)$ and $P(E_1\vert E_2) = P(E_1)$.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%% Section 3.1.4
\section{Characteristics of Probability Distributions}

We often wish to investigate certain key parameters related to the probability distributions associated with the events we are observing. We first introduce these for the case of discrete random variables:

\begin{defn}
    The \Emph{mean} (or \Emph{average} or \Emph{expectation value}) for a discrete random variable $X$ is \begin{equation*}
        E[X] := \langle n \rangle := \sum_{n=0}^NnP_N(n)
    \end{equation*}
    where $P_N$ is the probability distribution of $X$.
\end{defn}
For the Binomial distribution we have a mean of \begin{align*}
    \langle n \rangle &= \sum_{n=0}^Nn\begin{pmatrix} N \\ n\end{pmatrix} P^n(1-P)^{N-n} \\
    &= \sum_{n=0}^Nn\frac{N!}{n!(N-n)!}P^n(1-P)^{N-n} \\
    &= N\sum_{n=1}^N\frac{(N-1)!}{(n-1)!(N-n)!}P^n(1-P)^{N-n} \\
    &= N\sum_{n=0}^{N-1}\begin{pmatrix} N-1 \\ n\end{pmatrix} P^{n+1}(1-P)^{N-n-1} \\
    &= NP\sum_{n=0}^{N-1}\begin{pmatrix} N-1 \\ n\end{pmatrix} P^{n}(1-P)^{N-1-n} \\
    &= NP
\end{align*}


\begin{defn}
    The \Emph{variance} of a discrete random variable $X$ is defined as follows \begin{align*}
        E[(X-E[X])^2] &:= \langle \Delta n^2\rangle := \langle (n-\langle n\rangle)^2\rangle := \sum_{n=0}^N(n-\langle n \rangle)^2P_N(n) \\
        &= \sum_{n=0}^N(n^2+\langle n \rangle^2-2n\langle n \rangle)P_N(n) = \sum_{n=0}^Nn^2P_N(n) + \langle n\rangle^2 - 2\langle n\rangle^2 \\
        &= \sum_{n=0}^Nn^2P_N(n) - \langle n\rangle^2 =: E[X^2] - E[X]^2
    \end{align*}
    where we have used the fact that $P_N(n)$ is normalized and the definition of the mean.
\end{defn}

For a binomial distribution we have the following variance: 
\begin{align*}
    E[X]^2 - E[X]^2 &= \sum_{n=0}^Nn^2\begin{pmatrix} N \\ n\end{pmatrix} P^n(1-P)^{N-n} - N^2P^2 \\
    &= N\sum_{n=1}^Nn\frac{(N-1)!}{(n-1)!(N-n)!}P^n(1-P)^{N-n} - N^2P^2 \\
    &= NP\sum_{n=0}^{N-1}(n+1)\begin{pmatrix} N-1 \\ n\end{pmatrix} P^{n}(1-P)^{N-1-n} -N^2P^2 \\
    &= NP\left[\sum_{n=0}^{N-1}n\frac{(N-1)!}{n!(N-1-n)!}P^n(1-P)^{N-1-n} + \sum_{n=0}^{N-1}\begin{pmatrix} N-1 \\ n\end{pmatrix} P^{n}(1-P)^{N-1-n}\right] - N^2P^2 \\
    &= NP\left[\sum_{n=1}^{N-1}\frac{(N-1)!}{(n-1)!(N-1-n)!}P^n(1-P)^{N-1-n} + 1\right] - N^2P^2 \\
    &= NP\left[(N-1)P\sum_{n=0}^{N-2}\frac{(N-2)!}{n!(N-2-n)!}P^n(1-P)^{N-2-n}+1\right] - N^2P^2 \\
    &= NP\left[(N-1)P+1\right] -N^2P^2 \\
    &= N^2P^2 - NP^2+NP - N^2P^2 = NP(1-P)
\end{align*}

\begin{defn}
    The \Emph{standard deviation} of a random variable $X$ is the square root of its variance: \begin{equation*}
        SD[X] := \sqrt{E[(X-E[X])]^2} := \sigma := \sqrt{\langle \Delta n^2\rangle}
    \end{equation*}
\end{defn}

Evidently the standard deviation of a binomial distribution is $\sigma = \sqrt{NP(1-P)}$.

\begin{defn}
    The \Emph{relative uncertainty} of a random variable $X$ is the ratio $\frac{SD[X]}{E[X]} = \frac{\sigma}{\langle n \rangle}$.
\end{defn}


For example, the relative uncerainty of the binomial distribution is \begin{equation*}
    \frac{\sigma}{E[X]} = \frac{\sqrt{NP(1-P}}{PN} \propto \frac{1}{\sqrt{N}}
\end{equation*}
which goes to $0$ as $N\rightarrow \infty$. This is a form of the \Emph{law of large numbers}.


%%%%%%%%%%%%%%%%%%%%%% Chapter 3.2
\chapter{Continuous Random Variables and the Gaussian Distribution}

%%%%%%%%%%%%%%%%%%%% Section 3.2.1
\section{Gaussian Distribution and Continuous Random Variables}

\begin{defn}
    \Emph{Stirling's Formula} is defined to be \begin{equation*}
        N! \approx \left(\frac{N}{e}\right)^N\sqrt{2\pi N}
    \end{equation*}
    which becomes exact in the limit of $N\rightarrow \infty$.
\end{defn}
We use this formula to analyze the binomial distribution for $N\rightarrow \infty$: \begin{align*}
    P_N(n) &= \begin{pmatrix} N \\ n\end{pmatrix} P^n(1-P)^{N-n} = \frac{N!}{(N-n)!n!}P^n(1-P)^{N-n} \\
        &\approx \left(\frac{N}{e}\right)^N\sqrt{2\pi N}\left(\frac{e}{N-n}\right)^{N-n}\frac{1}{\sqrt{2\pi(N-n)}}\left(\frac{e}{n}\right)^n\frac{1}{\sqrt{2\pi n}}P^n(1-P)^{N-n} \\
        &= \sqrt{\frac{N}{2\pi n(N-n)}}N^N\left(\frac{P}{n}\right)^n\left(\frac{1-P}{N-n}\right)^{N-n}
\end{align*}

We now substitute in $x \equiv \frac{n - \langle n\rangle}{N}$, with $\langle n \rangle = NP$, to investigate the expansion around the mean of the Binomial distribution:
\begin{align*}
    P_N(x) \approx \sqrt{\frac{N}{2\pi (n-NP)(N-(n-NP)/N)/N}}N^N\left(\frac{P}{(n-NP)/N}\right)^{(n-NP)/N}\left(\frac{1-P}{N-(n-NP)/N}\right)^{N-(n-NP)/N} \\
    &= \frac{1}{\sqrt{2\pi \sigma^2}}\left(\frac{1}{1+\frac{x}{P}}\right)^{1/2+NP(1+x/P)}\left(\frac{1}{1-\frac{x}{1-P}}\right)^{1/2+N(1-P)(1-x/(1-P))} 
\end{align*}
where $\sigma = \sqrt{NP(1-P)}$. We take the natural log of both sides to obtain \begin{equation*}
    \ln P_N(x) = -\frac{1}{2}\ln(2\pi\sigma^2) - \left[\frac{1}{2}+NP\left(1+\frac{x}{P}\right)\right]\ln\left(1+\frac{x}{P}\right) - \left[\frac{1}{2}+N(1-P)\left(1-\frac{x}{1-P}\right)\right]\ln\left(1-\frac{x}{1-P}\right)
\end{equation*}
We need an expansion for $x << 1$, so we use \begin{equation*}
    \ln(1+\varepsilon) = \varepsilon - \frac{\varepsilon^2}{2} + O(\varepsilon^3)
\end{equation*}
This gives \begin{equation*}
    \ln P_N(x) = -\frac{1}{2}\ln(2\pi\sigma^2) - \left[\frac{1}{2}+NP\left(1+\frac{x}{P}\right)\right]\left(\frac{x}{P}-\frac{x^2}{2P^2}\right) - \left[\frac{1}{2}+N(1-P)\left(1-\frac{x}{1-P}\right)\right]\left(-\frac{x}{1-P}-\frac{x^2}{2(1-P)^2}\right)
\end{equation*}
Since we are taking $N\rightarrow \infty$, we can ignore the $1/2$ terms in each pair of brackets, so we have \begin{align*}
    \ln P_N(x) &\approx -\frac{1}{2}\ln(2\pi\sigma^2) - \left[NP+Nx\right]\left(\frac{x}{P}-\frac{x^2}{2P^2}\right) - \left[N(1-P)-Nx\right]\left(-\frac{x}{1-P}-\frac{x^2}{2(1-P)^2}\right) \\
    &\approx -\frac{1}{2}\ln(2\pi \sigma^2) - x(N-N)-x^2\left(\frac{N}{P} - \frac{N}{2P} + \frac{N}{1-P} - \frac{N}{2(1-P)}\right) \\
    &= -\frac{1}{2}\ln(2\pi \sigma^2) -\frac{1}{2}x^2N\left(\frac{1}{P}+\frac{1}{1-P}\right) \\
    &= -\frac{1}{2}\ln(2\pi \sigma^2) - \frac{1}{2}x^2 \frac{N^2}{\sigma^2}
\end{align*}
neglecting all terms of order $x^3$ and higher since $x << 1$ as we take $N\rightarrow \infty$. Thus, converting back from the $\ln$ we have found that the Gaussian distribution is a good approximation for the Binomial distribution for $N\rightarrow \infty$: \begin{equation*}
    P_N(n) \approx \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{(n-\langle n\rangle)^2}{2\sigma^2}\right\}
\end{equation*}

\subsection{Properties of Continuous Random Variables}

Let $(\R,\mathcal{B}(\R,\tau),P)$ denote a probability space over the real line with the Borel $\sigma$-algebra.

\begin{defn}
    The \Emph{cumulative distribution function}, CDF, for a continuous random variable $X$, is a function $F_X:\R\rightarrow [0,1]$ defined by $F_X(\lambda) = P(\{X\leq \lambda\}) = P(X\leq \lambda)$. In particular, $F_X(\lambda) = P\circ X^{-1}((-\infty, \lambda])$, where $P:\mathcal{B}(\R,\tau)\rightarrow \R$ is the probability measure for the space and $X:\R\rightarrow \R$ is the random variable of interest. The CDF satisfies the following properties, as a consequence of the probability measure $P$: \begin{enumerate}
        \item $F_X$ is monotonically increasing
        \item $\lim_{x\rightarrow -\infty}F_X(x) = 0$
        \item $\lim_{x\rightarrow \infty}F_X(x) = 1$
    \end{enumerate}
\end{defn}

\begin{defn}
    The \Emph{probability density function} for a continuous random variable $X$ is a function $p_X$ defined by if $F_X$ is ther CDF for $X$, then \begin{equation*}
        F_X(\lambda) = \int_{-\infty}^{\lambda}p_X(x)dx
    \end{equation*}
    Further, $p_X$ satisfies positivity, so $p_X:\R\rightarrow [0,\infty)$. Further, from properties of the CDF, \begin{equation*}
        \lim\limits_{\lambda\rightarrow \infty}F_X(\lambda) = \int_{-\infty}^{\infty}p_X(x)dx = 1
    \end{equation*}
\end{defn}

Now we characterize properties of a continuous random variable using its probability density function: 

\begin{defn}
    Let $X$ be a random variable with pdf $p_X$. The mean of $X$ is defined to be \begin{equation*}
        E[X] := \int_{-\infty}^{\infty}xp_X(x)dx
    \end{equation*}
    In general, if $g:\R\rightarrow \R$ is a continuous function, then \begin{equation*}
        E[g(X)] = E[g\circ X] := \int_{-\infty}^{\infty}g(x)p_X(x)dx
    \end{equation*}
\end{defn}

\begin{defn}
    The variance of a continuous random variable $X$ with pdf $p_X$ follows from the last definition, with \begin{equation*}
        \langle \Delta x^2\rangle := VAR[X] := E[(X-E[X])^2] = E[X^2] - E[X]^2 = \int_{-\infty}^{\infty}X^2p_X(x)dx - E[X]^2
    \end{equation*}
\end{defn}

\begin{defn}
    The pdf for a Gaussian random variable $X$ with mean $\mu$ and variance $\sigma^2$ is \begin{equation*}
        p_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}
    \end{equation*}
\end{defn}

\subsection{Change of Variables}

\begin{thm}
    If $X:\R^n\rightarrow \R^n$ is a random variable with continuous pdf $p_X:\R^n\rightarrow [0,\infty)$, and $\varphi:A\rightarrow B$ is a diffeomorphism with $A \subseteq \R^n$ open or closed with $P(\R^n\backslash A) = 0$ and $B \subseteq \R^n$ also open or closed, then the image measure $P\circ X^{-1} \circ \varphi^{-1}$ has the pdf \begin{equation*}
        p_{\varphi}(x) = \left\{\begin{array}{rc} \frac{p_X(\varphi^{-1}(x))}{\left|\det(\varphi'(\varphi^{-1}(x)))\right|}, & \text{if } x \in B, \\ 0, & \text{if } x \in B^c\end{array}\right\}
    \end{equation*}
\end{thm}
In the one dimensional case we have \begin{equation*}
    P(F(X) \leq \lambda) = \int_{-\infty}^{\lambda}p_F(x)dx = P(X\leq F^{-1}(\lambda)) = \int_{-\infty}^{F^{-1}(\lambda)}p_X(x)dx = \int_{-\infty}^{\lambda}p_X(F^{-1}(y))\left|\frac{dF^{-1}}{dy}(y)\right|dy
\end{equation*}
using change of variables and the inverse function theorem.

\begin{defn}
    If $U = h(Y)$, where $h(y)$ is injective and $Y$ is a real random variable with pdf $f_Y(y) > 0$, then $$f_U(u) = f_Y(h^{-1}(u))\left|\frac{d}{du}(h^{-1}(u))\right|$$
\end{defn}


\begin{eg}
    Consider a particle in one spatial dimension. Its momentum is a random variable with pdf $p(x) = \frac{\lambda}{2}\exp\{-\lambda|x|\}$. Its kinetic energy is given in dimensionless form $Y = F(x) = x^2$. Then, using the method of transformations we have that $$p_F(y) = (p(\sqrt{y})+p(-\sqrt{y}))\left|\frac{d}{dy}\sqrt{y}\right| = \frac{\lambda}{2\sqrt{y}}\exp\left\{-\lambda|x|\right\}$$
\end{eg}



%%%%%%%%%%%%%%%%%%%% Section 3.2.2
\section{Multivariate Random Variables and the Central Limit Theorem}

We now consider the cases where our probability space is $(\R^N,\mathcal{B}(\R^N,\tau),P)$.

\begin{defn}
    The \Emph{Joint PDF}, $p(X)$, for a random variable $X = (X_1,...,X_N)$, is the probability density of an outcome in a volume element $d^NX = \prod_{i=1}^Ndx_i$. The CDF associated to this pdf for $x = (x_1,...,x_N)$ is \begin{equation*}
        F_X(x) = \int_{-\infty}^{x_1}dx_1...\int_{-\infty}^{x_N}dx_Np(x_1,...,x_N)
    \end{equation*}
\end{defn}

\begin{defn}
    The $N$ random variables, $X_1,...,X_N$, are said to be \Emph{independent} if and only if the joint pdf factorizes: \begin{equation*}
        p(X) = \prod_{i=1}^Np_i(X_i)
    \end{equation*}
\end{defn}

\begin{defn}
    The \Emph{unconditional or marginal PDF} describes the behaviour of a subset of $m < N$ random variables, independent of the values of the others: \begin{equation*}
        p(x_1,...,x_m) := \int\prod_{i=m+1}^Ndx_ip(x_1,x_2,...,x_N) = \int_{-\infty}^{\infty}dx_{m+1}...\int_{-\infty}^{\infty}dx_Np(x_1,...,x_N)
    \end{equation*}
\end{defn}

\begin{defn}
    The \Emph{conditional PDF} describes the behaviour of a subset of $m < N$ random variables for fixed values of the others: \begin{equation*}
        p(x_1,...,x_m\vert x_{m+1},...,x_N) := \frac{p(x_1,...,x_N)}{p(x_{m+1},...,x_N)}
    \end{equation*}
    where $x_{m+1},...,x_N$ are fixed.
\end{defn}

\begin{namthm}[Central Limit Theorem]
    Consider $N$ independent identically distributed (i.i.d) random variables $X_1,...,X_N$ with $E[X_i] = \mu$ and $V[X_i] = \sigma^2$ for all $i$. Then $\overline{X} = \frac{1}{N}\sum_{i=1}^NX_i$ follows a Gaussian distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$ in the limit of large $N$.
\end{namthm}







%%%%%%%%%%%%%%%%%%%%%% Chapter 3.3
\chapter{Information and Entropy}


%%%%%%%%%%%%%%%%%%%% Section 3.3.1
\section{Introduction to Information Theory}

What is information really? Here is an attempt at a precise definition:

\begin{defn}
    \Emph{Information} is the average number of (minimal) yes-no questions one has to ask to determine the state of a system.
\end{defn}

\begin{eg}
    Consider an $8$ by $8$ chess board. If we only have a single piece, the state of the system would correspond to the location of the piece and in total we would have $M = 8^2$ different states. One can determine the state of the board by asking if the piece is in the upper or lower half, then proceeding to ask about the right or left half, halving the board each time until the piece is found. It follows that the average number of minimal yes-no questions one has to ask is $I = 6$, and we can generalize this by observing that \begin{equation*}
        I = 6 = 6\log_22 = \log_22^6 = \log_28^2 = \log_2M
    \end{equation*}
\end{eg}

We could have also defined information as follows: 
\begin{defn}
    \Emph{Information} is the gain in knowledge one obtains if the state of a system is revealed.
\end{defn}

We use this to address the question of how much information one gains if the probability distribution associated with the states of a system is revealed.

%%%%%%%%%%%%%%%%%%%% Section 3.3.2
\section{Information Content of a Distribution}

Let $X$ be a discrete random variable with a finite set of outcomes $\Omega$, with $|\Omega| = M$. Let $\{P(i)\}$ for all $1 \leq i \leq M$ be the associated probabilities of the outcomes. Suppose we have $N$ independent outcomes. The \Emph{apparent information} is $I = \log_2M^N = N\log_2M$. However, because of the CLT, or law of large numbers, we know the number of tiems we observe outcome $i$ is $N_i \approx NP_i$ for $N\rightarrow \infty$. Thus, in this limit the number of typical sequences of outomes corresponds to the number of ways of rearranging the $N_i$ outcomes for all $i$: \begin{equation*}
    g \equiv \frac{N!}{\prod_{i=1}^MN_i!}
\end{equation*}
This is the \Emph{multinomial coefficient}. Modifying this expression we obtain \Emph{Shannon's Theorem}: \begin{align*}
    \log_2g &= \log_2\left[\frac{N!}{\prod_{i=1}^MN_i!}\right] \\
    &\approx \log_2\left[\left(\frac{N}{e}\right)^N\sqrt{2\pi N}\prod_{i=1}^M\left(\frac{e}{N_i}\right)^{N_i}\frac{1}{\sqrt{2\pi N_i}}\right] \\
    &= \log_2\left[N^N\sqrt{2\pi N}\prod_{i=1}^M\left(\frac{1}{NP_i}\right)^{NP_i}\frac{1}{\sqrt{2\pi NP_i}}\right] \\
    &= \log_2\left[\prod_{i=1}^M\left(\frac{1}{P_i}\right)^{NP_i}\frac{1}{\sqrt{P_i}}\right] - \frac{M-1}{2}\log_2(2\pi N) \\
    &= -N\sum_{i=1}^MP_i\log_2P_i - \frac{1}{2}\sum_{i=1}^M\log_2P_i - \frac{M-1}{2}\log_2(2\pi N)\\
    &\approx -N\sum_{i=1}^MP_i\log_2P_i,
\end{align*}
using Stirling's formula and the approximation of large $N$, so we have an equality for $N\rightarrow \infty$.

\begin{defn}
    We quantify the \Emph{information content of a probability distribution} is the difference between the apparent information and the logarithm of the number of typical sequences of outcomes given the probability distribution: \begin{equation*}
        I(\{P_i\}) = \frac{I-\log_2g}{N} = \log_2M + \sum_{i=1}^MP_i\log_2P_i
    \end{equation*}
\end{defn}

Instead of $I$ we often use a kind of inverse quanitty to characterize a probability distribution. In this case the quantity measures the lack of knowledge or uncertainty instead of the information content:

\begin{defn}
    The \Emph{Shannon entropy} for a probability distribution is defined as \begin{equation*}
        S \equiv -\sum_{i=1}^MP_i\ln P_i = -\langle \ln P\rangle \propto\frac{\log_2g}{N}
    \end{equation*}
\end{defn}

The Shannon entropy and thermodynamic entropy are essentially the same in certain cases if we scale by $k_B$ to get the correct units, and this connection relies on the maximum entropy principle.


%%%%%%%%%%%%%%%%%%%% Section 3.3.3
\section{Maximum Entropy Principle}

\begin{thm}
    The \Emph{Maximum Entropy Principle} states that the best unbiased estimate of probabilities for given constraints is the one that maximizes the Shannon entropy.
\end{thm}

If we have $M$ possible outcomes and $s$ constraints of the form $g_j(P_1,...,P_M) = 0$ for all $1 \leq j \leq s$, then if a function $f(P_1,...,P_M) = 0$ is maximized with respect to these constraints there exist $\lambda_j \in \R$, $1 \leq j \leq s$, such that \begin{equation*}
    \nabla f = \sum_{j=1}^s\lambda_j\nabla g_j
\end{equation*}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Part 4
\part{Real Gases and Phase Transitions}


%%%%%%%%%%%%%%%%%%%%%% Chapter 4.1
\chapter{Kinetic Theory of Gases}


Kinetic theory studies the macroscopic properties of large numbers of particles starting from their classical equations of motion.

%%%%%%%%%%%%%%%%%%%% Section 4.1.1
\section{Hamiltonian Dynamics and Phase Space}

\begin{thm}
    The system of $N$ second order Lagrange's equations is equivalent to the following system of $2N$ first order equations: \begin{equation*}
        \dot{p}_i = -\frac{\partial \mathcal{H}}{\partial q_i},\;\;\;\;\;\;\dot{q}_i = \frac{\partial \mathcal{H}}{\partial p_i}
    \end{equation*}
    for $1 \leq i \leq s$. Here, $\mathcal{H} = \mathcal{H}(q_1,...,q_s,p_1,...,p_s,t) \equiv \sum_{j=1}^s\dot{q}_jp_j - \mathcal{L}$ is a Legendre transform of the Lagrangian $\mathcal{L}(q_1,...,q_s,\dot{q}_1,...,\dot{q}_s,t)$ with generalized momenta $p_j = \frac{\partial \mathcal{L}}{\partial \dot{q}_j}$ and generalized coordinates $q_j$ called the \Emph{Hamiltonian}. The above equations are called \Emph{Hamilton's equations of motion}.
\end{thm}

For conservative systems with time-independent holonomic constraints, the Hamiltonian equals the total mechanical energy: \begin{equation*}
    \mathcal{H} = E_{kin} + E_{pot} = E_{tot}
\end{equation*}

Another important feature of the Hamiltonian is that it is constant if and only if it does not depend on time. Indeed, \begin{equation*}
    \frac{d\mathcal{H}}{dt} = \sum_i\left(\frac{\partial \mathcal{H}}{\partial q_i}\dot{q}_i + \frac{\partial \mathcal{H}}{p_i}\dot{p}_i\right) + \frac{\partial \mathcal{H}}{\partial t} = \sum_i(-\dot{p}_i\dot{q}_i + \dot{q}_i\dot{p}_i)+\frac{\partial \mathcal{H}}{\partial t} = \frac{\partial \mathcal{H}}{\partial t}
\end{equation*}


\begin{eg}
    Consider the example of a single particle in a potential $V = V(\vec{r})$. As this corresponds to a conservative system and there are no constraints we have for the Hamiltonian: \begin{equation*}
        H(\vec{r},\vec{p}) = E_{kin} + E_{pot} = \frac{\vec{p}\cdot \vec{p}}{2m} + V(\vec{r})
    \end{equation*}
    For Hamilton's equation of motion this gives: \begin{equation*}
        \dot{\vec{p}} = -\frac{\partial \mathcal{H}}{\partial \vec{r}} = -\frac{\partial V}{\partial \vec{r}} = -\nabla V,\;\;\;\;\;\;\dot{\vec{r}} = \frac{\partial \mathcal{H}}{\partial \vec{p}} = \frac{\vec{p}}{m}
    \end{equation*}
    which is equivalent to Newton's equations of motion, as exprected.

    In the special case of a 1D oscillator with $V(\vec{r}) = \frac{1}{2}kx^2$, where $k$ is the spring constant, we have $\dot{p}_x = -kx, \ddot{x} = \frac{\dot{p}_x}{m} = -\frac{k}{m}x$, and for the other coordinates we just have zeros. For the Hamiltonian in this case we have \begin{equation*}
        H(x,p_x) = \frac{p_x^2}{2m} + \frac{1}{2}kx^2 \equiv E_{tot} = const,
    \end{equation*}
    since the Hamiltonian does not explicitly depend on time. Thus, the dynamics of the system in \Emph{phase space} is simply an ellipse of axes $\sqrt{2mE_{tot}}$ and $\sqrt{\frac{2E_{tot}}{k}}$.
\end{eg}

\begin{defn}
    The \Emph{Phase Space} of a system of $N$ particles is the space spanned by the generalized momenta and coordinates used to describe the particles.
\end{defn}



%%%%%%%%%%%%%%%%%%%% Section 4.1.2
\section{Microstates, Macrostates, and Phase Space Density}

In this context a microstate corresponds to a state in phase space, which is simply an initial condition of Hamilton's equations of motion at a given moment in time $t$: \begin{equation*}
    \mu(t) = (\vec{q}_1(t),\vec{p}_1(t),...,\vec{q}_N(t),\vec{p}_N(t))
\end{equation*}
The dimension of phase space is at most $6N$ for $N$ particles, where the exact number is determined by the number of degrees of freedom, and one often uses the symbol $\Gamma$ to denote it such that one can write \begin{equation*}
    \mu(t) \in \Gamma = \prod_{i=1}^N\R^3\times \R^3
\end{equation*}

Macrostates in the context of kinetic thoery are, for example, all microstates with the same internal energy $U$, the same number of particles $N$, and the same volume $V$. For a given fixed macrostate one can then ask the question how likely it is to find the system in a specific microstate. To address this question we will often follow a frequentist approach and introduce a \Emph{statistical ensemble} of microstates:

\begin{defn}
    Let us consider $M$ copies of a given system in the same macrostate at fixed time $t$ such that we have $M$ microstates, $\mu_j(t) \in \Gamma$ with $1 \leq j \leq M$. If we denote the number of different $\mu_j(t)$'s in an infinitesimal volume element $d\Gamma = \prod_{i=1}^Nd^3\vec{q}_id^3\vec{p}_i$ around $(\vec{q},\vec{p})$ as $dM(\vec{q},\vec{p},t)$, then we can define the \Emph{phase space density} as \begin{equation*}
        \rho(\vec{q},\vec{p},t)d\Gamma = \lim\limits_{M\rightarrow \infty}\frac{dM(\vec{q},\vec{p},t)}{M}
    \end{equation*}
\end{defn}

This is simply a joint PDF in phase space. Since the particles are not stationary but move according to Hamilton's equations of motion, the phase density can vary over time. Using the phase space density one can calculate macroscopic values for observables or function, $F(\vec{q},\vec{p})$ as one would for pdfs and cdfs based on \Emph{ensemble averages}: \begin{equation}
    \langle F\rangle(t) := \int d\Gamma\rho(\vec{q},\vec{p},t)F(\vec{q},\vec{p})
\end{equation}




%%%%%%%%%%%%%%%%%%%% Section 4.1.3
\section{Liouville's Theorem}

Note that as the phase space density is a PDF, it must be normalized so \begin{equation*}
    1 = \int d\Gamma\rho(\vec{q},\vec{p},t) = \int\prod_{i=1}^Nd\vec{q}_id\vec{p}_i\rho(\vec{q},\vec{p},t) = \int d\vec{q}_1...\int d\vec{q}_N\int d\vec{p}_1...\int d\vec{p}_N\rho(\vec{q},\vec{p},t)
\end{equation*}
where the boundary of the integrals integrates over the whole accessible phase space. This integral must not be time dependent, as even though $\rho(\vec{q},\vec{p},t)$ may vary over time, the normalization must hold for all times, so the integral acts as a conservation law. 

\begin{namthm}[Liouville's Theorem]
    The phase space density $\rho(\vec{q},\vec{p},t)$ behaves like an incompressible fluid: \begin{equation*}
        \frac{d\rho}{dt} \equiv \frac{\partial \rho}{\partial t} + \sum_{\alpha=1}^{3N}\left(\frac{\partial \rho}{\partial p_{\alpha}}\dot{p}_{\alpha} + \frac{\partial \rho}{\partial q_{\alpha}}\dot{q}_{\alpha}\right) = 0
    \end{equation*}
\end{namthm}

\begin{rmk}
    In summary, Liouvilles theorem states that the phase space density evolves like an incompressible fluid. So, it provides a specific condition for the evolution of the phase space density in time. It allows us to use the Hamiltonian of the system to provide a possible way of determining equilibrium densities. Moreover, any physically relevant equilibrium or non-equilibrium density has to be a solution to these equations.
\end{rmk}

We now investigate how we can define equilibrium from the microscopic perspective.

\begin{defn}
    We define the \Emph{equilibrium phase space density} as $\rho_{eq} \equiv \rho_{eq}(\vec{q},\vec{p})$. This means that \begin{equation*}
        \frac{\partial \rho_{eq}}{\partial t} = 0 \iff \sum_{\alpha=1}^{3N}\left(\frac{\partial \rho}{\partial p_{\alpha}}\dot{p}_{\alpha} + \frac{\partial \rho}{\partial q_{\alpha}}\dot{q}_{\alpha}\right) = 0 \iff \sum_{\alpha=1}^{3N}\left(-\frac{\partial \rho}{\partial p_{\alpha}}\frac{\partial \mathcal{H}}{\partial q_{\alpha}} + \frac{\partial \rho}{\partial q_{\alpha}}\frac{\partial \mathcal{H}}{\partial p_{\alpha}}\right) = 0
    \end{equation*}
\end{defn}

so the probability to find the system in a specific microstate is constant over time in equilibrium. 

\begin{note}
    One of the primary goals of statistical mechanics is to find appropriate equilibrium phase space densities which correspond to finding solutions to the last equation above for a given Hamiltonian $\mathcal{H}$.
\end{note}

Let $\rho = \rho(\mathcal{H}(\vec{q},\vec{p}))$, and let us check whether it would be a viable $\rho_{eq}$. Using the chain rule we find \begin{equation*}
    \sum_{\alpha=1}^{3N}\left(-\frac{\partial \rho}{\partial p_{\alpha}}\frac{\partial \mathcal{H}}{\partial q_{\alpha}} + \frac{\partial \rho}{\partial q_{\alpha}}\frac{\partial \mathcal{H}}{\partial p_{\alpha}}\right) = \sum_{\alpha=1}^{3N}\left(-\frac{d\rho}{d\mathcal{H}}\frac{\partial \mathcal{H}}{\partial p_{\alpha}}\frac{\partial \mathcal{H}}{\partial q_{\alpha}} + \frac{d\rho}{d\mathcal{H}}\frac{\partial \mathcal{H}}{\partial q_{\alpha}}\frac{\partial \mathcal{H}}{\partial p_{\alpha}}\right)  = 0
\end{equation*}
Thus $\rho(\mathcal{H}(\vec{q},\vec{p}))$ satisfies the basic requirements for an equilibrium phase space density. The functional form of $\rho(\mathcal{H}(\vec{q},\vec{p}))$ implies that it is constant on surfaces of constant mechanical energy in phase space, so all microstates with the same mechanical energy are equally likely. This is the \Emph{basic assumption of Statistical Mechanics} and it can be motivated by the maximum entropy principle.




%%%%%%%%%%%%%%%%%%%% Section 4.1.4
\section{Ergodicity}


How do we measure or estimate the phase space density? Experimentally even ensemble averages are rarely accessible since it would require to set up a large number of macroscopically identical systems or experiments, which is not typically feasible. Fortunately, the property of Ergodicity belonging to many systems can be used to circumvent this:

\begin{defn}
    \Emph{Ergodicity} means that ensemble averages are identical to time averages for almost all initial conditions: \begin{equation*}
        \langle F\rangle \equiv \int d\Gamma\rho_{eq}(\vec{q},\vec{p})F(\vec{q},\vec{p}) = \lim\limits_{T\rightarrow \infty}\frac{1}{T}\int_0^TdtF(\vec{q}(t),\vec{p}(t)) \equiv \overline{F}
    \end{equation*}
\end{defn}

To understand this further let us consider a discretization of phase space into little boxes $i$, and investigate a system's trajectory $\mu(t)$ starting from some initial conditions. This discretization allows us to turn the integral over time into a sum, where $\tau_i$ is the amount of time the trajectory spends in box $i$: \begin{equation*}
    \frac{1}{T}\int_0^T F(\vec{p}(t),\vec{q}(t))dt \approx \sum_i\frac{\tau_i}{T}F(\vec{p}_i,\vec{q}_i)
\end{equation*}
If the system is ergodic we then simply have $\frac{\tau_i}{T} = \rho_{eq}d\Gamma_i$, where $d\Gamma_i$ is the phase space volume corresponding to box $i$. This implies that the given system spends a relative amount of time in a given phase space volume that is identical to the probability given by the system's equilibrium phase space density in that volume.

\begin{rmk}
    In summary, for an ergodic system time averages of a single system are sufficient and experimentally much more accessible. However, \Emph{ergodicity is typically a hypothesis}.
\end{rmk}


\begin{note}
    The Ergodic hypothesis implies that a physical system will visit points in its phase space with a frequency equal to the probability density of the ensemble. 
\end{note}


A system is also ergodic only if its trajectory may access all parts of the phase space of positive measure, which is to say if the trajectory passes arbitrarily close to any point in the phase space infinitely many times as time tends towards infinity.

%%%%%%%%%%%%%%%%%%%% Section 4.1.5
\section{Time Reversibility and the Arrow of Time}


How does the fact that equilibriums in thermodynamics are asymptotically reached, with thermodynamic potentials being extremal at equilibrium due to the 2nd law of thermodynamics, translate to the notion of phase space density?

Let us consider Hamilton's equations of motion and how they behave under time reversal. This is mathematically achieved by doing the replacement $t\rightarrow -t$, which implies $\vec{q}\rightarrow \vec{q},\vec{p}\rightarrow -\vec{p},\dot{\vec{q}}\rightarrow -\dot{\vec{q}},\dot{\vec{p}} \rightarrow \dot{\vec{p}},$ and $\mathcal{H}\rightarrow \mathcal{H}$; the invariance of the Hamiltonian under time reversal is violated for weak nuclear interactions unless it is accompanied by a charge and parity reversal (CPT symmetry). Using these replacements in Hamilton's equations of motion we find \begin{equation*}
    \dot{\vec{p}} = -\frac{\partial \mathcal{H}}{\partial \vec{q}},\;\;\;\;\;\;-\dot{\vec{q}} = -\frac{\partial \mathcal{H}}{\partial \vec{p}}
\end{equation*}
so they are unaltered and Hamilton's equations have a time-reversal symmetry: any time reversed solution is also a solution. Now, starting from Liouville's theorem, let us consider any solution $\rho(t)$ of $\frac{\partial \rho}{\partial t} = -\sum_{\alpha=1}^{3N}\left(\frac{\partial \rho}{\partial p_{\alpha}}\dot{p}_{\alpha} + \frac{\partial \rho}{\partial q_{\alpha}}\dot{q}_{\alpha}\right)$ that converges over time to $\rho_{eq}$. Then, under time reversal we obtain another solution $\tilde{\rho}(t)$ that is consistent with Liouville's theorem but by construction does not converge to $\rho_{eq}$. Thus, $\rho_{eq}$ is not generically reached asymptotically. If the system is ergodic however, each trajectory $\mu(t)$ in phase space will stay in the neighborhood of $\rho_{eq}$ the majority of the time. Thus, ergodicity saves the notion of equilibrium and the arrow of time, but only probabilistically.



%%%%%%%%%%%%%%%%%%%%%% Chapter 4.2
\chapter{Classification of Phase Transitions}


We now consider phase transitions and critical phenomena. Important examples of this are:

\begin{eg}
    Important examples of this are: \begin{itemize}
        \item liquid-gas transition (water: $T_c = 645\;K, p_c = 22.1\;MPa$)
        \item paramagnetic-ferromagnetic transition
    \end{itemize}
\end{eg}

Throughout we consider \Emph{control parameter(s)}, and \Emph{power laws ($\neq$ characteristic scalars)} (proportionalities of the form $y \propto x^{\alpha}$). Power laws often satisfy a property known as \Emph{universality}, which states that power laws of similar forms hold across materials and phenomena for similar physical behaviour. In our current study, this behaviour is a change of phase.

\begin{qst}
    Why do phase transitions occur at all?
\end{qst}

From a formal perspective, all macroscopic properties can be deduced from the free energy or the partition function. Since phase transitions typically involve dramatic changes in various response functions, they must correspond to singularities in the free energy. For example, the Gibbs free energy for the Ising model, $G(T,N,\vec{J}) = -k_BT\ln Z_G$, has $Z_G$ non-analytic at phase transitions, despite having the same Hamiltonian $H$ in both phases. Such non-analycities only occur in the thermodynamic limit, when an infinite number of particles are considered.

\begin{defn}
    A \Emph{coexistence line} is a curve in the phase diagram which is the boundary between the set of conditions in which it is thermodynamically favorable for the system to be fully mixed and the set of conditions in which it is thermodynamically favorable for it to phase separate. Equivalently, it is a curve on which two distinct phases may coexist (i.e. it is the boundary between the regions that favour two distinct phases).
\end{defn}

\begin{defn}
    Many different observable quantities exhibit scaling behaviour and have associated \Emph{critical exponents} for their power laws. They are often independent of the specific system under consideration. The fact that two apparently different physical systems might share precisely the same sets of critical exponents is known as \Emph{universality}. Universality describes phenomena with the same behaviour up to a change in coordinates.

    Phenomena with the same set of critical exponents are said to form a \Emph{universality class}. Members of a universality class have in general only three things in common: the symmetry group of the Hamiltonian, the dimensionality, and whether or not the forces are short-ranged.
\end{defn}

\begin{note}
    Critical exponents are only definied in the limit $T\rightarrow T_c$.
\end{note}

As an example, for the liquid gas critical point we have the power law $\rho - \rho_c \propto (T_c - T)^{\beta}$, and for the ising critical point we have the power law $M(T) \propto (T_c - T)^{\beta}$ where $\beta = 0.332$ is the same critical exponent (up to measurement precision) for both phenomena!

In fact, all power-law singularities $(\chi, c_V, \xi)$ are shared by magnets and liquid/gas. Note there is no ``formula" for critical exponents ($D > 2$).

\begin{qst}
    How can we calculate the phase diagram of a system? How do we calculate the critical exponents? Why does universality occur and what are the factors that determine which set of phenomena have the same critical exponents?
\end{qst}

Many of these questions were answered by K.G. Wilson with his \Emph{Renormalization group theory}.

\begin{defn}
    The singular behaviour in the vicinity of a critical point is characterized by a set of \Emph{critical exponents}. These exponents describe the non-analyticity of various thermodynamic functions. The most common are listed below: \begin{itemize}
        \item \Emph{Order Parameter}: By definition there is more than one equilibrium phase on a coexistence line. The order parameter is a thermodynamic function that is different in each phase, and hence can be used to distinguish between them. Eg: for a magnet the average magnetization is an order parameter. These are neither unique, nor necessarily scalars!!
        \item \Emph{Response Functions}: The critical system is quite sensitive to external perturbations, as typified by the infinite compressibility at the liquid-gas critical point. The divergence in the response of the order parameter to a field conjugate to it is indicated by an exponent $\gamma$. For example, the heat capacity is the thermal response function, and its singularities at zero field are described by the exponent $\alpha$.
        \item \Emph{Long-range Correlations}: Divergences of the response functions at a critical point imply, and are consequences of, correlated fluctuations (of microscopic constituents of a material) over large distances.
    \end{itemize}
\end{defn}

\section{Ising Model}

Our motivation for the Ising model is two-fold:
\begin{itemize}
    \item Universality implies simple or minimal models should be sufficient
    \item For continuous phase transitions at $T \gg 0$, QM is often not important
\end{itemize}

The macrostate for this model is $M \equiv (T,B,N)$, which corresponds to the Gibbs canonical ensemble. The microstate is the collection of magnetic moments, $\{\sigma_i\}$, describing the direction and magnitude of spins. For this model \begin{equation*}
    E_{tot} = -\sum_{i=1}^N\sum_{j\neq i}J_{ij}\sigma_i\sigma_j - \mu_0\sum_{i=1}^NB_i\sigma_i
\end{equation*}
where the first term corresponds to the internal interactions (i.e. Hamiltonian), while the second term is the external work. We consider the following simplifications: $B_i = B \equiv$ constant, $\mu_0 = 1$, $\sigma_i \in \{-1,1\}$, and $J_{ij} = J$ for $i,j$ nearest neighbors, and $0$ otherwise. 

We have the \Emph{order parameter} corresponding to the average magnetization: $$m := \frac{M}{N} = \frac{1}{N}\left\langle \sum_{i=1}^N\sigma_i\right\rangle = -\frac{1}{N}\frac{\partial G}{\partial B}$$
For $B = 0$ we have the following: \begin{itemize}
    \item no phase transition for finite $T \neq 0$, in 1-dimension
    \item phase transition in 2-dimensions 
    \item no analytical results for 3-dimensions
\end{itemize}


\section{Existence of Phase Transitions: Energy-Entropy Argument}

\begin{thm}[Energy-Entropy Argument]
    In essence, the \Emph{energy-entropy argument} is as follows: At high temperature, the entropy $S$ always dominates the free energy, and the free energy is minimised by maximising $S$. At low temperatures, there is the possibility that the internal energy $E$ dominates $TS$ in the free energy, and the free energy may be minimised by minimising $E$. If the macroscopic states of the system obtained by these two procedures are different, then we conclude that at least one phase transition has occurred at some intermediate temperature.
\end{thm}

Recall that in thermodynamic equilibrium, the Gibbs potential $G(T,N,B)$ is minimized, and that functionally $G = U - TS - BM$. For $T \gg 1$, this implies $G \approx -TS$, so maximizing $S$ minimizes $G$. For $T \ll 1$, $G \approx U - BM$, so minimizing $U - BM$ minimizes $G$. 

If the two macroscopic states for $T \gg 1$ and $T \ll 1$ are different, there should be at least one phase transition at some intermediate $T$.

\begin{qst}
    How about the Ising model?
\end{qst}

\begin{eg}[Ising Model in 1D with zero temperature and non-zero magnetic field]
    For the Ising model in 1-dimension with $T = 0$ and $B \neq 0$, $$G = E_{tot} = -J\sum_{i=1}^N\sum_{j \in nn_i}\sigma_i\sigma_j - B\sum_{i=1}^N\sigma_i$$ This implies $U = -J\sum_i\sum_{j\in nn_i}\sigma_i\sigma_j$, which is minimzed for $\sigma_i = \sigma_j$, and $-BM = -B\sum_i\sigma_i$ which is minimized for either $\sigma_i = 1$ if $B > 0$, or $\sigma_i = -1$ if $B < 0$. This implies that the energy ground state corresponds to the microstates given by $\sigma_i = 1$ if $B > 0$, or $\sigma_i = -1$ if $B < 0$:
    \begin{figure}[H]
        \centering
        \caption{Symmetry Breaking}
        \includegraphics[scale = 0.8]{Images/symmetryBreak.PNG}
    \end{figure}

    If $B = 0$, since we have $T = 0$ we are dealing with a first order phase transition.
\end{eg}

\begin{defn}
    The \Emph{Enrenfest classification} of phase transitions is that a phase transition is said to be of \Emph{nth order} if any nth derivative of the free energy with respect to any of its arguments yields a discontinuity at the phase transition.

    Second order phase transitions are often called \Emph{critical phenomena}. For $n > 1$, or if the free energy is divergent, we call the transition a \Emph{continuous phase transition}. Continuous, or second-order, phase transititons are characterized by a divergent susceptibility, an infinite correlation length, and a power law decay of correlations near criticality.
\end{defn}


We note that this classification fails in many repsects. For example, thermodynamic quantities such as specific heat actually diverge at continuous transitions (critical phenomena), rather than exhibiting a simple discontinuity, as the Ehrenfest classification implies. This is related to the failure of Mean Field Theory.

\begin{qst}
    What about $m(B=0)$?
\end{qst}

This depends on the initial conditions, and corresponds to \Emph{spontaneous symmetry breaking}.

\begin{defn}
    \Emph{Spontaneous symmetry breaking} is a set of phenomena occur due to the fact that for the given system, when $H = 0$, $$H_{\Omega}[\{S_i\}] = H_{\Omega}[\{-S_i\}]$$ Even though the Hamiltonian is invariant under $\{S_i\}\rightarrow \{-S_i\}$, the statistical expectation values are not invariant under time-reversal symmetry. This is most evident in the case of magnetization in the absence of a magnetic field as we shall now discuss.
\end{defn}


\begin{eg}[Ising Model in 1D with non-zero temperature and zero magnetic field]
    We split into cases: \begin{itemize}
        \item[(i)] Consider an ordered phase: all spin up or all spin down. Since if the system is in one orientation it cannot swap to the other, the number of \Emph{accessible} states is always $1$, so $S = k_B\ln\Omega = k_B\ln 1 = 0$. It follows that $G = U-TS-BM = U = -2NJ$ when minimized.
        \item[(ii)] Now we consider two domains: In the thermodynamic limit we have $\approx N$ (really $N+1$) choices for where we can have the interface. Then $S \approx k_B\ln N$, and $U = -J\sum_i\sum_{j\in nn_i}\sigma_i\sigma_j = -2NJ$, where the $2J$ comes from the single interface. Now, the difference between these two minimized gibbs energies is $$\Delta G := G_{(ii)} - G_{(i)} = 2NJ + 2J - k_BT\ln N + 2NJ = 2J-k_BT\ln N\rightarrow -\infty$$ in the thermodynamic limit, so the system can lower $G$ by creating a domain wall. Domain walls correspond to instability. 
    \end{itemize}
    Proceeding inductively with this argument, we find that $G$ is minimized when the number of domain walls are maximized with the condition that half the spins are up and half are down, resulting in $m = 0$ for all $T \neq 0$ (i.e. there is no phase transition for non-zero temperatures)
\end{eg}

\begin{eg}[Ising Model in 2D with znon-zero temperatuer and zero magnetic field]
    Again we can split into cases like the 1D case. For an ordered phase we have $G = -4NJ$ (four neighbors instead of 2). For two domains we now need the number of possible ways of sectioning off the region into two domains, with the interface having a specified length $L$. In this case $U \approx -4NJ +2JL$, and $S = k_B\ln\Omega \approx k_BL\ln 3$. Then $\Delta G \approx L(2J - k_BT\ln 3)$, giving the approximate critical temperature $T_c \approx \frac{2J}{k_B\ln 3}$ with $T > T_c$ corresponding to disorder and $T < T_c$ corresponding to order (spontaneous magnetization).
\end{eg}

\begin{rmk}
    The (2D) Ising model exhibits a property known as scale invariance, in which the system remains nearly identical in general form at the critical temperature at various scales.
\end{rmk}



These heuristic arguments can be made rigorous, and easily generalized, such as for short-ranged interactions, and Hamiltonians with discrete symmetry.


\section{How Phase Transitions Occur In Practice}

\begin{eg}[Ising Model in 1D]
    Consider the Ising model in 1D, so we have $$E_{tot} = -B\sum_{i=1}^N\sigma_i-2J\sum_{i=1}^N\sigma_i\sigma_{i+1}$$
    Then the partition function is computed by $$Z(T,B,N) = \sum_{\sigma_1 \in \{-1,1\}}\cdots\sum_{\sigma_N\in\{-1,1\}}e^{\beta B\sum_i\sigma_i+\beta 2J\sum_{i}\sigma_i\sigma_{i+1}}$$
    We now divide into two cases: \begin{itemize}
        \item[(i)] $B = 0$, free boundaries: Then we write \begin{align*}
                Z &= \sum_{[\sigma_1,...,\sigma_N]}e^{2\beta J\sum_{i=1}^{N-2}\sigma_i\sigma_{i+1}}\sum_{\sigma_N}e^{2\beta J\sigma_{N-1}\sigma_N} \\
                &= \sum_{[\sigma_1,...,\sigma_N]}e^{2\beta J\sum_{i=1}^{N-2}\sigma_i\sigma_{i+1}}\sum_{\sigma_N}2\cosh(2\beta J\sigma_{N-1}) \\
                &= \sum_{[\sigma_1,...,\sigma_N]}e^{2\beta J\sum_{i=1}^{N-2}\sigma_i\sigma_{i+1}}\sum_{\sigma_N}2\cosh(2\beta J) \\
                &= (2\cosh(2\beta J))^{N-1}\sum_{\sigma_1}1 = 2\cdot(2\cosh(2\beta J))^{N-1}
        \end{align*}
            Now, we consider spatial \Emph{correlations} (\Emph{spin-spin or two-point correlation fluctuations}): \begin{equation*}
                G(i,j) := \langle (\sigma_i-\langle \sigma_i\rangle)(\sigma_j-\langle\sigma_j\rangle)\rangle = \langle \sigma_i\sigma_j\rangle - \langle \sigma_i\rangle\langle \sigma_j\rangle
            \end{equation*}
            It is a measure of how the local fluctuations in one part of the system effect those of another part. Typically such influences occur over a characteristic distance $\xi$, called the \Emph{correlation length}.
            For the Ising model $\langle \sigma_i \rangle = 0 = \langle \sigma_j\rangle$ when $T > 0$ in 1 dimension. We claim that in this case the correlation is translation invariant: \begin{align*}
                G(i,i+r) &= \langle \sigma_i\sigma_{i+r}\rangle = \frac{1}{Z}\sum_{[\sigma_1,...,\sigma_N]}\sigma_i\sigma_{i+r}e^{2\beta J\sum_{j}\sigma_j\sigma_{j+1}} \\
                &= \frac{1}{Z}\left[\sum_{[\sigma_1,...,\sigma_{N-1}]}\sigma_i\sigma_{i+r}e^{2\beta J\sum_{j=1}^{N-2}\sigma_j\sigma_{j+1}}2\cosh(2\beta J)\right] \\
                &= \frac{1}{Z}\left[\sum_{[\sigma_1,...,\sigma_{i+r-1}]}\sigma_ie^{2\beta J\sum_{j=1}^{i+r-2}\sigma_j\sigma_{j+1}}(2\cosh(2\beta J))^{N-i-r}\sum_{\sigma_{i+r}}\sigma_{i+r}e^{2\beta J\sigma_{i+r-1}\sigma_{i+r}}\right] \\
                &= \frac{1}{Z}\left[\sum_{[\sigma_1,...,\sigma_{i+r-1}]}\sigma_ie^{2\beta J\sum_{j=1}^{i+r-2}\sigma_j\sigma_{j+1}}(2\cosh(2\beta J))^{N-i-r}(2\sigma_{i+r-1}\sinh(2\beta J))\right] \\
                &= \frac{1}{Z}2(2\cosh(2\beta J))^{N-r-1}(2\sinh(2\beta J))^r \\
                &= \tanh(2\beta J)^r 
            \end{align*}
            Thus, we have \Emph{translational invariance}, with $G(i,i+r) \equiv G(r) = \tanh(2\beta J)^r$. At $T = 0$ we have $\langle \sigma_i\sigma_{i+r}\rangle = \tanh(\infty)^r = 1^r = 1$, which is to say we have \Emph{perfect long-range order}, the spin of one atom dictates the spin of all the others. If $T \neq 0$, $\tanh(\beta J) < 1$ which implies we can write $$G(r) = e^{-r/\xi}$$ where $\xi =(\ln(\coth(2\beta J)))^{-1} > 0$ is called the \Emph{characteristic length}. It is the distance from the $y$-axis that the tangent to the graph of $G(r)$ at $r = 0$ hits the $x$-axis. Note that $\xi\rightarrow \infty$ as $T\rightarrow 0$, which is \Emph{characteristic for all continuous phase transitions}.
        \item[(ii)] $B \neq 0$, with periodic boundary conditions: First, we can compute the partition function as follows: \begin{align*}
                Z &= \sum_{[\sigma_1,...,\sigma_N]}e^{2\beta J\sum_i\sigma_i\sigma_{i+1}+\beta B\sum_i\sigma_i} \\
                &= \sum_{[\sigma_1,...,\sigma_N]}\left(e^{2\beta J\sigma_1\sigma_2 + \frac{\beta B}{2}(\sigma_1+\sigma_2)}\right)\cdots \left(e^{2\beta J\sigma_N\sigma_1 + \frac{\beta B}{2}(\sigma_N+\sigma_1)}\right) \\
                &= \sum_{[\sigma_1,...,\sigma_N]}T_{\sigma_1,\sigma_2}\cdot \;\cdots \;\cdot T_{\sigma_N,\sigma_1}
        \end{align*}
            where $T$ is the \Emph{transfer matrix} defined by \begin{equation*}
                T = \begin{pmatrix} T_{1,1} & T_{1,-1} \\ T_{-1,1} & T_{-1,-1} \end{pmatrix} = \begin{pmatrix} e^{\beta(2J+B)} & e^{-\beta 2J} \\ e^{-\beta 2J} & e^{\beta (2J-B)} \end{pmatrix}
            \end{equation*}
            Note that $T_{\sigma_i,1}T_{1,\sigma_{i+2}}+T_{\sigma_i,-1}T_{-1,\sigma_{i+2}} = T^2_{\sigma_i,\sigma_{i+2}}$, simply by definition of matrix multiplication. Then we can concatenate the terms in our sum to form $$Z = \sum_{\sigma_1}T^N_{\sigma_1,\sigma_1} = \text{Tr}(T^N) = \lambda_1^N+\lambda_2^N$$ where $\lambda_1,\lambda_2$ are the eigenvalues of $T$. Via the characteristic polynomial for $T$, we have $$\lambda_{1,2} = e^{2\beta J}\left[\cosh(\beta B) \pm\sqrt{\sinh^2(\beta B)+e^{-4\beta 2J}}\right]$$ 
            For the special case of $B = 0$ we obtain $$\lambda_{1,2} = e^{\beta 2J}\pm e^{-\beta 2J}$$ which is either $2\cosh(2\beta J)$ or $2\sinh(2\beta J)$. Then $$Z = (\lambda_1^N+\lambda_2^N) = 2^N(\cosh^N(2\beta J)+\sinh^N(2\beta J))$$ 
            In the thermodynamic limit, $N\rightarrow \infty$, $Z = 2^N\cosh^N(2 \beta J)$, which matches the case for free boundaries.

            In general, $$Z = \lambda_1^N+\lambda_2^N = \lambda_1^N(1+\left(\frac{\lambda_2}{\lambda_2}\right)^N)\rightarrow \lambda_1^N$$ in the thermodynamic limit if $|\lambda_1| > |\lambda_2|$.

            Now, the \Emph{free energy per spin} is given by $$f(B,T) := \lim\limits_{N\rightarrow \infty}\frac{G(B,T,N)}{N} = -k_BT\ln\lambda_1 = -2J-k_BT\ln\left[\cosh(\beta B)+\sqrt{\sinh^2(\beta B)+e^{-4\beta 2J}}\right]$$

            Now we observe phase transitions for:\begin{itemize}
                \item $\lambda_1 = 0$
                \item $\lambda_1 = \lambda_2$ (if and only if $\sqrt{\sinh^2(\beta B)+e^{-4\beta 2J}} = 0$)
                \item $\lambda_1$ is a non-analytic unction
            \end{itemize}
            For the Ising model in 1 dimension, none of these conditions are met if $T > 0$, which implies there is no phase transition for $T > 0$, as we have seen previously with the Energy-Entropy argument.
    \end{itemize}
\end{eg}

 
\begin{qst}
    What happens at $T = 0$? in the last example for periodic boundaries?
\end{qst}

As $T\rightarrow 0$, $$\lambda_1 = e^{\beta 2J}\left[\cosh(\beta B)+\sqrt{\sinh^2(\beta B)}(1 + \mathcal{O}(e^{-4\beta 2J}))\right]$$ where $|\sinh(\beta B)|$ is non-analytic at $\beta B = 0$, and we can write $$\lambda_1 \approx e^{\beta 2J}[e^{|\beta B|}] = e^{\beta (2J+|B|)}$$
which implies $f = -k_BT\ln\lambda_1 \rightarrow -(2J+|B|)$ in the limit as $T\rightarrow 0$. Consequently, the average magnetization $m$, is now $$\langle \sigma_i \rangle = m = -\frac{\partial f}{\partial B} = \left\{\begin{array}{cc} 1 & B > 0 \\ -1 & B < 0 \end{array}\right.$$ which shows the partial is not defined or continuous at $B = 0$, so we have a \Emph{zero-temperature phase transition}.





\section{Mean-Field Theory}


In the Ising model we have $$E_{tot} = -B\sum_{i=1}^N\sigma_i - J\sum_{i=1}^N\sum_{j \in nn(i)}\sigma_i\sigma_j$$ for $J > 0$ a coupling factor, where the dimensionality of the system enters in the sum over nearest neighbors. Interactions in dimensions $D > 1$ make it hard, and often impossible, to analytically calculate $Z$.

\begin{axi}[Central Idea of Mean-Field Theory]
    We aim to approximate the Hamiltonian $H$ by a ``non-interacting" Hamiltonian, but keep the essential physics still present. This involves looking at the interactions of particles with the ``mean-field" rather than with eachother.
\end{axi}

In general, mean field theory studies the behaviour of high-dimensional random models by studying a simpler model that approximates the original by averaging over degrees of freedom. The effect of all the other components on any given component is approximated by a single averaged effect, reducing a many body problem to a one body problem. We aim to replace all interactions to any one body with an average or effective interaction. 

\begin{qst}
    How can we do that for the Ising model?
\end{qst}

First we linear the Hamiltonian. Observe: $$\sigma_i\sigma_j = (\langle \sigma_i\rangle + (\sigma_i - \langle \sigma_i\rangle))(\langle \sigma_j\rangle + (\sigma_j-\langle \sigma_j\rangle))$$
Assuming $\langle \sigma_i \rangle = \langle \sigma_j\rangle = m$, we can write $$\sigma_i\sigma_j = (m+(\sigma_i-m))(m+(\sigma_j-m)) = m^2 + m(\sigma_i-m)+m(\sigma_j-m) + O((\Delta \sigma)^2)$$
where we ignore terms representing differences of power greater than or equal to two, as in the limit the CLT applied to the Ensemble picture tells us these quantities will go to zero. So $\sigma_i\sigma_j \approx (\sigma_i+\sigma_j)m - m^2$. Then our mean-field partition function, assuming periodic boundary conditions, is \begin{align*}
    Z_{mf} &= \sum_{[\sigma_1,...,\sigma_N]}e^{\beta B\sum_{i=1}^N\sigma_i + \beta J\sum_{i=1}^N\sum_{j\in nn(i)}(m(\sigma_i+\sigma_j)-m^2)} \\
    &= \sum_{[\sigma_1,...,\sigma_N]}e^{\beta(B+Jm4D)\sum_{i=1}^N\sigma_i - \beta Jm^2N2D} \\
    &= e^{-\beta Jm^2N2D}\sum_{[\sigma_1,...,\sigma_N]}e^{\beta(B+Jm4D)\sum_{i=1}^N\sigma_i} \\
    &= e^{-\beta Jm^2N2D}\sum_{[\sigma_1]}e^{\beta(B+Jm4D)\sigma_1}\cdots \sum_{[\sigma_N]}e^{\beta(B+Jm4D)\sigma_N} \\
    &=e^{-\beta Jm^2N2D}[2\cosh(\beta(B+Jm4D))]^N 
\end{align*}
Now, the mean-field free energy per spin is $$f_{mf} := \frac{-k_BT\ln Z_{mf}}{N} = Jm^22D - \frac{1}{\beta}\ln[2\cosh(\beta(B+Jm4D))]$$
\begin{qst}
    How exactly can we determine $m$?
\end{qst}

We need to have self-consistency, so $m = -\left(\frac{\partial f_{mf}}{\partial B}\right)_{T,N}$. Note since $m$ is a function of the macroscopic variables, not a variable itself, when taking the partial we will need to employ the chain rule on $m$: \begin{align*}
    m &= -J4Dm\frac{\partial m}{\partial B} + \frac{1}{\beta}\tanh(\beta(B+Jm4D))\cdot(\beta+\beta J4D\frac{\partial m}{\partial B}) \\
    &= J4D\frac{\partial m}{\partial B}(-m+\tanh(\beta(B+Jm4D))) + \tanh(\beta(B+Jm4D))
\end{align*}
and so $$0 = (J4D\frac{\partial m}{\partial B} + 1)(-m+\tanh(\beta(B+Jm4D)))$$
In order for this to be zero, either the second factor must be zero of $\frac{\partial m}{\partial B} = -\frac{1}{J4D} < 0$. But, this would imply that as the magnetic field increases, the magnetization decreases, which is a non-physical result. Thus, we must have $$m = \tanh(\beta(B+Jm4D))$$
In the special case of $B = 0$, $m = \tanh(\beta Jm4D)$. We proceed with a graphical solution:

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{Images/OneSolution.PNG}
    \caption{Single solution for the magnetization, corresponding to $T > T_c$, with the horizontal axis being $m$}
    \label{fig:1sol}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{Images/ThreeSolutions.PNG}
    \caption{Three solutions for the magnetization, corresponding to $T < T_c$, with the horizontal axis being $m$}
    \label{fig:3sol}
\end{figure}

In the first figure, we observe that there is only one possible solution, with the magnetization being zero. However, in the second figure we have three possible solutions, and the exact one that is observed depends on which minimizes the Gibbs free energy. It turns out the non-zero magnetizations minimize the Gibbs free energy, with the exact one of the two depending on the history of the system.

Now, the critical temperature corresponds with the point that the slope of the tangent at the origin passes $1$, so $T_c$ is given by solving $$\frac{d}{dm}\tanh(\beta Jm4D)\Bigg\rvert_{m=0} = 1$$
We can calculate directly or use power series expansions to determine that $$T_c = \frac{4DJ}{k_B}$$
Note that this is a prediction and an approximation, and hence is not guaranteed to hold even to good approximation depending on the dimension and system involved. Nonetheless, this is a good starting point for a prediction of the critical point of the phase transition.

\begin{qst}
    What happens for $B \neq 0$?
\end{qst}
In this case the $\tanh$ graph gets shifted either right, for $B < 0$, or left, for $B > 0$.


\subsection{Critical Exponents}

Recall we have our self-consistency equation for the magnetization order parameter: $$m = \tanh(\beta(B+Jm4D)) = \tanh(\beta B+m\tau)$$ 
with $\tau := \frac{T_c}{T} = \frac{4DJ}{k_BT}$. Using a power series expansion we can write \begin{equation*}
    m = \tanh(\beta B+m\tau) = (\beta B+m\tau) -\frac{1}{3}(\beta B+m\tau)^3+\cdots
\end{equation*}
We note the following cases:
\begin{itemize}
    \item If $B = 0$: $m \approx m\tau - \frac{1}{3}(m\tau)^3$, for $\tau\rightarrow 1$ from above and $m \neq 0$, which implies $$m^2 = 3\frac{\tau-1}{\tau^3} = 3\frac{T_c-T}{T_c^3/T^2} \approx 3\frac{(T_c-T)}{T_c}$$
        which would imply $$m\propto \pm(T_c-T)^{1/2}$$
        where $1/2$ is our \Emph{critical exponent} $\beta$.
    \item If $T = T_c$ (the critical isotherm) we have $0 = \beta B -\frac{1}{3}m^3$, for $B\rightarrow 0$, so $$B\propto m^3$$
        where $3$ is our critical exponent $\delta$
    \item For the \Emph{isothermal magnetic susceptibility} we have \begin{equation*}
            \chi \equiv \frac{\partial m}{\partial B}\Bigg\rvert_{T,B=0}
    \end{equation*}
        which implies we can write it as $\chi\approx \beta +\chi T -m^2\tau^3\chi - \beta m^2\tau^2$, so $$\chi(1-\tau+m^2\tau^3) \approx \beta(1-m^2\tau^2)$$
        As $T\rightarrow T_c$ from above, with $m = 0$, $$\chi \approx \frac{\beta}{1-\tau} = \frac{1}{k_B(T-T_c)}$$
        giving the power law $$\chi\propto (T-T_c)^{-1}$$ 
        where $1$ is our critical exponent $\gamma$.
\end{itemize}

\begin{rmk}
    In summary: \begin{itemize}
        \item \Emph{Mean-field theory} predicts phase transitions at non-universal $T_c$ for all $D$ ($T_c(D)$) but with \Emph{universal critical exponents}. Its classification of \Emph{universal} and \Emph{non-universal} aspects is consistent with experiments. 
        \item Mean-field theory typically becomes exact only for high-dimensional systems (fluctuations are important)
        \item The Ising model correctly reproduces the critical exponents observed in experiments
        \item Mean-field theory provides a simple first approach to estimating the phase diagram
    \end{itemize}
\end{rmk}





\section{Renormalization Group Theory}

\begin{qst}
    What exactly happens at the critical point?
\end{qst}

\begin{rmk}
    We have the following properties of phase transititons (e.g. Ising model):\begin{itemize}
        \item $G(r) = e^{-r/\xi}$ (the spin-spin correlation function) with $\xi(T) \propto |T-T_c|^{-\nu}$ for $T\rightarrow T_c$ (the \Emph{correlation length}). 
        \item More generally, this implies \Emph{self-similarity} at the critical point in the form of power laws (invariance under zooming/rescaling)
    \end{itemize}
\end{rmk}

\begin{qst}
    Are there other values of $T$ for which we have self-similarity?
\end{qst}

\begin{eg}
    In the Ising model we have self-similarity at $T = 0$ and $T=\infty$ where $G(r) \equiv 0$ or $\xi = 0$. Recall that \begin{equation*}
        \boxed{G(r) := \langle \sigma_i\sigma_{i+r}\rangle - \langle \sigma_i\rangle\langle \sigma_{i+r}\rangle}
    \end{equation*}
\end{eg}

\begin{qst}
    How can we identify self-similarity/scale-invariance?
\end{qst}

We look for a fixed point under ``zooming out," aka a rescaling transformation: \begin{itemize}
    \item We rescale all length scales by a factor $b > 1$: $$\xi \mapsto \xi/b$$
    \item Fixed points are given by points where $\xi = \xi/b$, which only occurs if $\xi = 0$ (\Emph{trivial} solution, \Emph{stable}), and $\xi = \infty$ (\Emph{non-trivial} solution, \Emph{unstable})
\end{itemize}

\subsection{Coarse Graining and Renormalization Group Procedure}

We need to have full information about a system for a rescaling transformation. A possible simplification to this is to note $\xi\rightarrow \infty$ as $T\rightarrow T_c$. This implies that small length scales are expected to be less important for determining the behaviour close to $T_c$. We then introduce \Emph{coarse graining} in the lead up to a new transformation $R_b$: the \Emph{real-space renormalization group transformation}. Be warned that this transformation is not invertible.

\begin{rmk}
    Coarse graining may be done in many different ways, but the general idea is to remove microscopic details. We go to the `continuum limit', averaging over details in small regions to get effective laws for coarser systems. For example, in a 2D grid with spins we could take $3\times 3$ blocks and replace the blocks with with a single spin that occurs most, then rescale so that the new spins are the same size as the original spins. This is commonly referred to as \Emph{Kadanoff's block spin transformation}.
\end{rmk}

Let's consider the one dimensional Ising model with $B = 0$, periodic boundary conditions, and $N$ even. Then $$Z(k_1,N) = \sum_{[\sigma_1,...,\sigma_N]}e^{k_1\sum_{i=1}^N\sigma_i\sigma_{i+1}}$$
where $k_1 := \beta J'$ is a scaled coupling constant. For our renormalization scheme we take $b = 2$, and in our selection of neighboring spins we coarse-grain by taking the odd value spin. This is sometimes referred to as \Emph{decimating/decimation}. We do this as follows: \begin{align*}
    Z(k_1,N) &= \sum_{[\sigma_1,\sigma_3,...,\sigma_{N-1}]}\sum_{[\sigma_2,\sigma_4,...,\sigma_N]}e^{k_1\sum_{i=1}^N\sigma_i\sigma_{i+1}} \\
    &= \sum_{[\sigma_1,\sigma_3,...,\sigma_{N-1}]}\sum_{[\sigma_2,\sigma_4,...,\sigma_N]}\prod_{i=1}^{N/2}e^{k_1(\sigma_{2i-1}\sigma_{2i}+\sigma_{2i}\sigma_{2i+1})} \\
    &= \sum_{[\sigma_1,\sigma_3,...,\sigma_{N-1}]}\prod_{i=1}^{N/2}\sum_{[\sigma_{2i}]}e^{k_1(\sigma_{2i-1}\sigma_{2i}+\sigma_{2i}\sigma_{2i+1})} \\
    &= \sum_{[\sigma_1,\sigma_3,...,\sigma_{N-1}]}\prod_{i=1}^{N/2}2\cosh(k_1[\sigma_{2i-1}+\sigma_{2i+1}])
\end{align*}
The possible arguments of $\cosh$ are $-2k_1,0,2k_1$, which is just $\cosh(2k_1)$ and $\cosh(0) = 1$ since $\cosh$ is even. We can rewrite $\cosh$ using two new parameters here: \begin{equation*}
    2\cosh[k_1(\sigma_{2i-1}+\sigma_{2i+1})] = e^{k_0'+k_1'\sigma_{2i-1}\sigma_{2i+1}}
\end{equation*}
where $k_0'$ and $k_1'$ are renormalized coupling constants. Specifying certain values of the spins we can determine $k_0'$ and $k_1'$ to be \begin{align*}
    k_0' &= \ln(2\sqrt{\cosh(2k_1)}) \\
    k_1' &= \ln(\sqrt{\cosh(2k_1)})
\end{align*}
so we can write the partition function as \begin{equation*}
    Z(k_1,N) = e^{Nk_0'/2}\sum_{[\sigma_I]}e^{k_1'\sum_{I=1}^{N'}\sigma_I\sigma_{I+1}}
\end{equation*}
where $N' = N/b$ in general, or $N/2$ in our current case. In particular $Z(k_1,N) = e^{N'k_0'}Z(k_1',N')$, so $Z$ is \Emph{invariant} under renormalization up to a multiplicative constant.

\begin{qst}
    Is this constant relevant?
\end{qst}
For Gibb's free energy we have: \begin{align*}
    G(k_1,N) &= -k_BT\ln Z(k_1,N) \\
    &= -k_BT(N'k_0'+\ln Z(k_1',N')) \\
    &= G(k_1',N') - k_BTN'k_0'
\end{align*}
which results in simply a free energy offset, which is irrelevant for ensemble averages. Thus $H$ remains invariant under renormalization but $k_1\rightarrow k_1'$ with $$\boxed{k_1' = \frac{1}{2}\ln(\cosh(2k_1))}$$

\begin{qst}
    What are the fixed points of the renormalization transformation flow?
\end{qst}

For $k_1 > 0$ we observe that $k_1' < k_1$. The only fixed points occur at $k_1^* = 0$ and $k_1^* = \infty$, with $\infty$ being an unstable fixed point and $0$ being a trivial stable fixed point. $k_1^* = 0$ is the high temperature fixed point (weak coupling) where $T = \infty$ (non-interacting spins) and $\xi = 0$. $k_1^* = \infty$ is the low temperature fixed point (strong coupling) where $T = 0$ (fully aligned spins) and $\xi = 0$, corresponding to a non-trivial fixed point. We observe no phase transition for $T > 0$ and $B = 0$, consistent with our previous results.

\begin{qst}
    What happens in 2D?
\end{qst}

In a 2D square lattice we have coupling with all four neighbors. Under renormalization this coupling turns into coupling with diagonal neighbors and neighbors that were originally two across. These two types of neighbors have two types of $K$ constants, implying that under each renormalization our number of coupling constants increase. In general we must represent a renormalization trajectory on a possibly infinite dimensional manifold of coupling constants. Nonetheless, $\xi(\mathbf{K}) = \infty$ corresponds to the critical surface and for $\mathbf{K}^*$ a fixed point on this surface would be non-trivial. On the other hand, for fixed points where $\xi(\mathbf{K}^*) = 0$ we can either have weak-coupling (all coupling constants zero), or strong-coupling (a number of coupling constants infinite).

\subsection{Notes on Critical Exponents}

\begin{qst}
    How can we obtain the critical exponents?
\end{qst}
We can look for them in the vicinity of the non-trivial (unstable) fixed point $T_c$ for the real-space renormalization transformation: $(t,h)\mapsto (t',h')$ with $t = \frac{T-T_c}{T_c}$ and $h = \beta B$. For $t > 0$, $t' > t$, and for $t < 0$ $t' < t$, so the flow of the renormalization transformation is away from $T_c$ and is linear to first order in $t$: $$t' = \lambda_t(b)t$$ with $\lambda_t(b)$ a \Emph{constant of proportionality} satisfying $\lambda_t(b) > 1$ for $b > 1$ and $\lambda_t(1) =1$. Then renormalization twice gives \begin{equation*}
    \boxed{t'' = \lambda_t(b_2)t' = \lambda_t(b_2)\lambda_t(b)t = \lambda_t(b_2b)t}
\end{equation*}
The unique solution to this desired equality is $t' = b^{y_t}t$ with $y_t > 0$. This was found by Christensen and Moloney. In sum we have $$\boxed{t' = b^{y_t}t \;\;for\;\;t\rightarrow 0^{\pm}\;\;with\;y_t > 0}$$
We also have the following relation to correlation length: $\xi' = \xi/b$ and $\xi(t,0) \propto |t|^{-\nu}$ for $t\rightarrow 0^{\pm}$, so $$|b^{y_t}t|^{-\nu} = |t'|^{-\nu} = \frac{|t|^{-\nu}}{t}$$ for $t\rightarrow 0^{\pm}$ and $h = 0$, so $$b^{-y_t\nu} = b^{-1}$$ which implies $$\boxed{y_t = \frac{1}{\nu}}$$

\begin{rmk}
    In summary the properties of the linear flow at the non-trivial fixed point determine the critical exponents.
\end{rmk}




\subsection{Renormalization Group Method and the One-Dimensional Ising Model}

We shall average groups of spins and then determine which parameters characterize the renormalized lattice. Our result will be to calculate the fixed points. An unstable fixed point will correspond to a critical point. Further, the rate of change of the renormalized parameters near a critical point yields approximate values of the critical exponents.

The energy of the Ising chain with toroidal boundary conditions is \begin{equation*}
    E = -J\sum_{i=1}^N-\frac{1}{2}H\sum_{i=1}^N(s_i+s_{i+1})
\end{equation*}
It is convenient to absorb the factors of $\beta$ and define the dimensionless parameters $K = \beta J$ and $h = \beta H$. The partition function can be written as \begin{equation*}
    Z = \sum_{\{s\}}\exp\left[\sum_{i=1}^N(Ks_is_{i+1}+\frac{1}{2}h(s_i+s_{i+1})\right]
\end{equation*}
where the sum is over all possible spin configurations. We first consider $h = 0$.

For $d = 1$ Ising model we can write $Z$ as \begin{equation*}
    Z(K,N) = \sum_{s_1,s_2,s_3,...}e^{K(s_1s_2+s_2s_3)}e^{K(s_3s_4+s_4s_5)}\cdots
\end{equation*}
This form suggests that we sum over even spins $s_2,s_4,...,$ and write \begin{equation*}
    Z(K,N) = \sum_{s_1,s_3,s_5,...}\left[e^{K(s_1+s_3)} + e^{-K(s_1+s_3)}\right]\left[e^{K(s_3+s_5)} + e^{-K(s_3+s_5)}\right]\cdots
\end{equation*}
We write the partition function in its original form with $N/2$ spins and a different interaction $K'$. If such a rescaling were possible, we could obtain a recursion relation for $K'$ in terms of $K$. We require that \begin{equation*}
    e^{K(s_1+s_3)} + e^{-K(s_1+s_3)} = A(K)e^{K's_1s_3}
\end{equation*}
where the function $A(K)$ does not depend on $s_1$ or $s_3$. If the relation exists we can write \begin{equation*}
    Z(K,N) = \sum_{s_1,s_3,s_5,...}A(K)e^{K's_1s_3}A(K)e^{K's_3s_5}\cdots = [A(K)]^{N/2}Z(K',N/2)
\end{equation*}
In the limit $N\rightarrow \infty$ we know that $\ln Z$ is proportional to $N$, that is \begin{equation*}
    \ln Z = Ng(K)
\end{equation*}
where $g(K)$ is independent of $N$. Then we obtain \begin{equation*}
    \ln Z(K,N) = Ng(K) = \frac{N}{2}\ln A(K) + \ln Z(K',N/2) = \frac{N}{2}\ln A(K) + \frac{N}{2}g(K')
\end{equation*}
and \begin{equation*}
    g(K') = 2g(K) - \ln A(K)
\end{equation*}
We can find the form of $A(K)$ from the recurrence relation. We use the fact that the relation holds for all values of $s_1$ and $s_3$, and first consider $s_1=s_3 = 1$ or $s_1=s_3 = -1$, for which \begin{equation*}
    e^{2K}  +e^{-2K} = A(K)e^{K'}
\end{equation*}
and for opposite spins $2 = A(K)e^{-K'}$, so $A = 2e^{K'}$, and hence we obtain \begin{equation*}
    e^{2K} + e^{-2K} = 2e^{2K'}
\end{equation*}
or $$K' = R(K) = \frac{1}{2}\ln[\cosh(2K)]$$
Then we find that \begin{equation*}
    A(K) = 2\sqrt{\cosh(2K)}
\end{equation*}
We can use the form of $A(K)$ to rewrite $g(K')$ as \begin{equation*}
    g(K') = 2g(K) - \ln[2\sqrt{\cosh(2K)}]
\end{equation*}
$g(K')$ and $K'$ are the main results of the renormalization group analysis. Because $\frac{1}{2}\ln\cosh(2K) \leq K$, the successive use of the renormalization leads to a smaller value of $K$ (higher temperatures) and hence a smaller value of the correlation length. Thus $K = 0$ or $T = \infty$ is a trivial fixed point. This is expected since the Ising chain does not have a phase transition at nonzero temperature. The only other fixed point is when $K = \infty$, or $T = 0$, which is unstable because any perturbation away from $T = 0$ is amplified. The fixed point at $T = \infty$ is stable. 

Because there is no nontrivial fixed point between $T=0$ and $T=\infty$ the recursion relation is reversible and we can follow the transformation backward starting at $K \approx 0$ ($T=\infty$) and going to $K=\infty$ ($T = 0$). The advantage of starting from $T \approx \infty$ is that we can start with the exact solution for $K = 0$ and iterate the recursion relation to higher values of $K$ for which the interaction between the spins become increasingly important. To find the relation that works in this direction we solve for $K$ in terms of $K'$ and $g(K)$ in terms of $g(K')$. The result is \begin{align*}
    K &= \frac{1}{2}\cosh^{-1}(e^{2K'}) \\
    g(K) &= \frac{1}{2}\ln 2 + \frac{1}{2}K' + \frac{1}{2}g(K')
\end{align*}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Part 5
\part{Quantum Statistics}


%%%%%%%%%%%%%%%%%%%%%% Chapter 5.1
\chapter{Breakdown of Classical Mechanics}

Recall we need quantum mechanics to derive the 3rd law of quantum mechanics in ensemble thoery: $\lim\limits_{T\rightarrow 0}S(T) = 0$ which implies $\lim\limits_{T\rightarrow 0}C_{\vec{x}}(T) = 0$.

\section{Dilute Polyatomic Gases}

For each molecule of $n$ atoms we take the Hamiltonian \begin{equation*}
    H_1 = \sum_{i=1}^n\frac{\vec{p}_i\cdot\vec{p}_i}{2m} + V_{int}(\vec{q}_1,...,\vec{q}_n)
\end{equation*}

\begin{qst}
    What can we say about the motion of the atoms within a molecule?
\end{qst}

\subsection{Small Oscillations and Linear Stability}

With one degree of freedom (e.g. pendulum), we can write $\ddot{x} = f(x)$, for $x \in \R$, which we can express as $\dot{x} = v$ and $\dot{v} = f(x)$, giving the matrix form the DE $\begin{pmatrix} \dot{x} \\ \dot{v} \end{pmatrix} = \vec{F}(x,v)$. At the equilibrium point $\vec{F}(x=x_0,v=v_0) = 0$ if and only if $\dot{x} = 0$ and $\dot{v} = 0$, which implies $x = x_0$ is constant. Under a small perturbation, $x(t) = x_0+\eta(t)$, $\ddot{\eta} = \ddot{x} = f(x) = f(x_0+\eta)$, which we can expand about the equilibrium as $$f(x_0+\eta) \approx f(x_0) + f'(x_0)\eta + \frac{1}{2}f''(x_0)\eta^2+\cdots$$
where we will have $f(x_0) = 0$. To first order $\ddot{\eta} \approx f'(x_0)\eta$ assuming $f'(x_0) \neq 0$.
\begin{itemize}
    \item If $f'(x_0) < 0$: Harmonic oscillator. We find $\eta(t) = A\cos(\omega(t-t_0))$, where $A$ and $t_0$ are constants of integration, and $\omega^2 = -f'(x_0)$, giving \Emph{stable} oscillations about $x = x_0$ with frequency $\omega_0$.
    \item If $f'(x_0) > 0$, we write $\eta(t) = Ae^{\lambda t} + Be^{-\lambda t}$, where $\lambda^2 = f'(x_0)$. In this case for $A \neq 0$ perturpations grow, so the approximation of $\eta$ small breaks down - \Emph{linearly unstable}.
\end{itemize}

For the first case, as $f(x) = -\partial_xV(x)$, we find a concave up function near equilibrium (symmetric oscillations), while for the second case we find a concave down function near equilibrium. 

Now in general we have $$\ddot{q}_i = f_i(q_1,...,q_s),\;\;1\leq i\leq s$$ Let $\vec{q}^o$ denote the equilibrium point, so for all $i$, $f_i(\vec{q}_1^o,...,\vec{q}_s^o) = 0$. Under small perturbations we can write \begin{equation*}
    q_i(t) = q_i^o+\eta_i(t)
\end{equation*}
implying that \begin{equation*}
    \ddot{\eta}_i \approx \sum_{j=1}^s\frac{\partial f_i}{\partial q_j}\Bigg\rvert_{\vec{q}=\vec{q}^o}\eta_j
\end{equation*}
In vector notation $\ddot{\vec{\eta}} = F\vec{\eta}$, for $F$ the jacobian of the force at $\vec{q}=\vec{q}^o$.

Consider the special case of $F$ symmetric. Then $F$ is orthogonally diagonalizable with real eigenvalues. This creates decoupled ODEs corresponding to $s$ 1D problems. In general for a conservative system eigenvalues of $F$ are real and $F$ is diagonalizable, but not necessary orthogonally diagonalizable. Let $\vec{\mu}_a$, for $a = 1,...,s$, denote an eigenvector for the eigenvalue $\lambda_a^2$. We also say that $\vec{\mu}_a$ is a \Emph{normal mode}. We can write $\vec{\eta}(t) = \sum_ac_a(t)\vec{\mu}_a$, so $$\sum_a\ddot{c}_a(t)\vec{\mu}_a = \ddot{\vec{\eta}} = \sum_ac_a(t)\lambda^2\vec{\mu}_a$$
It follows by linear independence that $\ddot{c}_a(t) = \lambda_a^2c_a(t)$, which reduces us to a one-dimensional problem, with solutions \begin{equation*}
    c_a(t) = A_ae^{\lambda_at} + B_ae^{-\lambda_at}
\end{equation*}
Thus, our general solution is \begin{equation*}
    \vec{\eta}(t) = \sum_a\vec{\mu}_a\left[A_ae^{\lambda_at} + B_ae^{-\lambda_at}\right]
\end{equation*}
We have the following cases for our eigenvalues: \begin{itemize}
    \item If $\lambda_a^2 < 0$: $\lambda_a = i\omega_a$ for $\omega_a \in \R$, which corresponds with stable oscillations in the corresponding direction $\vec{\eta} = \vec{\mu}_a$
    \item If $\lambda_a^2 > 0$: $\lambda_a \in \R$, giving unstable solutions in the direction $\vec{\eta} = \vec{\mu}_a$.
\end{itemize}
Then we can write \begin{equation*}
    \vec{\eta}(t) = \sum_{a,\lambda_a^2>0}\vec{\mu}_a\left[A_ae^{\lambda_at} + B_ae^{-\lambda_at}\right] + \sum_{a,\lambda_a^2 < 0}\vec{\mu}_aA_a\cos(\omega_a(t-t_a))
\end{equation*}


\section{Linear Triatomic Molecule}


Consider a triatomic molecule with middle mass $M$ and side masses $m$ at locations $x_1,x_2,x_3$. Focusing on the  motion parallel to the molecule for each atom only we can write \begin{equation*}
    L = \frac{1}{2}m\dot{x}_1^2 + \frac{1}{2}M\dot{x}_2^2 + \frac{1}{2}m\dot{x}_3^2 - V(x_1-x_2) - V(x_2-x_3)
\end{equation*}
Assume $x_i = x_i^o$ is the equilibrium position, with $|x_1^o-x_2^o| = |x_2^o-x_3^o| = r_0$. We consider perturbations of the form $x_i(t) = x_i^o +\eta_i(t)$. Expanding $V$ about the equilibrium $r_o$ to the first non-zero order we have \begin{equation*}
    L \approx \frac{1}{2}m\dot{\eta}_1^2 + \frac{1}{2}M\dot{\eta}_2^2 + \frac{1}{2}m\dot{\eta}_3^2 - \frac{k}{2}\left[(\eta_1-\eta_2)^2+(\eta_2-\eta_3)^2\right]
\end{equation*}
where $k := \frac{\partial^2V}{\partial r^2}\Bigg\rvert_{r=r_0} > 0$ (minimum for stable molecule). Using Lagrange's equations $\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{\eta}_i}\right) - \frac{\partial L}{\partial \eta_i} = 0$ we can write $\ddot{\vec{\eta}} = F\vec{\eta}$ for the matrix \begin{equation*}
    F = \begin{pmatrix} -k/m & k/m & 0 \\ k/M & -2k/M & k/M \\ 0 & k/m & -k/m \end{pmatrix}
\end{equation*}
$F$ has eigenvalues $\lambda_1^2 = 0, \lambda_2^2 = -k/m$, and $\lambda_3^2 = -(k/m)(1+2m/M)$, with associated eigenvectors/normal modes: \begin{align*}
    \vec{\mu}_1 &= \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix} \;\;(\text{translation}) \\
        \vec{\mu}_2 &= \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix} \;\;(\text{out of phase oscillation, }\omega_2 = \sqrt{k/m}) \\
    \vec{\mu}_3 &= \begin{pmatrix} 1 \\ -2m/M \\ 1 \end{pmatrix}\;\;(\omega_3 = \sqrt{(k/m)(1+2m/M)})
\end{align*}
Then a general solution of this system is given by \begin{equation*}
    \vec{\eta}(t) = \vec{\mu}_1(A+Bt) + \vec{\mu}_2C\cos(\omega_2(t-t_2)) + \vec{\mu}_3D\cos(\omega_3(t-t_3))
\end{equation*}




\section{Dilute Polyatomic Gas in the Canonical Ensemble}

For each molecule of $n$ atoms we write \begin{equation*}
    H_1 = \sum_{i=1}^n\frac{\vec{p}_i\cdot\vec{p}_i}{2m}+V_{int}(\vec{q}_1,...,\vec{q}_n)
\end{equation*}
Assume no interactions between the molecules: \begin{equation*}
    Z(T,N) = \frac{Z(T,1)^N}{N!} = \frac{1}{N!}\left[\int\prod_{i=1}^n\frac{d^3\vec{p}_id^3\vec{q}_i}{h^3}e^{-\beta\sum_{i=1}^n\frac{\vec{p}_i\cdot\vec{p}_i}{2m}-\beta V_{int}(\vec{q}_1,...,\vec{q}_n)}\right]^N
\end{equation*}
We take the approximation for $V_{int}$ at temperatures much smaller than the corresponding dissociation temperature $\approx 10^4\;K$: \begin{itemize}
    \item (Stable - non-zero eigenvalues) equilibrium positions $(\vec{q}_1^*,...,\vec{q}_n^*)$, \textbf{minimize} potential $V_{int}$ 
    \item Taylor expansion around equilibrium: \begin{equation*}
            V_{int} = V_{int}^* + \frac{1}{2}\sum_{i,j=1}^n\sum_{\alpha,\beta=1}^3\frac{\partial^2V_{int}}{\partial q_{i,\alpha}\partial q_{j,\beta}}\Bigg\rvert_{\{\vec{q}_i^*\}}u_{i,\alpha}u_{j,\beta} + O(u^3)
    \end{equation*}
        with $\vec{u}_i := \vec{q}_i-\vec{q}_i^*$
    \item We diagonalize the matrix with entries $\frac{\partial^2V_{int}}{\partial q_{i,\alpha}\partial q_{j,\beta}}$ ($3n\times 3n$ symmetric matrix) which has $3n$ real non-negative eigenvalues $\{k_s\}$ with associated eigenvectors, which we move to with a change of variables $\{\vec{u}_i\}\rightarrow \{\tilde{u}_s\}$ (amplitudes of eigenmods) and corresponding conjugate momenta $\tilde{p}_s = m\dot{\tilde{u}}_s$. This is a unitary or canonical transformation, and so it preserves lengths: $\prod_{i,\alpha}du_{i,\alpha}dp_{i,\alpha} = \prod_sd\tilde{u}_sd\tilde{p}_s$. Using this transformation we can write \begin{equation*}
            H_1 = V_{int}^* + \sum_{s=1}^{3n}\left[\frac{1}{2m}\tilde{p}_s^2 + \frac{k_s}{2}\tilde{u}_s^2\right]
    \end{equation*}
\end{itemize}

It follows that the partition function becomes \begin{align*}
    Z(T,1) &= \int\prod_s\frac{d\tilde{u}_sd\tilde{p}_s}{h}e^{-\beta\left[V_{int}^* + \sum_{s=1}^{3n}\left[\frac{1}{2m}\tilde{p}_s^2 + \frac{k_s}{2}\tilde{u}_s^2\right]\right]} \\
    &=e^{-\beta V_{int}^*}\prod_s\frac{1}{h}\left[\int d\tilde{u}_se^{-\beta\frac{k_s}{2}\tilde{u}_s^2}\int d\tilde{p}_se^{-\beta\frac{1}{2m}\tilde{p}_s^2}\right] \\
    &= e^{-\beta V_{int}^*}\frac{1}{h^{3n}}\left[\sqrt{\frac{2\pi}{\beta k_S}}\right]^{\tilde{n}}\left[\sqrt{\frac{2\pi m}{\beta}}\right]^{3n}\cdot \tilde{V}^{\frac{3n-\tilde{n}}{3}}
\end{align*}
where $\tilde{n}$ is the number of eigenvectors with $k_s \neq 0$. The average internal energy can then be written \begin{equation*}
    \langle H_1\rangle = -\frac{\partial \ln Z_1}{\partial \beta} = V_{int}^*+\frac{3n+\tilde{n}}{2}k_BT
\end{equation*}
where each \Emph{quadratic degree of freedom} classically contributes $k_BT/2$ (this is the equipartition theorem). $\tilde{n}$ corresponds with the number of \Emph{vibrational modes}. Note $\tilde{n} \neq 3n$ since $V_{int}$ depends on $\{\Delta \vec{q}_i\}$ only: \begin{itemize}
    \item translational symmetry ($V_{int}(\vec{q}_1+\vec{c},...,\vec{q}_n+\vec{c}) = V_{int}(\vec{q}_1,...,\vec{q}_n)$), so $k_{trans,i} = 0$
    \item rotational symmetry: $k_{rot,i} = 0$, for $0 < i \leq r$, where $r$ depends on the shape of the molecule, where generally $r = 3$
\end{itemize}
Then $\tilde{n} = 3n-3-r$ in three dimensions, so \begin{equation*}
    \langle H_1\rangle = V_{int}^* + \frac{6n-3-r}{2}k_BT
\end{equation*}
We can then find the specific heat capacities \begin{equation*}
    C_V = \frac{d\langle H_1\rangle}{dT} = \frac{6n-3-r}{2}k_B
\end{equation*}
and \begin{equation*}
    C_p = C_V+k_B = \frac{6n-1-r}{2}k_B
\end{equation*}
which are both independent of $T$. Note that the adiabatic exponent $\gamma :=\frac{C_p}{C_V}$ is what is experimentally accessible.


\section{Experimental Tests and QM Description}


Using our previous results we have the predictions that for a monoatomic gas $C_V/Nk_B = \frac{3}{2}$ and for a diatomic gas $C_V/Nk_B = \frac{7}{2}$ (note both are independent of $T$). However, for a diatomic gas experimentally we observe a decrease in $C_V/Nk_B$ from $7/2$ to $5/2$ and finally to $3/2$ as the temperature decreases - hence we see a breakdown of the classical description.

\begin{qst}
    Can QM explain this?
\end{qst}

\begin{itemize}
    \item Vibrational modes: For a diatomic molecule $\tilde{n} = 6-3-2 = 1$ - only vibrations along the $x$-axis with stiffness/spring constant $k_{vib}$ (harmonic oscillator) and normal frequency $\omega_{vib} = \sqrt{k_{vib}/m}$. The classical partition function is \begin{equation*}
            Z_{vib}^c = \frac{k_BT}{\hbar\omega}
    \end{equation*}
        and the classical average energy is \begin{equation*}
            \langle H_{vib}^c\rangle = 2\frac{k_BT}{2}
        \end{equation*}
        as expected.
\end{itemize}

Now, to explain why we observe these discrepancies we breakdown the Hamiltonian into different contributions: \begin{equation*}
    H_1 = H_{trans}(\vec{R},\vec{P}) + H_{rot}(\theta,\phi,\psi,p_{\theta},p_{\phi},p_{\psi}) + H_{vib}(\vec{q},\vec{p})
\end{equation*}
where the center coordinates are Euler angles, and the left coordinates are center of mass coordinates.

\begin{qst}
    Is this expression exact?
\end{qst}
In general vibrations lead to variations in moments of intertia, so $H_{vib}$ and $H_{rot}$ are coupled. Thus this expression assumes the variations in moments of intertia are small, so we can neglect the interactions. Now we look at different parts of the Hamiltonian individually and check whether QM can explain the experimental observations.

For a quantum harmonic oscillator we have $H_{vib}^q = \hbar\omega(n+\frac{1}{2})$, where $n = 0,1,2,3,...$, so we have quantized energy levels, and $n$ is the \Emph{occupation number}. For the QM partition function we have \begin{equation*}
    Z_{vib}^q = \sum_{n=0}^{\infty}e^{-\beta(\hbar \omega(n+1/2))} = \frac{e^{-\beta \hbar \omega/2}}{1-e^{-\beta\hbar\omega}}
\end{equation*}
Do we recover the classical result for high $T$? As $T\rightarrow \infty,\beta\rightarrow 0$ and we find \begin{equation*}
    Z_{vib}^* \approx \frac{1-\beta \hbar\omega/2}{\beta\hbar\omega} \approx \frac{k_BT}{\hbar\omega} = Z_{vib}^c
\end{equation*}
What happens for low $T$. We observe that \begin{equation*}
    \langle H_{vib}^q\rangle = \frac{\hbar\omega}{2}+\hbar\omega\frac{e^{-\beta\hbar\omega}}{1-e^{-\beta\hbar\omega}}
\end{equation*}
The first term independent of $T$ corresponds with quantum fluctuations while the second term dependent on $T$ corresponds with thermal fluctuations. Now the specific heat is \begin{equation*}
    C_{vib}^q=\frac{d\langle H_{vib}^q\rangle}{dT} = k_B(\hbar\omega\beta)^2\frac{e^{-\beta\hbar\omega}}{(1-e^{-\beta\hbar\omega})^2}
\end{equation*}
As $T\rightarrow 0$ associated momenta are frozen, and thermal fluctuations are insufficient to reach higher energy states. QM can explain the experimentally observed drop in $C_V$ from $7/2Nk_B$ to $5/2Nk_B$, corresponding to the breakdown of the classical equipartition theorem.

Next consider the rotational modes: the Lagrangian for rotations of a diatomic molcule (rigid body) are given by \begin{equation*}
    T_{rot}^c = \frac{I}{2}\left(\dot{\theta}^2+\sin^2\theta\dot{\phi}^2\right) = \mathcal{L}_{rot}^c
\end{equation*}
$\theta$ and $\phi$ are angles of orientation of the molecule and $I$ is the moment of inertia. Using the Lagrange equations we then have \begin{equation*}
    p_{\theta} = \frac{\partial \mathcal{L}_{rot}^c}{\partial \dot{\theta}} = I\dot{\theta},\;\;p_{\phi} = \frac{\partial \mathcal{L}_{rot}^c}{\partial \dot{\phi}} = I\sin^2\theta \dot{\phi}
\end{equation*}
which implies that the Hamiltonian is \begin{equation*}
    H_{rot}^c = \dot{\theta}p_{\theta} + \dot{\phi}p_{\phi}-\mathcal{L}_{rot}^c = \frac{1}{2I}\left(p_{\theta}^2+\frac{p_{\phi}^2}{\sin^2\theta}\right) = \frac{\vec{L}\cdot\vec{L}}{2I}
\end{equation*}


\begin{prop}
    An arbitrary rotation may be expressed as the product of three successive rotations about three (in general) different axis.
\end{prop}

A specific choicer corresponds to Euler angles: Substitute $x,y,z$ to rotations about the $z$-axis with $\phi$: $\xi,\eta,\zeta$. Then rotate $\theta$ about the $\xi$ axis giving $\xi',\eta',\zeta'$. Finally rotate $\psi$ about the $\xi'$ axis giving $x',y',z'$.

Then we have that $\vec{x}' = A\vec{x}$ where $A = BCD$ and $\theta \in [0,\pi],\phi,\psi\in[0,2\pi]$, and \begin{align*}
    D &= \begin{pmatrix} \cos\phi & \sin\phi & 0 \\ -\sin\phi & \cos\phi & 0 \\ 0 & 0 & 1 \end{pmatrix} \\
    C &= \begin{pmatrix} 1 & 0 & 0 \\ 0 & \cos\theta & \sin\theta \\ 0 & -\sin\theta & \cos\theta \end{pmatrix} \\
    B &= \begin{pmatrix} \cos\psi & \sin\psi & 0 \\ -\sin\psi & \cos\psi & 0 \\ 0 & 0 & 1 \end{pmatrix}
\end{align*}

It follows that the kinetic energy can be written as \begin{equation*}
    T_{rot} = \frac{I_1}{2}(\dot{\theta}^2+\dot{\phi}^2\sin^2\theta)+\frac{I_3}{2}(\dot{\psi}^2+\dot{\phi}^2\cos\theta)^2
\end{equation*}
for symmetrical tops with moments of inertia $I_1 = I_2, I_3$ in the principle axis frame. For a diatomic molecule $I_3 = 0$, so \begin{equation*}
    T_{rot} = \frac{I_1}{2}(\dot{\theta}^2+\dot{\phi}^2\sin^2\theta)
\end{equation*}
Then the rotational partition function becomes \begin{align*}
    Z_{rot}^c &= \frac{1}{h^2}\int_0^{\pi}d\theta\int_0^{2\pi}\int_{-\infty}^{\infty}dp_{\theta}dp_{\phi}e^{-\frac{\beta}{2I}\left(p_{\theta}^2+\frac{p_{\phi}^2}{\sin^2\theta}\right)} \\
    &= \frac{1}{h^2}2\pi\int_0^{\pi}\sqrt{\frac{2\pi I}{\beta}}\sqrt{\frac{2\pi I\sin^2\theta}{\beta}} \\
    &= \frac{4\pi^2}{h^2}\frac{I}{\beta}\int_0^{\pi}d\theta\sin\theta = \frac{2I}{\hbar^2\beta}
\end{align*}

Then the average rotational energy is \begin{equation*}
    \langle H_{rot}^c\rangle = \frac{1}{\beta} = 2\frac{k_BT}{2}
\end{equation*}

Now we go to QM where the angular momentum is quantized as $\vec{L}\cdot\vec{L} = \hbar^2l(l+1)$, $l = 0,1,2,3,...$, and each state is degenerated $2L=1$ times $(L_z = -l,-l+1,...,l-1,l)$. The natural choice for the partition function is \begin{equation*}
    Z_{rot}^q = \sum_{l=0}^{\infty}e^{-\beta(\hbar^2l(l+1)/2I)}(2l+1) = \sum_{l=0}^{\infty}e^{-\theta_{rot}l(l+1)/T}(2l+1)
\end{equation*}
where $\theta_{rot} = \frac{\hbar^2}{2Ik_B}$ (the characteristic temperature). Consider the limit of $T \gg \theta_{rot}$, in which case we can use the Euler-Maclaurin expansion: \begin{align*}
    Z_{rot}^q \approx \int_0^{\infty}dx(2x+1)e^{-\frac{\theta_{rot}x(x+1)}{T}} \\
    &= \frac{T}{\theta_{rot}} = Z_{rot}^c
\end{align*}
so we recover the classical partition function. Next, in the low temperature limit, $T\ll \theta_{rot}$, the partition function takes the form $Z_{rot}^q \approx 1 + 3e^{-2\theta_{rot}/T}$. Then the average energy in this limits is \begin{equation*}
    \langle H_{rot}^q\rangle \approx 6k_B\theta_{rot}e^{-2\theta_{rot}/T}
\end{equation*}
which gives the specific heat capacity \begin{equation*}
    C_{rot}^q \approx 3k_B\left(\frac{2\theta_{rot}}{T}\right)^2e^{-2\theta_{rot}/T}
\end{equation*}
Usually $\theta_{rot}\approx 1-10\;K$.

\textbf{Summary}: \begin{itemize}
    \item QM can explain the variations in the heat capacity of diatomic molecules with $T$
    \item Vibrational modes and associated momenta do not contribute below $\theta_{vib} \approx 10^3-10^4\;K$
    \item The two rotational modes do not contribute below $\theta_{rot} \approx 1-10\;K$
    \item For $T <\theta_{rot}$ the diatomic molecule behaves as a monoatomic particle
    \item Including $\frac{1}{h^{3n}}$ for the phase space volume and $\frac{1}{N!}$ for indistinguishable particles is not sufficient to save the classical stat mech.
    \item Full QM is necessary
    \item Note: at even lower $T$, the heat capacity vanishes (3rd law) due to other QM effects.
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%% Chapter 5.2
\chapter{Condensed Matter and Solid-State Physics}

\begin{defn}
    An \Emph{ordered solid} is an arrangement of atoms or molecules in a crystal structure or lattice.
\end{defn}

\begin{defn}
    A \Emph{crystal lattice} is an array of ``small boxes" infinitely repeating in all spatial directions (neglects boundary conditions); these boxes are called \Emph{unit cells} and it is the smallest unit of volume that contains all of the structural and symmetry information to build the macroscopic structure of the lattice by translation.
\end{defn}

Examples include cubic lattices, base-centered orthorhombic latices, etcetera.

\begin{qst}
    What can we say about the motion of the atoms within a solid?
\end{qst}



\section{Vibrations of a Solid}

We start by regarding a solid as a ``very large molecule" with $$H = \sum_{i=1}^N\frac{\vec{p}_i\cdot\vec{p}_i}{2m} + V_{int}(\vec{q}_1,...,\vec{q}_N)$$
and approximate $V_{int}$ as before: \begin{itemize}
    \item Equilibrium positions $(\vec{q}_1^*,...,\vec{q}_N^*)$ minimize $V_{int}$ (ground state of the solid). For an ordered solid we have a periodic arrangement of atoms forming the crystal lattice, which implies $$\vec{q}_i^* = \vec{q}^*(l,m,n) = l\hat{a}+m\hat{b}+n\hat{c}$$ for $l,m,n \in \Z$. In the following let $\vec{r} := l\hat{a}+m\hat{b}+n\hat{c}$.
    \item Consider $T \neq 0$, so we have small deformations. Performing a second order Taylor expansion on $V_{int}$ with $\vec{u}(\vec{r}) = \vec{q}_{\vec{r}} - \vec{r}$ being the perturbation from equilibrium \begin{equation*}
            V_{int} = V_{int}^* + \frac{1}{2} \sum_{\begin{array}{c}\vec{r},\vec{r}' \\ \alpha,\beta\end{array}}\frac{\partial^2V_{int}}{\partial q_{\vec{r},\alpha}\partial q_{\vec{r}',\beta}}u_{\alpha}(\vec{r})u_{\beta}(\vec{r}')
    \end{equation*}
    \item Next we diagonalize and find normal modes of the crystal. By translational symmetry of a lattice we know that \begin{equation*}
        \frac{\partial^2V}{\partial q_{\vec{r},\alpha}\partial q_{\vec{r}',\beta}} = K_{\alpha,\beta}(\vec{r}-\vec{r}')
    \end{equation*}
        which can be thought of as entries of a $3N\times 3N$ matrix, where $N$ is the number of lattice points. Then we apply the DFT which partially diagonalizes the matrix as we now see: \begin{equation*}
            u_{\alpha}(\vec{r}) = \sum_{\vec{k}} \frac{e^{i\vec{k}\cdot\vec{r}}}{\sqrt{N}}\tilde{u}_{\alpha}(\vec{k})
        \end{equation*}
        where $\vec{k}$ is the wavevector with $|\vec{k}| = \frac{2\pi}{\lambda}$, $e^{i\vec{k}\cdot\vec{r}}$ represents waves in the lattice (\Emph{phonons}), and $\tilde{u}_{\alpha}(\vec{k})$ are the Fourier coefficients or amplitudes. $\vec{k}$ is restricted due to the discrete periodic lattice arrangement (wavelength can't be shorter than twice lattice spacing). For example in a simple cubic lattice of spacing $a$, we require $k_{\alpha} \in [-\pi/a,\pi/a]$, for $\alpha \in \{x,y,z\}$ corresponding to two transversal and one longitudinal orientation. This restricted region is known as the \Emph{Brillouin zone}. So we have an upper bound on $|\vec{k}| \equiv$ a lower bound on wavelength $\lambda$.
\end{itemize}

\begin{qst}
    Why are larger values of $|k_{\alpha}|$ excluded?
\end{qst}
Any $\vec{k}$ outside the Brillouin zone can be written as the sum of $\vec{k}_{in}$ and $\vec{k}'$ where $\vec{k}' = \frac{2\pi}{a}\begin{pmatrix} i \\ j \\ k\end{pmatrix}$ with $i,j,k \in \Z$ and $\vec{k}_{in} \in$ the Brillouin zone. It follows that $\vec{k}\cdot\vec{r} = \vec{k}_{in}\cdot\vec{r} + 2\pi i'$ for $i' \in \Z$, so the complex exponential of the two wave-vectors are equivalent. So wavevectors inside Brillouin zone fully determine the $u_{\alpha}(\vec{r})$. Then we can write \begin{equation*}
    V_{int} = V_{int}^* + \frac{1}{2N}\sum_{\begin{array}{c} (\vec{r},\vec{r}') \\ (\vec{k},\vec{k}') \\ \alpha,\beta\end{array}} K_{\alpha\beta}(\vec{r}-\vec{r}')e^{i\vec{k}\cdot\vec{r}}\tilde{u}_{\alpha}(\vec{k})e^{i\vec{k}'\cdot\vec{r}'}\tilde{u}_{\beta}(\vec{k}')
\end{equation*}
Now we perform a change of variables to relative, $\vec{\rho} = \vec{r} - \vec{r}'$, and center of mass, $\vec{R} = \frac{\vec{r}+\vec{r}'}{2}$, coordinates. This turns the potential into \begin{equation*}
    V_{int} = V_{int}^* + \frac{1}{2N}\sum_{\begin{array}{c} \vec{k},\vec{k}' \\ \alpha,\beta \end{array}}\left(\sum_{\vec{R}}e^{i(\vec{k}+\vec{k}')\cdot\vec{R}}\right)\left(\sum_{\vec{\rho}}K_{\alpha\beta}(\vec{\rho})e^{i(\vec{k}-\vec{k}')\cdot\vec{\rho}/2}\tilde{u}_{\alpha}(\vec{k})\tilde{u}_{\beta}(\vec{k}')\right)
\end{equation*}
The first bracket takes the form $N\delta_{\vec{k}+\vec{k}',0}$ in the thermodynamic limit. Then we can write \begin{equation*}
    V_{int} = V_{int}^*+\frac{1}{2}\sum_{\vec{k},\alpha,\beta}\left[\sum_{\vec{\rho}}K_{\alpha\beta}(\vec{\rho})e^{i\vec{k}\cdot\vec{\rho}}\right]\tilde{u}_{\alpha}(\vec{k})\tilde{u}_{\beta}(-\vec{k})
\end{equation*}
where the first bracketed term we denote by $\tilde{K}_{\alpha\beta}(\vec{k})$, and $\tilde{u}_{\beta}(-\vec{k}) = \tilde{u}_{\beta}^*(\vec{k})$. So we have \begin{equation*}
    V_{int} = V_{int}^*+\frac{1}{2}\sum_{\vec{k},\alpha,\beta}\tilde{K}_{\alpha\beta}(\vec{k})\tilde{u}_{\alpha}(\vec{k})\tilde{u}_{\beta}^*(\vec{k})
\end{equation*}
which is a linear superposition of contributions depending on single wave vectors. We assume the Fourier modes are decoupled yielding partial diagonalization. We now need to diagonalize $[\tilde{K}_{\alpha\beta}(\vec{k})]$ for each $\vec{k}$. These matrices depend on the specific crystal and its symmetries, so for simplicity we assume $\tilde{K}_{\alpha\beta}(\vec{k}) = \delta_{\alpha,\beta}\tilde{K}(\vec{k})$. Then the Hamiltonian becomes \begin{equation*}
    H = V_{int}^* + \sum_{\vec{k},\alpha}\left[\frac{1}{2m}|\tilde{p}_{\alpha}(\vec{k})|^2 + \frac{\tilde{K}(\vec{k})}{2}|\tilde{u}_{\alpha}(\vec{k})|^2\right] 
\end{equation*}
where $T = \sum_{i=1}^N\frac{m}{2}\dot{\vec{q}}_i^2 = \sum_{\vec{k},\alpha}\frac{m}{2}\dot{\tilde{u}}_{\alpha}(\vec{k})\dot{\tilde{u}}_{\alpha}(\vec{k})^* = \sum_{\vec{k},\alpha}\frac{|\tilde{p}_{\alpha}(\vec{k})|^2}{2m}$, with $\tilde{p}_{\alpha}(\vec{k}) = \frac{\partial \mathcal{L}}{\partial \dot{\tilde{u}}_{\alpha}(\vec{k})} = m\dot{\tilde{u}}_{\alpha}(\vec{k})$. This is the Hamiltonian for small deformations, which equates to $3N$ independent harmonic oscillators with frequencies $\omega_{\alpha}(\vec{k}) = \sqrt{\frac{\tilde{K}(\vec{k})}{m}}$ in the wave amplitudes, which reduces us to the same problem as the dilute polyatomic gas in the canonical ensemble. Then \begin{equation*}
    \langle H\rangle = V_{int}^*+\frac{3N+\tilde{n}}{2}k_BT \geq V^* + \frac{6N-6}{2}k_BT
\end{equation*}
and as $N\rightarrow \infty$, $C = C_V \approx 3Nk_B$.


\section{Experimental Tests and QM Description}

At $T$ near zero we observe $C_V\propto T^3$. Can QM explain this as well? First observe that \begin{equation*}
    H^q = V_{int}^* + \sum_{\vec{k},\alpha}\hbar\omega_{\alpha}(\vec{k})\left(n_{\vec{k},\alpha}+\frac{1}{2}\right)
\end{equation*}
with QM microstate $\{n_{\vec{k},\alpha}\}, n_{\vec{k},\alpha} = 0,1,2,3,...$ ($3N$ occupation numbers). The partition function is then \begin{equation*}
    Z^q = \sum_{\{n_{\vec{k},\alpha}\}}e^{-\beta H^q} = e^{-\beta E_0}\prod_{\vec{k},\alpha}\frac{1}{1-e^{-\beta\hbar\omega_{\alpha}(\vec{k})}}
\end{equation*}
where $E_0 = V_{int}^* + \sum_{\vec{k},\alpha}\hbar\omega_{\alpha}(\vec{k})\frac{1}{2}$ is the ground state energy. THe average energy is \begin{equation*}
    \langle H^q\rangle = E_0+\sum_{\vec{k},\alpha}\hbar\omega_{\alpha}(\vec{k})\langle n_{\vec{k},\alpha}\rangle
\end{equation*}
where \begin{equation*}
    \langle n_{\vec{k},\alpha}\rangle = \sum_{n_{\vec{k}_1,1}}\cdots\sum_{n_{\vec{k}_N,3}}n_{\vec{k}_i,j}\frac{e^{-\beta H^q}}{Z^q}  = \frac{1}{e^{\beta\hbar\omega_{\alpha}(\vec{k})} - 1}
\end{equation*}
Then \begin{equation*}
    \langle H^q\rangle = E_0+\sum_{\vec{k},\alpha}\hbar\omega_{\alpha}(\vec{k})\frac{1}{e^{\beta\hbar\omega_{\alpha}(\vec{k})}-1}
\end{equation*}
Left we have to determine $\omega_{\alpha}(\vec{k})$ or equivalently $\tilde{K}(\vec{k})$ which correspond to eigenvalues or spring constant/stiffness:

\begin{itemize}
    \item We have the Einstein model: Assume $\omega_{\alpha}(\vec{k}) = \omega_E$ (identical frequencies) Then the average energy becomes \begin{equation*}
            \langle H^q\rangle = E_0 + 3N\frac{\hbar\omega_E}{e^{\beta\hbar\omega_E}-1}
    \end{equation*}
        and $C_V = 3Nk_B\left(\frac{T_E}{T}\right)^2\frac{e^{-T_E/T}}{(1-e^{-T_E/T})^2}$ with $T_E := \frac{\hbar\omega_E}{k_B}$. This is not consistent with experiments where $C \propto T^3$ for $T\rightarrow 0$.
    \item In the Debye model we take the Taylor series expansion of $\tilde{K}(\vec{k})$ for small $|\vec{k}|$ (or long wavelengths): \begin{equation*}
            \tilde{K}(\vec{k}) = C+\vec{A}\cdot\vec{k}+\vec{k}\cdot(B\vec{k})+\cdot
    \end{equation*}
        By symmetry of $K(\vec{r}-\vec{r}')$, $K(\vec{r}-\vec{r}') = K(\vec{r}'-\vec{r})$ which implies $\tilde{K}(\vec{k}) = \tilde{K}(-\vec{k})$, so no odd terms ($\vec{A} = 0$). Translations correspond to $\vec{k} = 0$ and have $\tilde{K} = 0$, so continuity gives $\lim\limits_{\vec{k}\rightarrow 0}\tilde{K}(\vec{k}) = 0$, which implies $C = 0$. Ignoring crystal symmetries we can assume $B$ is diagonal with diagonal entries $B$, so \begin{equation*}
            \tilde{K}(\vec{k}) = Bk^2 + O(k^4)
        \end{equation*}
        and $\omega(\vec{k}) = \sqrt{\frac{\tilde{K}(\vec{k})}{m}} = \sqrt{\frac{B}{m}}k$ is our \Emph{dispersion relation} for phonons. $v := \sqrt{\frac{B}{m}}$ is the speed of sound in crystals. 
        \begin{defn}
            \Emph{Phonons} are the quanta of vibrational modes characterized by $\vec{k}$ (equivalent to photons in EM)
        \end{defn}
        full dispersion relation of phonons in a real isotropic crystal depends on $\omega$, with $\omega_l$: longitudinal waves and $\omega_t$: transversal waves, corresponding to different speeds of sound. 
        
        Now we have the average energy \begin{equation*}
            \langle H^q\rangle = E_0 + \sum_{\vec{k},\alpha} \frac{\hbar vk}{e^{\beta\hbar vk}-1}
        \end{equation*}
        Over which $\vec{k}$ do we have to sum? We have periodic boundary conditions for a lattice $(L_x\times L_y\times L_z)$, so we require standing waves: \begin{equation*}
            \vec{k} = 2\pi \begin{pmatrix} n_x/L_x \\ n_y/L_y \\ n_z/L_z \end{pmatrix}
        \end{equation*}
        for $n_x,n_y,n_z \in \Z$. In the thermodynamic limit, $V\rightarrow \infty$, the number of modes in volume element $d^3\vec{k}$ is \begin{equation*}
            dN = dn_xdn_ydn_z = \frac{dk_x}{2\pi/L_x}\frac{dk_y}{2\pi/L_y}\frac{dk_z}{2\pi/L_z}=\frac{V}{(2\pi)^3}d^3\vec{k}
        \end{equation*}
        Then in the limit $V\rightarrow \infty$ we approximate \begin{equation*}
            \langle H^q\rangle = E_0 + 3V \int_{B.Z.} \frac{d^3\vec{k}}{(2\pi)^3}\frac{\hbar vk}{e^{\beta\hbar vk}-1}
        \end{equation*}
        Can we reproduce the classical result for $T\rightarrow \infty$?
        \begin{equation*}
            \langle H^q\rangle \approx E_0 + 3V\int_{B.Z}\frac{d^3\vec{k}}{(2\pi)^3}\cdot\frac{1}{\beta} = E_0+3Nk_BT
        \end{equation*}
        which agrees with the classical case and is independent of the Brillouin zone.

        What happens for low $T$? Define $T_D := \frac{\hbar vk_{max}}{k_B} \approx \frac{\hbar v}{k_B}\cdot\frac{\pi}{a}$ for a simple cubic unit cell. For $T << T_D$, we can replace the B.Z. with all space in the integral and make the change of variables $x := \beta\hbar v|\vec{k}|$ and $d^3\vec{k} = 4\pi x^2dx/(\beta\hbar v)^3$:\begin{equation*}
            \langle H^q \rangle \approx E_0+\frac{3V}{8\pi^3}\left(\frac{k_BT}{\hbar v}\right)^34\pi k_BT\int_0^{\infty}dx\frac{x^3}{e^x-1} = E_0+\frac{\pi^2}{10}V\left(\frac{k_BT}{\hbar v}\right)^3k_BT
        \end{equation*}
        and $C_V \approx k_BV\frac{2\pi^2}{5}\left(\frac{k_BT}{\hbar v}\right)^3 \propto T^3$ as expected from experiments, and which is independent of the Brillouin zone.
\end{itemize}

For $T \ll T_D$ we have the following physical interpretations: \begin{itemize}
    \item Only a fraction of the phonons can be thermally excited, these are the low-frequency ones with energy quanta $\hbar\omega(\vec{k}) < k_BT$ since $\langle n_{\vec{k}}\rangle = \frac{1}{e^{\beta\hbar\omega(\vec{k})}-1}$
    \item The corresponding wave vectors are \begin{equation*}
            |\vec{k}| < k^*(T) = \frac{\omega^*(T)}{v} = \frac{k_BT/\hbar}{v}
    \end{equation*}
    \item The number of these modes in $d$ dimensions is: \begin{equation*}
            Vk^*(T)^d\sim V\left(\frac{k_BT}{\hbar v}\right)^d
    \end{equation*}
    \item Treating each excited mode classically, it should contribute $k_BT$ to $\langle H^q\rangle$, so $\langle H^q\rangle \sim V\left(\frac{k_BT}{\hbar v}\right)^dk_BT$, and $C_V\sim Vk_B\left(\frac{k_BT}{\hbar v}\right)^d \sim T^d$.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%% Chapter 5.3
\chapter{Black-body Radiation}

A black-body absorbs light perfectly at all wavelengths, and re-emits EM radiation. The experimental design consists of a 3D box (cavity) in a heat bath (so we are at thermal equilibrium). We take observations of emitted EM radiation through a tiny hole of area $A$.
\begin{itemize}
    \item The escaping energy flux per unit area and per unit time follows the Stefan-Boltzmann law: \begin{equation*}
            \phi = \sigma T^4
    \end{equation*}
        with $\sigma \approx 5.67\times 10^{-8}Wm^{-2}K^{-4}$ (Stefan's constant)
    \item The distribution of emitted wavelengths/frequencies (=spectrum) is universal and only depends on $T$
\end{itemize}
In nature this is seen in cosmic background radiation



\section{Classical Description}

The Hamiltonian for electromagnetic field in Fourier space using appropriate coordinates in a vacuum is \begin{equation*}
    H^c = \frac{1}{2}\sum_{\vec{k},\alpha}\left[|\tilde{p}_{\vec{k},\alpha}|^2 + \omega_{\alpha}(\vec{k})^2|\tilde{u}_{\alpha}(\vec{k})|^2\right]
\end{equation*}
Photons (EM waves) are characterized by wavenumber $\vec{k}$ and two possible polarizations $\alpha$ (no longitudinal mode since $\nabla\cdot\vec{E} = 0$), and $\omega_{\alpha}(\vec{k}) = ck$, where $c$ is the speed of light. The sum of harmonic oscillators as for the case of phonons. Do we sum over the same $\vec{k}$ as for phonons? The box side length $L$ will induce periodic boundary conditions (standing waves): $\vec{k} = \frac{2\pi}{L}\begin{pmatrix} n_x \\ n_y \\ n_z\end{pmatrix}$, but no Brillouin zone since there is no discrete lattice for photons. $|\vec{k}|$ can be arbitrarily large, $n_x,n_y,n_z$ are unbounded, giving infinitely many Fourier modes $(N = \infty)$.

In the classical case this implies infinite energy (each Fourier mode carries $k_BT$ energy) resulting in the \Emph{ultraviolet catastrophe} (large $|\vec{k}| = $ small $\lambda$).

\section{QM Description (Planck)}

The QM Hamiltonian is \begin{equation*}
    H^q = \sum_{\vec{k},\alpha}\hbar\omega_{\alpha}(\vec{k})(n_{\vec{k},\alpha} + \frac{1}{2}) = \sum_{\vec{k},\alpha}\hbar cK(n_{\vec{k},\alpha}+\frac{1}{2})
\end{equation*}
with QM microstate $\{n_{\vec{k},\alpha}\}$, $n_{\vec{k},\alpha} = 0,1,2,3,...$ as for phonons, giving $2N$ occupation numbers (number of photons with a given wavevector and polarization)
The average energy is then \begin{equation*}
    \langle H^q \rangle = E_0+\sum_{\vec{k},\alpha}\hbar\omega_{\alpha}(\vec{k}) \langle n_{\vec{k},\alpha}\rangle = E_0+\sum_{\vec{k},\alpha}\hbar ck\frac{1}{e^{\beta\hbar ck}-1}
\end{equation*}
with $E_0 = \sum_{\vec{k},\alpha}\hbar ck\frac{1}{2}$. In the thermodynamic limit $V = L^3 \rightarrow \infty$: the number of modes in a volume element $d^3\vec{k}$ is \begin{equation*}
    dN = \frac{V}{(2\pi)^3}d^3\vec{k}
\end{equation*}
Then the average energy per unit volume is \begin{equation*}
    \frac{\langle H^q\rangle}{V} \approx \int_{-\infty}^{\infty}d^3\vec{k}\frac{2}{(2\pi)^2}\hbar ck\frac{1}{2}+\int_{-\infty}^{\infty}d^3\vec{k}\frac{2}{(2\pi)^3}\frac{\hbar ck}{e^{\beta \hbar ck}-1}
\end{equation*}
where the first integral diverges and is an infinite zero point energy density per volume, but constant, and the second integral is what we take as $\frac{\langle H^q\rangle^*}{V}$ (since potentials are only defined up to a constant). The finite energy stored in excited modes $\langle H^q\rangle^*$, as the zero-point energy is irrelevant, as only energy differences can be measured (no ultraviolet catastrophe).

\begin{qst}
    Can QM reproduce the Stefan-Boltzmann law?
\end{qst}

The number of photons hitting the entry area $A$ in time interval $dt$ from direction $\vec{\Omega}$ is \begin{equation*}
    dN(\vec{\Omega}) = A\cdot cdt\cos\theta \cdot n\cdot \frac{d\vec{\Omega}}{4\pi}
\end{equation*}
which implies $dN(\theta) = Acdt\cos\theta n\frac{1}{4\pi}\int d\vec{\Omega}\Bigg\rvert_{\theta,\theta+d\theta}$, and hence $dN(\theta) = Acdt\cos\theta n \frac{\sin\theta 2\pi d\theta}{4\pi}$ and \begin{equation*}
    \frac{N}{Adt} = cn\frac{1}{2}\int_0^{\pi/2}d\theta\cos\theta\sin\theta = \frac{c}{4}n
\end{equation*}
Similarly for the average escaping energy flux per unit area and per unit time: \begin{equation*}
    \phi = \frac{c}{4}\frac{\langle H^q\rangle^*}{V} = \sigma T^4
\end{equation*}
with $\sigma = \frac{\pi^2}{60}\frac{k_B^4}{\hbar^3c^2}$ universal.

Can QM also explain the black-body spectrum? \begin{equation*}
    \frac{\langle H^q\rangle^*}{V} = \int_0^{\infty}dk\frac{\hbar c}{\pi^2}\frac{k^3}{e^{\beta\hbar ck}-1}
\end{equation*}
where the integrand is the energy density in the wave vector $k$, $\epsilon(k,T)$. The average escaping energy flux per unit area and per unit time in the interval $[k,k+dk]$: \begin{equation*}
    I(k,T)dk = \frac{c}{4}\epsilon(k,T)dk \approx \left\{\begin{array}{cc} ck_BTk^3/(4\pi)^2,& k\ll k^*(T) \\ \hbar c^2k^3e^{-\beta\hbar ck}/(4\pi)^2,& k\gg k^*(T) \end{array}\right.
\end{equation*}
with $k^*(T) = k_BT/(\hbar c)$ as experimentally observed. From $\sigma$ and $I_{max,k}(k,T)$ one can determine $k_B$.

What can we say about other thermodynamic properties of blackbody radiation?

\begin{equation*}
    Z^q = e^{-\beta E_0}\prod_{\vec{k},\alpha}\frac{1}{1-e^{-\beta\hbar ck}}
\end{equation*}
with $E_0 = \sum_{\vec{k},\alpha}\hbar ck\frac{1}{2}$, so $Z = \prod_{\vec{k},\alpha}\frac{e^{-\beta\hbar ck/2}}{1-e^{-\beta\hbar ck}}$. The free energy is then \begin{equation*}
    F = -k_BT\ln Z = k_BT\sum_{\vec{k},\alpha}\left[\beta\hbar ck/2+\ln(1-e^{-\beta \hbar ck})\right]
\end{equation*}
In the thermodynamic limit, $dN = \frac{V}{(2\pi)^3}d^3\vec{k}$, and \begin{equation*}
    F \approx \frac{V}{\pi^2}\int_0^{\infty}dk\left[\frac{\hbar ck^3}{2}+k^2k_BT\ln(1-e^{-\beta \hbar ck})\right]
\end{equation*}
WE then have a pressure \begin{equation*}
    P = -\frac{\partial F}{\partial V}\Bigg\rvert_T \approx -\int_0^{\infty}\frac{\hbar ck^3}{2\pi^2}dk - \int_0^{\infty}\frac{k_BT}{\pi^2}k^2\ln(1-e^{-\beta \hbar ck})dk
\end{equation*}
where the first term is $P_0$, the infinite zero-point pressure (negative!), which is related to the Casimir force. Proceeding by integration by parts on the second integral we can write \begin{equation*}
    P\approx P_0+\frac{1}{3}\frac{\langle H^q\rangle^*}{V}
\end{equation*}



\section{Black-hole thermodynamics}


%%%%%%%%%%%%%%%%%%%%%% Chapter 5.4
\chapter{Quantum Microstates}


The behaviour of many systems for low $T$ is not captured by classical stat mech (heat capacity of polyatomic gas or solid, black-body radiation,...) including the 3rd law of thermodynamics. We need a theory that explains low $T$ behaviour (QM).

For classical systems our microstates were $\{\vec{p}_i,\vec{q}_i\}$, while in quantum mechanics a microstate is given by a \Emph{unit vector} in hilbert space $|\psi\rangle$. If we have an orthonormal basis $\{|n\rangle\}$, we can write $$|\psi\rangle = \sum_n\langle n \vert \psi\rangle |n\rangle$$
For example if we take the coordinate basis $|\{\vec{q}_i\}\rangle$ we can write \begin{equation*}
    \langle \{\vec{q}_i\}|\psi\rangle = \psi(\vec{q}_1,...,\vec{q}_N)
\end{equation*}
Normalization means that \begin{equation*}
    \langle \psi|\psi\rangle = \sum_n\langle \psi|n\rangle \langle n|\psi\rangle = 1
\end{equation*}
or in the case of a continuous basis \begin{equation*}
    \langle \psi|\psi\rangle = \int \prod_{i=1}^Nd^d\vec{q}_i|\psi(\vec{q}_1,...,\vec{q}_N)|^2 = 1
\end{equation*}


\section{Observables}

In a classical system an observable is a function $O(\{\vec{p}_i,\vec{q}_i\})$, while in quantum mechanics an observable is a Hermitian operator on the Hilbert space - $O = O^{\text{\textdagger}} = \overline{O}^t$. 

\begin{rmk}
    Experimentally observed values of $O$ are not unique for fixed $|\psi\rangle$, instead it is a random variable: all eigenvalues of $O$ can in principle be observed for an orthonormal eigenbasis of $O$: $O|n\rangle = o_n|n\rangle$. Then $$|\langle n|\psi\rangle|^2 = \text{the probability to measure eigenvalue $o_n$}$$
    The expectation value or average for an operator is \begin{equation*}
        \langle O \rangle = \langle \psi|O|\psi\rangle = \sum_no_n|\langle n|\psi\rangle|^2
    \end{equation*}
\end{rmk}




\section{Time Evolution}

In a classical system time evolution is described by the Hamiltonian $H(\{\vec{p}_i,\vec{q}_i\})$ plus Hamilton's equations of motion $\partial_t\vec{p}_i = -\frac{\partial H}{\partial \vec{q}_i}$ and $\partial_t\vec{q}_i = \frac{\partial H}{\partial \vec{p}_i}$. In quantum mechanics we have the Schr\"{o}dinger equation \begin{equation*}
    i\hbar\partial_t|\psi(t)\rangle = H|\psi(t)\rangle
\end{equation*}

For an orthonormal eigenbasis of $H$, $H|n\rangle = \mathcal{E}_n|n\rangle$, we can represent $|\psi(t)\rangle = \sum_n\langle n|\psi(t)\rangle |n\rangle$, so by Schr\"{o}dinger's equation $i\hbar\partial_t\langle n|\psi(t)\rangle = \mathcal{E}_n\langle n|\psi(t)\rangle$ for all $n$, and \begin{equation*}
    \langle n|\psi(t)\rangle = e^{-i\mathcal{E}_nt/\hbar}\langle n|\psi(0)\rangle
\end{equation*}


%%%%%%%%%%%%%%%%%%%%% Chapter 5.5
\chapter{Quantum Macrostates}

\begin{rmk}
    The macrostate depends only on a few thermodynamic quantities. Many different microstates belong to the same macrostate - ensembles. For a fixed macrostate, each corresponding microstate occurs with a certain probability - ensemble averages.
\end{rmk}

In the case of a classical system: \begin{equation*}
    \langle O(\{\vec{p}_i,\vec{q}_i\})\rangle(t) = \left\{\begin{array}{cc} \int\prod_{i=1}^Nd^3\vec{p}_id^3\vec{q}_iO(\{\vec{p}_i,\vec{q}_i\})\rho(\{\vec{p}_i,\vec{q}_i\},t) & \text{continuous} \\ \sum_{\alpha}p_{\alpha}O(\mu_{\alpha}(t)) & \text{discrete} \end{array}\right.
\end{equation*}
In QM we need to take two averages. The value of $O$ for fixed microstate is a random variable. Then the microstates are also random variables (for \Emph{mixed states}). Then \begin{align*}
    \overline{\langle O\rangle} &:= \sum_{\alpha}p_{\alpha}\langle O\rangle_{\alpha} = \sum_{\alpha}p_{\alpha}\langle \psi_{\alpha}(t)|O|\psi_{\alpha}(t)\rangle \\
    &= \sum_{\alpha,m,n}p_{\alpha}\langle \psi_{\alpha}|m\rangle\langle m|O|n\rangle\langle n|\psi_{\alpha}\rangle \tag{$\langle m|n\rangle=\delta_{m,n}$} \\
    &= \sum_{\alpha,m,n}p_{\alpha}\langle n|\psi_{\alpha}\rangle\langle \psi_{\alpha}|m\rangle\langle m|O|n\rangle \\
    &= \sum_{m,n}\Bigg\langle n \Bigg\vert\sum_{\alpha}p_{\alpha}|\psi_{\alpha}\rangle\langle \psi_{\alpha}|\Bigg\vert m \Bigg\rangle\langle m|O|n\rangle \\
    &= \sum_{m,n}\langle n|\rho(t)|m\rangle\langle m|O|n\rangle \\
    &= \sum_n\langle n|\rho(t)O|n\rangle = tr(\rho(t)O)
\end{align*}
using the fact that $\sum_m|m\rangle\langle m|$ is the identity. Here \begin{equation*}
    \rho(t) = \sum_{\alpha}p_{\alpha}|\psi_{\alpha}\rangle\langle \psi_{\alpha}|
\end{equation*}
plays the same role as the phase space density in the classical case.




\section{Density Matrix}

\begin{qst}
    What are the properties of $\rho$? Do they coincide with the properties of the phase space density?
\end{qst}

\begin{itemize}
    \item $\rho$ must be normalized: \begin{align*}
            \overline{\langle \text{id}\rangle} &= tr(\rho) = \sum_n\langle n|\rho|n\rangle = \sum_{\alpha,n}p_{\alpha}\langle n|\psi_{\alpha}\rangle\langle \psi_{\alpha}|n\rangle \\
            &= \sum_{\alpha,n}p_{\alpha}|\langle n|\psi_{\alpha}\rangle|^2 = \sum_{\alpha}p_{\alpha}\sum_n|\langle n|\psi_{\alpha}\rangle|^2 \\
            &= \sum_{\alpha}p_{\alpha} = 1
        \end{align*}
    \item $\rho$ corresponds to a pure state if and only if $\rho^2 = \rho$ (assume $|\psi_{\alpha}\rangle$ are orthonormal without loss of generality): \begin{align*}
            \rho^2 &= \left(\sum_{\alpha}p_{\alpha}|\psi_{\alpha}\rangle \langle \psi_{\alpha}|\right)\left(\sum_{\beta}p_{\beta}|\psi_{\beta}\rangle\langle \psi_{\beta}|\right) \\
            &= \sum_{\alpha,\beta}p_{\alpha}p_{\beta}|\psi_{\alpha}\rangle\langle \psi_{\alpha}|\psi_{\beta}\rangle\langle\psi_{\beta}| \\
            &= \sum_{\alpha}p_{\alpha}^2|\psi_{\alpha}\rangle\langle \psi_{\alpha}|
    \end{align*}
        For a \Emph{pure state} $\beta$: $p_{\alpha} = \delta_{\alpha,\beta}$, which implies $\rho^2 = \rho$. Conversely, if $\rho^2 = \rho$, $tr(\rho^2) = tr(\rho) = 1$, which implies $\sum_{\alpha}p_{\alpha}^2 = 1$ which implies we have a pure state since if any $p_{\alpha} \in (0,1)$, $p_{\alpha}^2 < p_{\alpha}$. The density matrix allows us to describe pure and mixed states.
    \item $\rho$ is Hermitian $(\rho^{\text{\textdagger}} = \rho)$: \begin{align*}
            \langle m|\rho^{\text{\textdagger}}|n\rangle &= \langle n | \rho |m\rangle^* = \sum_{\alpha}p_{\alpha}\langle n | \psi_{\alpha}\rangle^*\langle \psi_{\alpha}|m\rangle^* \\
            &= \sum_{\alpha}p_{\alpha}\langle \psi_{\alpha}|n\rangle\langle m|\psi_{\alpha}\rangle \\
            &= \sum_{\alpha}p_{\alpha}\langle m|\psi_{\alpha}\rangle \langle \psi_{\alpha}|n\rangle = \langle m|\rho|n\rangle
    \end{align*}
        which implies $\overline{\langle O\rangle} \in \R$.
    \item $\rho$ is positive definite: \begin{equation*}
            \langle \phi|\rho|\phi\rangle = \sum_{\alpha}p_{\alpha}\langle \phi|\psi_{\alpha}\rangle\langle \psi_{\alpha}|\phi\rangle = \sum_{\alpha}p_{\alpha}|\langle \phi|\psi_{\alpha}\rangle|^2\geq 0
    \end{equation*}
        This implies all eigenvalues are non-negative. The eigenvalues are the $p_{\alpha}$ if $|\psi_{\alpha}\rangle$ are orthonormal.
\end{itemize}

Moving forward we notate $\langle O\rangle = tr(\rho O)$.



\section{Time Evolution of Desity Matrix and Equilibrium}

For classical systems we have Liouville's equation which says that the phase space density acts as an incompressible fluid: \begin{equation*}
    0=\frac{d}{dt}\rho = \partial_t\rho - \sum_{\alpha=1}^{3N}\frac{\partial H}{\partial q_{\alpha}}\frac{\partial \rho}{\partial p_{\alpha}} - \frac{\partial H}{\partial p_{\alpha}}\frac{\partial \rho}{\partial q_{\alpha}} = \partial_t\rho - \{H,\rho\}
\end{equation*}
where $\{H,\rho\}$ is the \Emph{Poisson bracket}. This implies that \begin{equation*}
    \partial_t\rho = \{H,\rho\}
\end{equation*}
In quantum mechanics we instead have the Schr\"{o}dinger equation, which implies \begin{align*}
    i\hbar\partial_t\langle n|\rho(t)|m\rangle &= i\hbar \partial_t\left[\sum_{\alpha}p_{\alpha}\langle n|\psi_{\alpha}(t)\rangle\langle \psi_{\alpha}(t)|m\rangle\right] \\
    &= \sum_{\alpha}p_{\alpha}i\hbar\left[\left(\partial_t\langle n|\psi_{\alpha}(t)\rangle\right)\langle \psi_{\alpha}(t)|m\rangle + \langle n|\psi_{\alpha}(t)\rangle\left(\partial_t\langle \psi_{\alpha}(t)|m\rangle\right)\right] \\
    &= \sum_{\alpha}p_{\alpha}\left[\langle n|H|\psi_{\alpha}(t)\rangle\langle\psi_{\alpha}(t)|m\rangle - \langle n|\psi_{\alpha}(t)\rangle \langle \psi_{\alpha}(t)|H|m\rangle \right] \\
    &= \langle n | H\rho | m\rangle - \langle n | \rho H| m \rangle \\
    &= \langle n|[H,\rho]|m\rangle
\end{align*}
Thus the quantum equivalent to Liouville's theorem is \begin{equation*}
    i\hbar \partial_t\rho(t) = [H,\rho]
\end{equation*}
In equilibrium we have $\partial_t\rho = 0$, so $[H,\rho] = 0$. As in analogy with classical statistical mechanics, we postulate that $\rho = \rho(H)$.



%%%%%%%%%%%%%%%%%%%%% Chapter 5.6
\chapter{Quantum Ensembles}

\begin{qst}
    What is the density matrix for a given ensemble?
\end{qst}

\section{Microcanonical Ensemble}

Recall in the microcanonical ensemble our macrostate is $(E,\vec{q},\vec{N})$, corresponding with an isolated system with internal energy $E = $ constant. In equilibrium $[H,\rho] = 0$, which implies $\rho$ and $H$ have a common set of eigenstates.

\begin{rmk}
    The \Emph{central postulate of statistical mechanics} is that all allowed states are equally likely $(\rho = \rho(H))$.
\end{rmk}

In the basis of energy eigenstates we can write \begin{equation*}
    \langle n|\rho|m\rangle = \sum_{\alpha}p_{\alpha}\langle n|\psi_{\alpha}\rangle\langle \psi_{\alpha}|m\rangle = \left\{\begin{array}{cc} \frac{1}{\Omega}, & \varepsilon_n = E, m =n \\ 0, & \varepsilon_n = E, m \neq n \end{array}\right.
\end{equation*}
with $\Omega$ fixed by normalization, $tr\rho = 1$, so we have \begin{equation*}
    \Omega(E) = tr\left(\sum_n^{\varepsilon_n = E}|n\rangle \langle n|\right)
\end{equation*}
Finally we need to identify a suitable entropy. Recall we have the candidate Shannon-Boltzmann entropy, $S(E,\vec{q},\vec{N}) = k_B\ln\Omega(E,\vec{q},\vec{N})$. The \Emph{Volume entropy} (gibbs) is better suited if the thermodynamic limit is not taken: $S(E,\vec{q},\vec{N}) = k_B\ln\Omega(U< E,\vec{q},\vec{N})$.

\section{Canonical Ensemble}

Recall the canonical ensemble has the macrostate $(T,\vec{q},\vec{N})$, which is a system coupled to a heat bath with fixed $T$. We consider the combined system of the heat bath, $\sum_2$, and our system, $\sum_1$. We assume $\sum_1,\sum_2$ exchange heat to reach thermal equilibrium but the interaction is weak which means we can neglect the energy due to interactions: $U_{\sum} = U_{\sum_1} + U_{\sum_2}$. Recall $|\psi_{\alpha}\rangle$ is the possible state of system $\sum_1$ corresponding to eigenstate of $H_1$ with eigenenergy $\varepsilon_{\alpha}$. For $\sum$: \begin{align*}
    \Omega_{\sum}(E) &= \sum_{\alpha}\Omega_{\sum_1}(\varepsilon_{\alpha})\cdot \Omega_{\sum_2}(E-\varepsilon_{\alpha})
\end{align*}
which implies $p_{\alpha} \propto \Omega_{\sum_2}(E-\varepsilon_{\alpha})$. Using a Taylor expansion of the entropy around $E$ since $E \gg \varepsilon_{\alpha}$: \begin{equation*}
    k_B\ln\Omega_{\sum_2}(E-\varepsilon_{\alpha}) = k_B\ln\Omega_{\sum_2}(E) - k_B\varepsilon_{\alpha}\left(\frac{\partial }{\partial E_2}\ln\Omega_{\sum_2}(E_2)\right)\Bigg\rvert_{E_2 = E}+\cdots \approx k_B\ln\Omega_{\sum_2}(E) - \frac{\varepsilon}{T}
\end{equation*}
since $E-\varepsilon_{\alpha}\approx E$. Then $p_{\alpha}\propto\Omega_{\sum_2}(E-\varepsilon_{\alpha})\propto e^{-\beta\varepsilon_{\alpha}}$ with $\beta = \frac{1}{k_BT}$. It follows that \begin{equation*}
    \rho\propto \sum_{\alpha}e^{-\beta\varepsilon_{\alpha}}|\psi_{\alpha}\rangle\langle\psi_{\alpha}| = e^{-\beta H_1}\sum_{\alpha}|\psi_{\alpha}\rangle\langle\psi_{\alpha}|
\end{equation*}
which implies that \begin{equation*}
    \rho = \frac{e^{-\beta H_1}}{tr(e^{-\beta H_1})} = \frac{e^{-\beta H_1}}{Z(\beta)}
\end{equation*}
with $Z = tr(e^{-\beta H_1}) = \sum_ne^{-\beta\varepsilon_n}$ which is our partition function, where we sum over the discrete energy levels including multiplicity/degenerated energies. 

\begin{itemize}
    \item Observables: \begin{equation*}
            \langle O\rangle = tr(\rho O) = \frac{tr(e^{-\beta H}O)}{tr(e^{-\beta H})} 
        \end{equation*}
    \item Internal energy: \begin{equation*}
            U = \langle H\rangle = -\frac{\partial }{\partial \beta}\ln Z
        \end{equation*}
    \item Helmholtz free energy: \begin{equation*}
            F(T,\vec{q},\vec{N}) = -k_BT\ln Z
    \end{equation*}
\end{itemize}



\section{Grand Canonical Ensemble}

In the grand canonical ensemble the macrostate is $(T,\vec{q},\vec{\mu})$, and the system coupled to a heat bath with fixed $T$ and a particle reservoir at fixed $\vec{\mu}$. We again consider the total system as one isolated system with the reservoir $\sum_2$ and the system of interest $\sum_1$. We assume $U_1 \ll U_2$ and $N_1 \ll N_2$. For the combined system $\sum = \sum_1 \cup \sum_2$, we have $U \approx U_1+U_2, N \approx N_1+N_2,$ and $V = V_1+V_2$.

THepossible states of $\sum_1$ consist of joint eigenstates of the Hamilton operator $H_1$ and the number operator $N_1$ ($[H_1,N_1] = 0$): \begin{align*}
    H_1|E_{\alpha}(N_1)\rangle &= \varepsilon_{\alpha}(N_1)|E_{\alpha}(N_1)\rangle \\
    N_1|E_{\alpha}(N_1)\rangle &= N_1|E_{\alpha}(N_1)\rangle
\end{align*}
where \begin{equation*}
    \rho = \sum_{N_1}\sum_{\alpha}p_{\alpha}(N_1)|E_{\alpha}(N_1)\rangle\langle E_{\alpha}(N_1)|
\end{equation*}
for $\sum$ in the microcanonical ensemble \begin{equation*}
    \Omega_{\sum}(E,N,V) = \sum_{N_1}\sum_{\alpha}\Omega_{\sum_1}(\varepsilon_{\alpha}(N_1),N_1,V_1)\Omega_{\sum_2}(E-\varepsilon_{\alpha}(N_1),N_2, V_2)
\end{equation*}
We then have \begin{equation*}
    p_{\alpha}(N_1) \propto \Omega_{\sum_2}(E-\varepsilon_{\alpha}(N_1),N-N_1,V_2)
\end{equation*}
Taking a Taylor expansion we can write \begin{equation*}
    k_B\ln\Omega_{\sum_2}(E-\varepsilon_{\alpha}(N_1),N-N_1,V_2) \approx S_2(E,N,V_2) - \varepsilon_{\alpha}(N_1)\left(\frac{\partial S_2}{\partial U_2}\right)_{V_2,N_2}(E,N,V_2) - N_1 \left(\frac{\partial S_2}{\partial N_2}\right)_{U_2,V_2}(E,N,V_2)
\end{equation*}
We also have \begin{equation*}
    \left(\frac{\partial S_2}{\partial U_2}\right)_{N_2,V_2}(E,N,V_2) \approx \left(\frac{\partial S_2}{\partial U_2}\right)_{N_2,V_2}(E_2,N_2,V_2) = \frac{1}{T}
\end{equation*}
and 
\begin{equation*}
    \left(\frac{\partial S_2}{\partial N_2}\right)_{U_2,V_2}(E,N,V_2) \approx \left(\frac{\partial S_2}{\partial N_2}\right)_{U_2,V_2}(E_2,N_2,V_2) = -\frac{\mu}{T}
\end{equation*}
It follows that \begin{equation*}
    p_{\alpha}(N_1) \propto e^{-\beta(\varepsilon_{\alpha}(N_1) - \mu N_1)}
\end{equation*}
Then \begin{equation*}
    \rho \propto \sum_{N_1}\sum_{\alpha}e^{-\beta(\varepsilon_{\alpha}(N_1)-\mu N_1)}|E_{\alpha}(N_1)\rangle \langle E_{\alpha}(N_1)| = e^{-\beta(H_1-\mu \hat{N}_1)}\sum_{N_1}\sum_{\alpha}|E_{\alpha}(N_1)\rangle\langle E_{\alpha}(N_1)|
\end{equation*}
Thus, we have that \begin{equation*}
    \boxed{\rho(T,\mu) = \frac{e^{-\beta(\hat{H}-\mu\hat{N})}}{tr(e^{-\beta(\hat{H}-\mu\hat{N})})} = \frac{e^{-\beta(\hat{H}-\mu\hat{N})}}{Q}}
\end{equation*}
With $$Q(T,\mu) = tr e^{-\beta(\hat{H}-\mu\hat{N})} = \sum_{N=0}^{\infty}\sum{\alpha}e^{-\beta(E_{\alpha}(N)-\mu N)}=\sum_{N=0}^{\infty}e^{\beta \mu N}Z_N(T)$$
\begin{itemize}
    \item Observables: $$\langle O\rangle = tr(\rho O) = \frac{tr(e^{-\beta(\hat{H}-\mu\hat{N})}O)}{tr(e^{-\beta(\hat{H}-\mu\hat{N})})}$$
    \item Particle number: $$\langle N\rangle = k_BT\frac{\partial }{\partial \mu}\ln Q = \frac{\partial}{\partial(\beta \mu)}\ln Q$$
    \item Internal energy: $$U = \langle H\rangle = -\frac{\partial }{\partial \beta}\ln Q + \mu \langle N\rangle$$
    \item Grand Potential: $$\mathcal{G}(T,\vec{\mu},\vec{q}) = -k_BT\ln Q$$
\end{itemize}





\subsection{Ideal Gas in the Canonical Ensemble}

We consider a single particle in a box of volume $V = L^3$. Then \begin{equation*}
    H_1 = \frac{\vec{p}\cdot\vec{p}}{2m} = -\frac{\hbar^2}{2m}\nabla^2 
\end{equation*}
the representation of momentum operators in coordinate basis. Consider the energy eigenstates $H_1|\vec{k}\rangle = \varepsilon(\vec{k})|\vec{k}\rangle$. In the coordinate basis \begin{equation*}
    |\vec{k}\rangle = \sum_{\vec{k}}|\vec{x}\rangle\langle \vec{x}|\vec{k}\rangle
\end{equation*}
so \begin{equation*}
    -\frac{\hbar^2}{2m}\nabla^2\langle \vec{x}|\vec{k}\rangle = \varepsilon(\vec{k})\langle \vec{x}|\vec{k}\rangle
\end{equation*}
The solution to this DE is the plane waves \begin{equation*}
    \langle \vec{x}|\vec{k}\rangle = \frac{e^{i\vec{k}\cdot\vec{x}}}{\sqrt{V}}
\end{equation*}
so $\varepsilon(\vec{k}) = \frac{\hbar^2k^2}{2m}$. By periodic boundary equations $\vec{k} = \frac{2\pi }{L}\langle n_x,n_y,n_z\rangle$, with $n_i \in \Z$, which corresponds with an infinite number of energy eigenstates. In the thermodynamic limit as $V\rightarrow \infty$, the number of modes in the volume element $d^3\vec{k}$ is \begin{equation*}
    dN = \frac{V}{(2\pi)^3}d^3\vec{k}
\end{equation*}
Then the partition function in the thermodynamic limit is \begin{align*}
    Z_1 &= tr(e^{-\beta H_1}) = \sum_{\vec{k}}e^{-\beta \hbar^2k^2/2m} \\
    &= V\int\frac{d^3\vec{k}}{(2\pi)^3}e^{-\beta\hbar^2k^2/2m} \\
    &= \frac{V}{(2\pi)^3}\left(\sqrt{\frac{2\pi mk_BT}{\hbar^2}}\right)^3 = \frac{V}{\lambda^3}
\end{align*}
with $\lambda := \frac{h}{\sqrt{2\pi mk_BT}}$ the characteristic wavelength.

The density matrix can be found as follows: \begin{align*}
    \langle \vec{x}'|\rho|\vec{x}\rangle &= \sum_{\vec{k}}\langle \vec{x}'|\vec{k}\rangle \frac{e^{-\beta\varepsilon(\vec{k})}}{Z_1}\langle \vec{k}|\vec{x}\rangle \\
    &= \frac{\lambda^3}{V}\int\frac{Vd^3\vec{k}}{(2\pi)^3}\frac{e^{-i\vec{k}\cdot(\vec{x}-\vec{x}')}}{V}e^{-\beta\hbar^2k^2/2m} \\
    &= \frac{\lambda^3}{V}\frac{1}{(2\pi)^3}\prod_{j=1}^3\int dk_je^{-\frac{\beta\hbar^2}{2m}\left(k_j+i(x_j-x_j')\frac{m}{\beta\hbar^2}\right)^2} \cdot e^{-(x_j-x_j')^2m/2\beta\hbar^2} \\
    &= \frac{1}{V}e^{-m||\vec{x}-\vec{x}'||^2/2\beta\hbar^2} \\
    &= \frac{1}{V}e^{-\pi||\vec{x}-\vec{x}'||^2/\lambda^2}
\end{align*}
The diagonal elements are $\langle \vec{x}|\rho|\vec{x}\rangle = \frac{1}{V} = $ the probability of finding the particle at $\vec{x}$. For the off-diagonal elements there is no classical analog. The particle has a wave packet of size $\lambda = \frac{h}{\sqrt{2\pi mk_BT}}$, the \Emph{thermal wavelength}. As $T\rightarrow \infty$, $\lambda\rightarrow 0$, so off-diagonal elements vanish (classical limit). As $T\rightarrow 0$, $\lambda \rightarrow \infty > L$ which is an issue of our approximation!!




\section{Entropy and Extremal Properties of Thermodynamic Potentials}



\section{Third Law of Thermodynamics}






%%%%%%%%%%%%%%%%%%%%% Chapter 5.7
\chapter{Ideal Quantum Gas}

\section{Indistinguishable Particles}

\subsection{Fermions}

\subsection{Bosons}

\subsection{Anyons}

\section{Hilbert Space of Indistinguishable Particles}


\subsection{Fermionic and Bosonic Subspaces}

\section{Ideal Quantum Gas in the Canonical Ensemble}

\section{Ideal Quantum Gas in the Grand Canonical Ensemble}


\subsection{The Degenerate Fermi Gas}


\subsection{Thermodynamic Properties of the Ideal Quantum Gas}


\subsection{Thermodynamic Properties of the Degenerate Fermi Gas}


\subsection{The Degenerate Bose Gas}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Part 6
\part{Complexity}


%%%%%%%%%%%%%%%%%%%%%% Chapter 6.1
\chapter{Crackling Noise and Earthquakes}

\section{Dynamics of Crackling Noise}

\begin{defn}
    \Emph{Crackling noise} relates to phenomena in which events of all sizes (/an enourmous range of sizes) occur with probability following a power distribution.
\end{defn}

Crackling is observed in many phenomena including paper crumbling, rise krispies, magnets and magnetization, earthquakes, fire, crackling foam, solar flares, fractures, etc.

In the case of Earthquakes the power law is observed as the number of earthquakes versus their magnitude, following the \Emph{Gutenberg-Richter law}: $$Probability\sim Size^{-Power}$$

\subsection{Plasticity}

We observe what are known as \Emph{dislocation avalanches} when bending plastics, such as forks, where there are sudden changes once the strain has reached a certain point, and these fractures result in an avalanche of fractures.

\subsection{Magnetic Barkhausen Noise}

In Barkhausen noise we observe events of all sizes, and structure on all scales. This is both observed temporaly and spatially in the magnetic with avalanche fractals in time and space. This is also seen in the Hysterisis curve of the magnetization.

\subsection{What is an avalanche?}

To understand what an avalanch we look at hysteresis in the $T = 0$ random field Ising model: $$\mathcal{H} = -\sum_{ij, nn}JS_iS_j-\sum_iHS_i-h_iS_i$$
where $S_i = \pm 1$ is the magnetic spin of the domain, $J$ the coupling between neighboring spins, $H$ the external magnetic field, and $h_i$ the random field at the site, ``dirt," chosen from a Gaussian distribution of RMS width $R$.

It has the following dynamics: \begin{itemize}
    \item Starts all spins down: $H = -\infty$
    \item Increase field slowly 
    \item Spin flips when pushed over
    \item Initial center spin pushed by $H$
    \item Pushes neighbors causing an avalanche
\end{itemize}
The coupling with neighbors causes the avalanches. For $R < R_c$ we see neighbors dominating and one big avalanche (first spin triggers all others). For $R > R_c$ we observe large disorder, with dirt dominating and many small avalanches occurring (each spin to itself). At $R = R_c$ we have the power law $M-M_c \sim (H-H_c)^{1/\delta}$ for the critical exponent $\delta$.

\section{The Renormalization Group}

Why do we observe universality? It corresponds to a fixed point under coarse graining. In this model our system space consists of properties $=$ coordinates. We have many ways of coarse-graining including $\epsilon$-expansion, Monte Carlo, functional RG, etc. In any case the flow on the system space involves a dynamical system on a space of dynamical systems. We look for the system state $S^*$ which is a fixed point at which we observe self-similarity, and that lies on the critical manifold $C$. The critical manifold should flow into $S^*$, corresponding to universality, while all other positions flow away.

We can also have spontaneous criticality, involving generic scale invariance and self-organized criticality. For instance we could have an attracting fixed point. Sometimes we still observe fluctations in these cases. On the other hand we could have a slow driving/inhomogeneous/long range forces which drive to the critical point. This occurs in such phenomena as earthquakes, sandpiles, magnets, front propagation, forest fires, etcetera. 

\section{Avalanches}

\subsection{Avalanche Temporal Shapes}

\begin{note}
    An avalanche or cascade occurs when one event causes one or more subsequent events, which in turn may cause further events in a chain reaction
\end{note}

\begin{defn}
    The \Emph{temporal profiles} of avalanches of a fixed duration $\tau$ are the average of avalanche shapes. 
\end{defn}

At the critical point we observe universal scaling of avalanches. For instance the distribution of avalanche sizes has a power-law scaling at the critical point. We obtain the average avalanche shape by averaging the temporal profiles of all avalanches that have a fixed duration $T$. At criticality the average avalanche shape is a universal function of the rescaled time $t/T$, meaning that the average avalanche shapes for different durations can be rescaled to collapse onto a single curve. 

For some threshold height, we consider an event to be described by the temporal window in which the magnitude is above the threshold. When we plot the average size versus the duration of the event we obtain a power law of the form $$S \sim \xi^{\sigma\nu}\sim T^{\sigma\nu z}$$

\begin{rmk}[Magnetization case]
    Dividing the time scale by $T$ and the voltage scale by $V_{max}$, we obtain universal scaling forms. Experimentally and theoretically we observe that for larger durations, $T$, the characteristic parabola appears to widen.
\end{rmk}

\subsection{Spatial Structure}

We expect that all geometrical features of large avalanches should be universal: 
\begin{itemize}
    \item Correlation functions and fractal dimensions
    \item Aspect ratios
    \item Topology (i.e. holes,  interconnectedness, etc)
    \item Front shapes
    \item Height and width distributions
\end{itemize}
Avalanches often appear to evolve fractally in the spatial domain. An example of this evolution is the distribution of earthquakes, or the adjustment of foam (which produces crackling noise) as one squashes it and applies stress.


\subsection{Earthquakes}

In Elastic rebound theory earthquakes are modeled as the tectonic plates shearing about some line in a very slow manor over many years (for example $5 cm/year$). THen when the stress has reached a certain degree one part of the plates snaps, with the component of the plate on either side of the axis moving in the opposite direction at very high speeds ($3.5 km/s$). This then casues a chain reaction as the other portions of the plate break and shift.
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{Images/Earthquake.PNG}
    \caption{Diagram of earthquake sheering}
    \label{fig:quake}
\end{figure}

THis model is described by the Seismic moment, $M = \mu A \delta_e$ where $\mu$ is the shear modulus of the rocks involved, $A$ is the area of the rupture along the geological fault, and $\delta_e$ is the average slip oon $A$. However, this values are often very large so instead we use the moment magnitude: $$m = (\log_{10}M - d)/c$$

Over time we observe that the strength of the stress on the plates. In particular, instead of perfectly periodic behaviour in the actual stress, with a linear increase then a sudden drop (triangular wave), the rate of increasing stress and the magnitude it hits before an event occurs and it drops varies. This is due to the presence of aftershocks and interference between quakes and aftershocks.

Quakes cause both static and dynamic stress changes in the nearby plates, which can cause other quakes farther away. The dynamic stress changes are typically seen as wave pulses of stress that propogate out from the initial quake location, while static stresses are constant shifts in the stress for a region due to a shift (a step like function if you will).

The Gutenberg Richter law is a scale-free and universal way of describing quake phenomena. 

\subsection{Space Weather}

In solar wind and flare phenomena we observe crackling noise as well as associated power-law relations.


%%%%%%%%%%%%%%%%%%%%%% Chapter 6.2
\chapter{Complex Systems}


\section{Introduction and Overview}

\begin{defn}
    \Emph{Complexity} is a scientific theory which asserts that some systems display behavioral phenomena that are completely inexplicable by any conventional analysis of the systems' constituent parts (the whole is more than the sum of its parts - the behvaiour as a whole is not suggested by the behaviour of a part). These phenomena commonly referred to as \Emph{emergent behaviour} seem to occur in many complex systems involving living organisms (eg the brain)
\end{defn}

\begin{rmk}
    Behind many complex systems there is a \Emph{network}, that defines the pattern of connections between the interacting components.
\end{rmk}

\begin{eg}[Network examples]
    The internet, which is composed of machines and routers. Social networks such as facebook. Biological networks such as neural networks.
\end{eg}

\subsection{Neuron Networks}

Neuron's are composed of a central cell body as well as a tell of axon's (connecting fibers) which end off in axon terminals (transmitters). The central body also has a number of dendrites which act as recievers. A signal or event in a Neuron corresponds with an action potential, where once the electrical potential passes a certain threshold, it spikes in a characteristic fashion. This spikes in a neural network can cause \Emph{neuronal avalanches}, where one neuron spiking influences neighboring neurons making it more likely for them to spike in a cascading fashion. These avalanches follow a power law distribution of the proportion versus the sequence size or total number of spikes. In particular these types of avalanches can be seen in the brain, where one neuron spiking in a particular location can cause a large portion of neurons to spike in succession in that location.

At criticality we obtain \Emph{optimiality}, which equates to a maximum of information transfer, storage, and computational power. Being subcritical corresponds with a state akin to a comma, while being supercritical can correspond to a state akin to a seizure.

\begin{rmk}
    We can model these different criticality stages and avalanches using a branching process described by directed percolation.
\end{rmk}




%%%%%%%%%%%%%%%%%%%%%% - Appendices
\begin{appendices}


\end{appendices}


\end{document}


%%%%%% END %%%%%%%%%%%%%
